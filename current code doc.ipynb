{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by loading and then merging the datasets we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv('key.csv', sep=',')\n",
    "dat2 = pd.read_csv('SP500_finratios.csv', sep=',', parse_dates=['adate', 'qdate', 'public_date'])\n",
    "dat3 = pd.read_csv('ratings2.csv', sep=',', parse_dates=['datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the columns of the first data set are renamed to match the names of the other data sets\n",
    "dat1.columns = ['gvkey','linktype','permno','permco','linkdt','linkenddt','conm','tic','cusip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the duplicates in the key data set are removed, so that pd.merge() will work\n",
    "dat1 = dat1.set_index('permno')\n",
    "dat1 = dat1[~dat1.index.duplicated(keep='first')]\n",
    "dat1.reset_index(level=0, inplace=True)   #The permnos are converted back to a normal variable, otherwise an error can occur when merging on permno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the variable datadate is renamed public_date and both are transformed to the same format, so\n",
    "# that pd.merge()recognises them as one and the same\n",
    "dat3['public_date'] = dat3['datadate']\n",
    "del dat3['datadate']\n",
    "dat2['public_date'] = pd.to_datetime(dat2.public_date)\n",
    "dat3['public_date'] = pd.to_datetime(dat3.public_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1and2 = pd.merge(dat1, dat2, on='permno', how='inner', validate='one_to_many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.merge(dat1and2, dat3, on=['gvkey', 'public_date', 'conm', 'tic', 'cusip'], how='inner', validate='one_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>linktype</th>\n",
       "      <th>permco</th>\n",
       "      <th>linkdt</th>\n",
       "      <th>linkenddt</th>\n",
       "      <th>conm</th>\n",
       "      <th>tic</th>\n",
       "      <th>cusip</th>\n",
       "      <th>adate</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_capital</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>DIVYIELD</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.338</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366</td>\n",
       "      <td>-8.617</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29853</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.741</td>\n",
       "      <td>1.478</td>\n",
       "      <td>1.961</td>\n",
       "      <td>1.961</td>\n",
       "      <td>0.726</td>\n",
       "      <td>9.038</td>\n",
       "      <td>9.364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29854</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>8.184</td>\n",
       "      <td>10.913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29855</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>9.392</td>\n",
       "      <td>12.469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29856</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>8.557</td>\n",
       "      <td>11.359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29857</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.789</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.944</td>\n",
       "      <td>1.944</td>\n",
       "      <td>0.681</td>\n",
       "      <td>7.114</td>\n",
       "      <td>11.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29858 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       permno   gvkey linktype  permco    linkdt linkenddt  \\\n",
       "0       21020    1045       LC   20010  19500101  19620130   \n",
       "1       21020    1045       LC   20010  19500101  19620130   \n",
       "2       21020    1045       LC   20010  19500101  19620130   \n",
       "3       21020    1045       LC   20010  19500101  19620130   \n",
       "4       21020    1045       LC   20010  19500101  19620130   \n",
       "...       ...     ...      ...     ...       ...       ...   \n",
       "29853   13168  199356       LC   53964  20111221         E   \n",
       "29854   13168  199356       LC   53964  20111221         E   \n",
       "29855   13168  199356       LC   53964  20111221         E   \n",
       "29856   13168  199356       LC   53964  20111221         E   \n",
       "29857   13168  199356       LC   53964  20111221         E   \n",
       "\n",
       "                              conm   tic      cusip      adate  ...  \\\n",
       "0      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2008-12-31  ...   \n",
       "1      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "2      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "3      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "4      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "...                            ...   ...        ...        ...  ...   \n",
       "29853              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29854              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29855              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29856              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29857              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "\n",
       "      debt_capital de_ratio  cash_ratio  quick_ratio  curr_ratio  at_turn  \\\n",
       "0            1.338   -9.366       0.428        0.603       0.664    0.816   \n",
       "1            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "2            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "3            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "4            1.366   -8.617       0.551        0.712       0.780    0.799   \n",
       "...            ...      ...         ...          ...         ...      ...   \n",
       "29853        0.286    0.741       1.478        1.961       1.961    0.726   \n",
       "29854        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29855        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29856        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29857        0.289    0.789       1.396        1.944       1.944    0.681   \n",
       "\n",
       "         ptb  PEG_trailing  DIVYIELD  splticrm  \n",
       "0        NaN           NaN       NaN        B-  \n",
       "1        NaN           NaN       NaN        B-  \n",
       "2        NaN           NaN       NaN        B-  \n",
       "3        NaN           NaN       NaN        B-  \n",
       "4        NaN           NaN       NaN        B-  \n",
       "...      ...           ...       ...       ...  \n",
       "29853  9.038         9.364       NaN       NaN  \n",
       "29854  8.184        10.913       NaN       NaN  \n",
       "29855  9.392        12.469       NaN       NaN  \n",
       "29856  8.557        11.359       NaN       NaN  \n",
       "29857  7.114        11.603       NaN       NaN  \n",
       "\n",
       "[29858 rows x 52 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's have a look at it. First of, we're interested in the distribution of the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BBB     4946\n",
       "BBB+    4133\n",
       "A-      3615\n",
       "A       3196\n",
       "BBB-    2654\n",
       "A+      1670\n",
       "BB+     1262\n",
       "AA-      902\n",
       "BB-      824\n",
       "BB       757\n",
       "AA       417\n",
       "B+       305\n",
       "AAA      296\n",
       "AA+      199\n",
       "B-       133\n",
       "B        109\n",
       "CCC+      50\n",
       "D          4\n",
       "CCC        2\n",
       "Name: splticrm, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['splticrm'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as there are only four observations of rating D, and only two observations of rating CCC, our data set does not allow us to draw any conclusions for these ratings and we have to drop them from our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat[dat['splticrm'] != 'CCC']\n",
    "dat = dat[dat['splticrm'] != 'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we consider our numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              bm          ps         pcf        dpr        npm        gpm  \\\n",
      "min     0.001000    0.047000 -224.460000  -0.001000 -51.493000 -37.707000   \n",
      "mean    0.511099    2.530784   12.210846   0.489392   0.080365   0.431986   \n",
      "50%     0.391000    1.814000   11.113000   0.305000   0.092000   0.406000   \n",
      "max   137.237000  145.774000  280.893000  80.554000   1.799000   0.982000   \n",
      "\n",
      "            cfm       roa        roe      roce     efftax     GProf  \\\n",
      "min  -47.694000 -0.595000 -34.647000 -1.111000 -12.365000 -1.143000   \n",
      "mean   0.152864  0.142624   0.166138  0.176364   0.290338  0.294441   \n",
      "50%    0.152500  0.139000   0.137000  0.154000   0.302000  0.263500   \n",
      "max    2.054000  0.626000  15.502000  2.279000  29.944000  1.255000   \n",
      "\n",
      "      equity_invcap  debt_invcap  totdebt_invcap  capital_ratio     int_debt  \\\n",
      "min      -17.816000     0.000000        0.000000      -5.619000     0.000000   \n",
      "mean       0.593529     0.389615        0.471293       0.395291     0.336795   \n",
      "50%        0.633000     0.348000        0.401000       0.353000     0.055000   \n",
      "max        1.000000    18.816000       21.164000      18.816000  1278.000000   \n",
      "\n",
      "      int_totdebt   cash_lt  \n",
      "min      0.000000  0.000000  \n",
      "mean     0.079749  0.311298  \n",
      "50%      0.048000  0.147000  \n",
      "max    115.262000  6.502000   \n",
      "\n",
      "      invt_act   debt_at  debt_ebitda  short_debt  curr_debt   lt_debt  \\\n",
      "min   0.000000  0.000000 -2487.500000    0.000000   0.030000  0.000000   \n",
      "mean  0.222505  0.254492     2.430452    0.158646   0.395143  0.345054   \n",
      "50%   0.194000  0.242000     1.813000    0.097000   0.363000  0.364000   \n",
      "max   5.389000  1.729000  2328.100000    5.712000   1.000000  1.408000   \n",
      "\n",
      "       ocf_lct  cash_debt    fcf_ocf     dltt_be  debt_assets  debt_capital  \\\n",
      "min  -3.037000  -0.916000 -53.180000    0.000000     0.043000      0.002000   \n",
      "mean  0.717987   0.230432   0.571597    0.880104     0.618524      0.499772   \n",
      "50%   0.613000   0.169000   0.725000    0.488000     0.617000      0.472000   \n",
      "max   4.559000   5.736000   2.033000  157.897000     1.919000      4.234000   \n",
      "\n",
      "         de_ratio  cash_ratio  quick_ratio  curr_ratio   at_turn        ptb  \\\n",
      "min  -1228.100000    0.001000      0.09100    0.113000  0.013000   0.181000   \n",
      "mean     2.908976    0.764793      1.49689    1.899965  0.814618   3.899753   \n",
      "50%      1.550000    0.441000      1.18000    1.573000  0.657000   2.629000   \n",
      "max   1818.100000   11.403000     12.82500   12.825000  5.912000  65.486000   \n",
      "\n",
      "      PEG_trailing  \n",
      "min     -33.082000  \n",
      "mean      2.819965  \n",
      "50%       1.167000  \n",
      "max     100.212000  \n"
     ]
    }
   ],
   "source": [
    "des = dat.loc[:, 'bm':'cash_lt'].describe()\n",
    "ind = [3, 1, 5, 7]   #printing the entire .describe() information consumes unnecessarily much computation power, so I index the lines I'm interested in\n",
    "print(des.iloc[ind], '\\n')\n",
    "des = dat.loc[:, 'invt_act':].describe()  #I do this in two steps, because I don't want any variables hidden behind \"...\"\n",
    "ind = [3, 1, 5, 7]\n",
    "print(des.iloc[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output suggests that several variables have extreme outliers - for instance bm has a minimum of 0.001000, a mean of  0.506463, but a maximum of 137.237000. Visualising the data with boxplots shows this quite notably. This means the imputation method we initially considered, which was based on linear regression, is probably not the best way to handle the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12caec1b588>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEGCAYAAAC0DiQ1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAALRUlEQVR4nO3df6zd9V3H8dd7XJ2MxnS0TBHQoiXTuYiD/jHUP8gG2i0LxsQ/WFxoookxGpxGoyMkNvUP/1GM0OgMQaU4Mv/AOQmJRZiGqDFoq/JDGa5k3aBOafuHs7WaFT7+cU63a3tb2tJz3ufA45E095zv9977eefmnGfP/Zx7z60xRgDo8ZbuAQDezEQYoJEIAzQSYYBGIgzQaOVc3nnjxo1j06ZNMxoF4I1p7969h8YYl6117pwivGnTpuzZs+fCTAXwJlFVXzjdOdsRAI1EGKCRCAM0EmGARiIM0EiEARqJMEAjEQZoJMIAjUQYoJEIAzQSYYBGIgzQSIQBGokwQCMRBmgkwgCNRBigkQgDNDqnvzF3vnbu3Jknnngi69evz3333TePJQGWwlwivG/fvhw6dCjHjh2bx3IAS8N2BEAjEQZoJMIAjUQYoJEIAzQSYYBGIgzQSIQBGokwQCMRBmgkwgCNRBigkQgDNBJhgEYiDNBIhAEaiTBAIxEGaCTCAI1EGKCRCAM0EmGARiIM0EiEARqJMEAjEQZoJMIAjUQYoJEIAzQSYYBGIgzQSIQBGokwQCMRBmgkwgCNRBigkQgDNBJhgEYiDNBIhAEaiTBAIxEGaDSXCB84cOCrl3fu3JmdO3fOY1mAhbcyj0WOHTv21cv79u2bx5IAS8F2BEAjEQZoJMIAjUQYoJEIAzQSYYBGIgzQSIQBGokwQCMRBmgkwgCNRBigkQgDNBJhgEYiDNBIhAEaiTBAIxEGaCTCAI1EGKCRCAM0EmGARiIM0EiEARqJMEAjEQZoJMIAjUQYoJEIAzQSYYBGIgzQSIQBGokwQCMRBmgkwgCNRBigkQgDNBJhgEYr817wqaeeSpLceOONp5y76667cv311895IoDTO3z4cHbs2JHt27dnw4YNF/zzL9Qj4e3bt3ePAPD/7Nq1K88880weeOCBmXz+uUb46NGjZzx/5MiR7N27d07TAJzZ4cOHs3v37owxsnv37hw+fPiCr7FQj4QTj4aBxbFr1668+uqrSZJXXnllJo+GXzPCVfWTVbWnqvYcPHjwgg9wsiNHjsx8DYCz8fjjj+f48eNJkuPHj+exxx674Gu8ZoTHGPeOMbaMMbZcdtllF3yAk61bt27mawCcjZtuuikrK5OfX1hZWcnNN998wddYuO2IHTt2dI8AkCTZtm1b3vKWSSYvuuii3HbbbRd8jblG+JJLLjnj+XXr1vkRNWBhbNiwIVu3bk1VZevWrTP5EbW5/5zwmXgUDCyabdu2Zf/+/TN5FJw0RPjaa69Nktx9993zXhrgnG3YsCH33HPPzD7/wu0JA7yZiDBAIxEGaCTCAI1EGKCRCAM0EmGARiIM0EiEARqJMEAjEQZoJMIAjUQYoJEIAzQSYYBGIgzQSIQBGokwQCMRBmgkwgCNRBigkQgDNBJhgEYiDNBIhAEaiTBAIxEGaCTCAI1EGKCRCAM0EmGARiIM0EiEARqJMEAjEQZoJMIAjUQYoJEIAzRamcciF198cY4ePZok2bx58zyWBFgKc4nwFVdckUOHDiVJbr/99nksCbAUbEcANBJhgEYiDNBIhAEaiTBAIxEGaCTCAI1EGKCRCAM0EmGARiIM0EiEARqJMEAjEQZoJMIAjUQYoJEIAzQSYYBGIgzQSIQBGokwQCMRBmgkwgCNRBigkQgDNBJhgEYiDNBIhAEaiTBAIxEGaCTCAI1EGKCRCAM0EmGARiIM0EiEARqJMEAjEQZoJMIAjUQYoJEIAzRamccimzdvzoEDB7J+/fp5LAewNGqMcdbvvGXLlrFnz54ZjgPwxlNVe8cYW9Y6ZzsCoJEIAzQSYYBGIgzQSIQBGokwQCMRBmgkwgCNRBigkQgDNBJhgEYiDNBIhAEaiTBAIxEGaCTCAI1EGKCRCAM0EmGARiIM0Oic/tBnVR1M8oXzXGtjkkPn+bEdlm3eZPlmNu/sLdvMb9R5v22McdlaJ84pwq9HVe053V8bXUTLNm+yfDObd/aWbeY347y2IwAaiTBAo3lG+N45rnUhLNu8yfLNbN7ZW7aZ33Tzzm1PGIBT2Y4AaCTCAI1mHuGq2lpVz1fVvqr62KzXOx9VdVVV/WVVPVdV/1xVH50ev7SqHquqz03fvr171tWq6qKq+seqemR6fWHnrar1VfVQVX12+nW+YZHnTZKq+vnp7eHZqvpkVX3DIs1cVb9fVS9X1bOrjp12vqq6Y3o/fL6qfmiBZv716e3i6ar6k6pavygzrzXvqnO/WFWjqjauOnbO8840wlV1UZLfTvKBJO9K8uGqetcs1zxPx5P8whjju5K8N8nPTOf8WJLPjDGuSfKZ6fVF8tEkz626vsjz3p1k9xjjO5Ncm8ncCztvVV2R5GeTbBljvDvJRUluzWLNfH+SrScdW3O+6e351iTfPf2Y35neP+ft/pw682NJ3j3G+J4k/5rkjmRhZr4/p86bqroqyc1Jvrjq2PnNO8aY2b8kNyR5dNX1O5LcMcs1L9Dcfzr9Aj+f5PLpscuTPN8926oZr8zkTva+JI9Mjy3kvEm+McnnM30ieNXxhZx3Os8VSV5McmmSlSSPJPnBRZs5yaYkz77W1/Tk+16SR5PcsAgzn3TuR5I8uEgzrzVvkocyeTCxP8nG1zPvrLcjTtyQT3hpemxhVdWmJO9J8mSSbxpjfClJpm/f0TfZKX4ryS8leXXVsUWd99uTHEzyB9Ptk/uq6pIs7rwZYxxI8huZPNL5UpL/HGP8eRZ45qnTzbcs98UfT/Jn08sLOXNV3ZLkwBjjqZNOnde8s45wrXFsYX8mrqrWJfnjJD83xvhy9zynU1UfSvLyGGNv9yxnaSXJdUk+PsZ4T5KjWaCth7VM91J/OMnVSb4lySVV9ZHeqV6Xhb8vVtWdmWwNPnji0Brv1jpzVb0tyZ1JfmWt02sce815Zx3hl5Jcter6lUn+bcZrnpeq+rpMAvzgGONT08P/UVWXT89fnuTlrvlO8v1Jbqmq/Un+KMn7quoTWdx5X0ry0hjjyen1hzKJ8qLOmyQ3Jfn8GOPgGOMrST6V5Puy2DMnp59voe+LVbUtyYeS/NiYfi+fxZz5OzL5j/mp6f3vyiT/UFXfnPOcd9YR/vsk11TV1VX19ZlsWj884zXPWVVVkt9L8twY4zdXnXo4ybbp5W2Z7BW3G2PcMca4coyxKZOv6V+MMT6SxZ3335O8WFXvnB56f5J/yYLOO/XFJO+tqrdNbx/vz+TJxEWeOTn9fA8nubWq3lpVVye5JsnfNcx3iqramuSXk9wyxvjvVacWbuYxxjNjjHeMMTZN738vJbluehs/v3nnsKn9wUye8XwhyZ3z3lQ/yxl/IJNvG55O8k/Tfx9MsiGTJ78+N317afesa8x+Y772xNzCzpvke5PsmX6NP53k7Ys873TmHUk+m+TZJH+Y5K2LNHOST2ayX/2VaQx+4kzzZfJt9AuZPHn3gQWaeV8me6kn7nu/uygzrzXvSef3Z/rE3PnO69eWARr5jTmARiIM0EiEARqJMEAjEQZoJMIsjaratNarWcEyE2GARiLMslmpql3T1559aPobbfur6teq6m+rak9VXVdVj1bVC1X1U90Dw5mIMMvmnUnuHZPXnv1ykp+eHn9xjHFDkr/K5DVgfzST14b+1Y4h4WyJMMvmxTHG30wvfyKTXzlPvvaaJM8keXKM8V9jjINJ/mf1X2qARSPCLJuTf8/+xPX/nb59ddXlE9dXZj0UnC8RZtl8a1XdML384SR/3TkMvF4izLJ5Lsm2qno6kz899PHmeeB18SpqAI08EgZoJMIAjUQYoJEIAzQSYYBGIgzQSIQBGv0fzCov6HzDtQ0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the outliers we show in these boxplots are so extreme that when we tried using linear regression for\n",
    "# the imputation, they influenced the slope so much that some negative values where imputed ...\n",
    "sns.boxplot(x = 'bm', data = dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12caeceacf8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x = 'int_debt', data = dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12caf76fa58>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x = 'int_totdebt', data = dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the data we have, we take a look at the data we do not have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a little indulgence\n",
    "class color:\n",
    "   bold = '\\033[1m'\n",
    "   end = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mColumn Names         Total NAs      NAs per observations\u001b[0m\n",
      "permno               0              0.000000\n",
      "gvkey                0              0.000000\n",
      "linktype             0              0.000000\n",
      "permco               0              0.000000\n",
      "linkdt               0              0.000000\n",
      "linkenddt            0              0.000000\n",
      "conm                 0              0.000000\n",
      "tic                  0              0.000000\n",
      "cusip                0              0.000000\n",
      "adate                13             0.000435\n",
      "qdate                0              0.000000\n",
      "public_date          0              0.000000\n",
      "bm                   638            0.021372\n",
      "ps                   12             0.000402\n",
      "pcf                  27             0.000904\n",
      "dpr                  1862           0.062374\n",
      "npm                  12             0.000402\n",
      "gpm                  12             0.000402\n",
      "cfm                  98             0.003283\n",
      "roa                  27             0.000904\n",
      "roe                  666            0.022310\n",
      "roce                 109            0.003651\n",
      "efftax               2108           0.070615\n",
      "GProf                12             0.000402\n",
      "equity_invcap        18             0.000603\n",
      "debt_invcap          65             0.002177\n",
      "totdebt_invcap       82             0.002747\n",
      "capital_ratio        59             0.001976\n",
      "int_debt             3213           0.107631\n",
      "int_totdebt          3044           0.101970\n",
      "cash_lt              26             0.000871\n",
      "invt_act             4275           0.143206\n",
      "debt_at              76             0.002546\n",
      "debt_ebitda          162            0.005427\n",
      "short_debt           1146           0.038389\n",
      "curr_debt            4016           0.134530\n",
      "lt_debt              59             0.001976\n",
      "ocf_lct              4011           0.134363\n",
      "cash_debt            140            0.004690\n",
      "fcf_ocf              571            0.019128\n",
      "dltt_be              661            0.022143\n",
      "debt_assets          26             0.000871\n",
      "debt_capital         197            0.006599\n",
      "de_ratio             26             0.000871\n",
      "cash_ratio           4002           0.134061\n",
      "quick_ratio          4002           0.134061\n",
      "curr_ratio           4002           0.134061\n",
      "at_turn              27             0.000904\n",
      "ptb                  638            0.021372\n",
      "PEG_trailing         10354          0.346844\n",
      "DIVYIELD             6736           0.225647\n",
      "splticrm             4384           0.146858\n"
     ]
    }
   ],
   "source": [
    "col_Names = dat.columns.values\n",
    "total_NAs = pd.isna(dat).sum()\n",
    "percentage_NAs = dat.isna().sum()/len(dat)\n",
    "print(color.bold + \"%-20s %-14s %s\" %(\"Column Names\", \"Total NAs\", \"NAs per observations\") + color.end )\n",
    "#I used the % operator because tab didn't work and this allows me to define the spaces between the items\n",
    "\n",
    "#the loop prints one line after another\n",
    "for item_a, item_b, item_c in zip(col_Names, total_NAs, percentage_NAs):\n",
    "    print(\"%-20s %-14d %.6f\" %(item_a, item_b, item_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown here, 4384 observations have no long term credit rating (splticrm), which means we cannot use those observations for our prediction models. Still, the observations might be helpful for imputing missing values, so we will drop them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variable that stands out is PEG_trailing, and we are unsure how valid it is to impute more than one third of the data. However, dropping the missing values in PEG_trailing is not an option, since they are not missing completely at random: We can show that the missingness is systematic by plotting it against another variable. So, if we were to just drop the missing values, we would bias the remaining data. Instead, we could drop the entire variable, but we might as well keep it in for now, impute the missing values, and if we later find that it reduces the predictive power, we can still drop it. (Spoiler: It does not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12caf7c9e48>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(dat.pcf, dat.PEG_trailing.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we fill in the missing data by using IterativeImputer. Its method is to model each feature with missing values as a function of other features in a round-robin regression. As stated above, linear regression is not an ideal function here, so we decided to use scikit-learn's ExtraTreesRegressor, because it works better with extreme outliers and non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_dat = dat.loc[:, 'bm':'PEG_trailing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the imputation method\n",
    "imp = IterativeImputer(estimator = ExtraTreesRegressor(n_estimators=10, random_state=1, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\impute\\_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# actually imputing the values\n",
    "imputed = imp.fit_transform(numerical_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the imputation returns an array, so we transform the data back to a dataframe\n",
    "imputed = pd.DataFrame(data = imputed, columns = numerical_dat.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the missing data is imputed, we add public_date, permno and splticrm to the data frame. The first two will be needed later for further analysis, the latter is added so we can drop all observations with missing credit rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = pd.concat([imputed, dat.loc[:, 'public_date'], dat.loc[:, 'permno'], dat.loc[:, 'splticrm']], axis = 'columns')\n",
    "imputed = imputed.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>public_date</th>\n",
       "      <th>permno</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.4653</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>3.6490</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.5750</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>6.3546</td>\n",
       "      <td>-3.1471</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>21020.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.3733</td>\n",
       "      <td>0.153</td>\n",
       "      <td>3.287</td>\n",
       "      <td>4.8817</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.8864</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>9.2910</td>\n",
       "      <td>-2.5938</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>21020.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3733</td>\n",
       "      <td>0.152</td>\n",
       "      <td>3.258</td>\n",
       "      <td>4.8817</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.8864</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>9.2910</td>\n",
       "      <td>-2.5938</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>21020.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.3733</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.640</td>\n",
       "      <td>4.8817</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.8864</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>9.2056</td>\n",
       "      <td>-2.5938</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>21020.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.4105</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.753</td>\n",
       "      <td>4.4286</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-1.8864</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.617</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>9.6632</td>\n",
       "      <td>-2.5376</td>\n",
       "      <td>2010-05-31</td>\n",
       "      <td>21020.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29832</th>\n",
       "      <td>0.0780</td>\n",
       "      <td>11.083</td>\n",
       "      <td>28.586</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728</td>\n",
       "      <td>1.982</td>\n",
       "      <td>2.851</td>\n",
       "      <td>2.851</td>\n",
       "      <td>0.650</td>\n",
       "      <td>11.9060</td>\n",
       "      <td>8.0178</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>13035.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29833</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>13.734</td>\n",
       "      <td>37.120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>14.7840</td>\n",
       "      <td>9.7270</td>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>13035.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29834</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.452</td>\n",
       "      <td>33.655</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>13.4040</td>\n",
       "      <td>10.9732</td>\n",
       "      <td>2016-11-30</td>\n",
       "      <td>13035.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29835</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>11.099</td>\n",
       "      <td>29.996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>11.9470</td>\n",
       "      <td>10.9732</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>13035.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29836</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.672</td>\n",
       "      <td>30.415</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717</td>\n",
       "      <td>1.714</td>\n",
       "      <td>2.451</td>\n",
       "      <td>2.451</td>\n",
       "      <td>0.665</td>\n",
       "      <td>13.1570</td>\n",
       "      <td>18.0835</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>13035.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25468 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bm      ps     pcf     dpr    npm    gpm    cfm    roa     roe  \\\n",
       "0      0.4653   0.113  -5.253  3.6490 -0.072  0.163 -0.018  0.024 -1.5750   \n",
       "1      0.3733   0.153   3.287  4.8817 -0.074  0.150 -0.018  0.010 -1.8864   \n",
       "2      0.3733   0.152   3.258  4.8817 -0.074  0.150 -0.018  0.010 -1.8864   \n",
       "3      0.3733   0.123   2.640  4.8817 -0.074  0.150 -0.018  0.010 -1.8864   \n",
       "4      0.4105   0.127   2.753  4.4286 -0.079  0.150 -0.025  0.009 -1.8864   \n",
       "...       ...     ...     ...     ...    ...    ...    ...    ...     ...   \n",
       "29832  0.0780  11.083  28.586  0.0000  0.243  0.982  0.279  0.258  0.2820   \n",
       "29833  0.0740  13.734  37.120  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29834  0.0740  12.452  33.655  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29835  0.0740  11.099  29.996  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29836  0.0740  12.672  30.415  0.0000  0.212  0.978  0.253  0.240  0.2500   \n",
       "\n",
       "        roce  ...  de_ratio  cash_ratio  quick_ratio  curr_ratio  at_turn  \\\n",
       "0     -0.058  ...    -9.366       0.428        0.603       0.664    0.816   \n",
       "1     -0.092  ...    -8.291       0.629        0.787       0.859    0.787   \n",
       "2     -0.092  ...    -8.291       0.629        0.787       0.859    0.787   \n",
       "3     -0.092  ...    -8.291       0.629        0.787       0.859    0.787   \n",
       "4     -0.110  ...    -8.617       0.551        0.712       0.780    0.799   \n",
       "...      ...  ...       ...         ...          ...         ...      ...   \n",
       "29832  0.268  ...     0.728       1.982        2.851       2.851    0.650   \n",
       "29833  0.250  ...     0.704       1.986        2.595       2.595    0.682   \n",
       "29834  0.250  ...     0.704       1.986        2.595       2.595    0.682   \n",
       "29835  0.250  ...     0.704       1.986        2.595       2.595    0.682   \n",
       "29836  0.250  ...     0.717       1.714        2.451       2.451    0.665   \n",
       "\n",
       "           ptb  PEG_trailing  public_date   permno  splticrm  \n",
       "0       6.3546       -3.1471   2010-01-31  21020.0        B-  \n",
       "1       9.2910       -2.5938   2010-02-28  21020.0        B-  \n",
       "2       9.2910       -2.5938   2010-03-31  21020.0        B-  \n",
       "3       9.2056       -2.5938   2010-04-30  21020.0        B-  \n",
       "4       9.6632       -2.5376   2010-05-31  21020.0        B-  \n",
       "...        ...           ...          ...      ...       ...  \n",
       "29832  11.9060        8.0178   2016-09-30  13035.0       BBB  \n",
       "29833  14.7840        9.7270   2016-10-31  13035.0       BBB  \n",
       "29834  13.4040       10.9732   2016-11-30  13035.0       BBB  \n",
       "29835  11.9470       10.9732   2016-12-31  13035.0       BBB  \n",
       "29836  13.1570       18.0835   2017-01-31  13035.0       BBB  \n",
       "\n",
       "[25468 rows x 41 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the time has come to train our prediction model. We assume that including all variables will lead to overfitting, and using just a subset of all variables should improve the result. We test that assumption by trying out both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputed.loc[:, :'PEG_trailing']   # using all variables at our disposal\n",
    "y = pd.factorize(imputed.loc[:, 'splticrm'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['roe', 'curr_ratio', 'bm', 'de_ratio', 'dpr', 'at_turn', 'debt_ebitda']   # using just a subset\n",
    "X = imputed.loc[:, subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.95\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing both models, it turns out that using all variables is actually more helpful than using just a subset of variables. Apparently, random forest is not very susceptible to overfitting. Next, we try a support vector classifier (SVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputed.loc[:, :'PEG_trailing']   # using all variables at our disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.94\n",
      "Test score:       0.95\n",
      "Best parameters: {'classifier': SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
      "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False), 'classifier__C': 100, 'classifier__gamma': 1, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and SVC estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [SVC(kernel='rbf')],\n",
    "               'classifier__gamma': [1, 10],\n",
    "               'classifier__C': [10, 100]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['roe', 'curr_ratio', 'bm', 'de_ratio', 'dpr', 'at_turn', 'debt_ebitda']   # using just a subset\n",
    "X = imputed.loc[:, subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.91\n",
      "Test score:       0.93\n",
      "Best parameters: {'classifier': SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=10, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 10, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and SVC estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [SVC(kernel='rbf')],\n",
    "               'classifier__gamma': [1, 10],\n",
    "               'classifier__C': [10, 100]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for both the entire dataset and the subset, random forest works better than SVC. So from now on, we will only use random forest and the entire set of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are interested in how earlier data compares to more recent data in their predictive power. To do this, we split the dataset in two halves, one half containing all observations until the end of June 2013, the other half containing all later observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsthalf = imputed.set_index('public_date')\n",
    "firsthalf.sort_index()\n",
    "firsthalf = firsthalf.loc['20100131':'20130701']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_assets</th>\n",
       "      <th>debt_capital</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>0.1346</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>4.2058</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.5750</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.338</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>6.1151</td>\n",
       "      <td>-2.0547</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.153</td>\n",
       "      <td>3.287</td>\n",
       "      <td>4.2556</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.5493</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.137</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>5.8324</td>\n",
       "      <td>-2.0973</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31</th>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.152</td>\n",
       "      <td>3.258</td>\n",
       "      <td>4.2556</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.5493</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.137</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>5.8324</td>\n",
       "      <td>-2.0973</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <td>0.1282</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.640</td>\n",
       "      <td>4.2556</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.5493</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.137</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>5.8324</td>\n",
       "      <td>-2.0973</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-31</th>\n",
       "      <td>0.1193</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.753</td>\n",
       "      <td>5.9203</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-5.0948</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>1.131</td>\n",
       "      <td>1.366</td>\n",
       "      <td>-8.617</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>5.6513</td>\n",
       "      <td>-1.9089</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-28</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.228</td>\n",
       "      <td>13.788</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.0580</td>\n",
       "      <td>1.7372</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-31</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.384</td>\n",
       "      <td>15.541</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.3200</td>\n",
       "      <td>1.9944</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-30</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.702</td>\n",
       "      <td>19.114</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.555</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.8530</td>\n",
       "      <td>1.8227</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-31</th>\n",
       "      <td>0.4520</td>\n",
       "      <td>1.690</td>\n",
       "      <td>19.380</td>\n",
       "      <td>0.3610</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.423</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.537</td>\n",
       "      <td>1.789</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.809</td>\n",
       "      <td>2.7350</td>\n",
       "      <td>3.3768</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-30</th>\n",
       "      <td>0.4520</td>\n",
       "      <td>1.692</td>\n",
       "      <td>19.402</td>\n",
       "      <td>0.3610</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.554</td>\n",
       "      <td>0.423</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.537</td>\n",
       "      <td>1.789</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.809</td>\n",
       "      <td>2.7380</td>\n",
       "      <td>3.3768</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12107 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bm     ps     pcf     dpr    npm    gpm    cfm    roa  \\\n",
       "public_date                                                              \n",
       "2010-01-31   0.1346  0.113  -5.253  4.2058 -0.072  0.163 -0.018  0.024   \n",
       "2010-02-28   0.1282  0.153   3.287  4.2556 -0.074  0.150 -0.018  0.010   \n",
       "2010-03-31   0.1282  0.152   3.258  4.2556 -0.074  0.150 -0.018  0.010   \n",
       "2010-04-30   0.1282  0.123   2.640  4.2556 -0.074  0.150 -0.018  0.010   \n",
       "2010-05-31   0.1193  0.127   2.753  5.9203 -0.079  0.150 -0.025  0.009   \n",
       "...             ...    ...     ...     ...    ...    ...    ...    ...   \n",
       "2013-02-28   0.4470  1.228  13.788  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-03-31   0.4470  1.384  15.541  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-04-30   0.4470  1.702  19.114  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-05-31   0.4520  1.690  19.380  0.3610  0.062  0.426  0.101  0.124   \n",
       "2013-06-30   0.4520  1.692  19.402  0.3610  0.062  0.426  0.101  0.124   \n",
       "\n",
       "                roe   roce  ...  debt_assets  debt_capital  de_ratio  \\\n",
       "public_date                 ...                                        \n",
       "2010-01-31  -1.5750 -0.058  ...        1.120         1.338    -9.366   \n",
       "2010-02-28  -1.5493 -0.092  ...        1.137         1.376    -8.291   \n",
       "2010-03-31  -1.5493 -0.092  ...        1.137         1.376    -8.291   \n",
       "2010-04-30  -1.5493 -0.092  ...        1.137         1.376    -8.291   \n",
       "2010-05-31  -5.0948 -0.110  ...        1.131         1.366    -8.617   \n",
       "...             ...    ...  ...          ...           ...       ...   \n",
       "2013-02-28   0.1050  0.132  ...        0.555         0.426     1.248   \n",
       "2013-03-31   0.1050  0.132  ...        0.555         0.426     1.248   \n",
       "2013-04-30   0.1050  0.132  ...        0.555         0.426     1.248   \n",
       "2013-05-31   0.1040  0.135  ...        0.554         0.423     1.242   \n",
       "2013-06-30   0.1040  0.135  ...        0.554         0.423     1.242   \n",
       "\n",
       "             cash_ratio  quick_ratio  curr_ratio  at_turn     ptb  \\\n",
       "public_date                                                         \n",
       "2010-01-31        0.428        0.603       0.664    0.816  6.1151   \n",
       "2010-02-28        0.629        0.787       0.859    0.787  5.8324   \n",
       "2010-03-31        0.629        0.787       0.859    0.787  5.8324   \n",
       "2010-04-30        0.629        0.787       0.859    0.787  5.8324   \n",
       "2010-05-31        0.551        0.712       0.780    0.799  5.6513   \n",
       "...                 ...          ...         ...      ...     ...   \n",
       "2013-02-28        0.550        1.787       2.396    0.810  2.0580   \n",
       "2013-03-31        0.550        1.787       2.396    0.810  2.3200   \n",
       "2013-04-30        0.550        1.787       2.396    0.810  2.8530   \n",
       "2013-05-31        0.537        1.789       2.404    0.809  2.7350   \n",
       "2013-06-30        0.537        1.789       2.404    0.809  2.7380   \n",
       "\n",
       "             PEG_trailing  splticrm  \n",
       "public_date                          \n",
       "2010-01-31        -2.0547        B-  \n",
       "2010-02-28        -2.0973        B-  \n",
       "2010-03-31        -2.0973        B-  \n",
       "2010-04-30        -2.0973        B-  \n",
       "2010-05-31        -1.9089        B-  \n",
       "...                   ...       ...  \n",
       "2013-02-28         1.7372       BBB  \n",
       "2013-03-31         1.9944       BBB  \n",
       "2013-04-30         1.8227       BBB  \n",
       "2013-05-31         3.3768       BBB  \n",
       "2013-06-30         3.3768       BBB  \n",
       "\n",
       "[12107 rows x 39 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsthalf   # checking if splitting the data worked as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = firsthalf.loc[:, :'PEG_trailing']\n",
    "y = firsthalf.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.98\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondhalf = imputed.set_index('public_date')\n",
    "secondhalf.sort_index()\n",
    "secondhalf = secondhalf.loc['20130701':'20170201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = secondhalf.loc[:, :'PEG_trailing']\n",
    "y = secondhalf.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our surprise, it seems like the more recent data is worse at predicting the long term credit ratings. We wonder if this means that using lagged data would improve our prediction of them. We try this out, and we start by lagging all data by one month, then half a year, then one year, and finally three and a half years (which is basically the same as using the independent variables in firsthalf to predict the dependent variable in second half)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged = imputed.set_index(['public_date', 'permno']) # we need these indeces to instruct unstack().shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_month_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(1)\n",
    "# lagged.loc[:, :'PEG_trailing'] because we want to shift everything BUT splticrm\n",
    "# unstack() makes sure that the data is shifted within its group only, defined by permno\n",
    "# the 1 in shift() is for 1 month\n",
    "one_month_lag = one_month_lag.stack(dropna=False) # stack back together\n",
    "one_month_lag = pd.concat([one_month_lag, lagged.loc[:, 'splticrm']], axis = 'columns') # adding splticrm again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we added splticrm we need to get rid of the first month, for which shift created NAs\n",
    "one_month_lag = one_month_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_assets</th>\n",
       "      <th>debt_capital</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_date</th>\n",
       "      <th>permno</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2010-02-28</th>\n",
       "      <th>10104.0</th>\n",
       "      <td>0.2330</td>\n",
       "      <td>4.692</td>\n",
       "      <td>13.871</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.322</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.2180</td>\n",
       "      <td>0.242</td>\n",
       "      <td>...</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.982</td>\n",
       "      <td>1.739</td>\n",
       "      <td>2.2370</td>\n",
       "      <td>2.2450</td>\n",
       "      <td>0.453</td>\n",
       "      <td>3.9370</td>\n",
       "      <td>1.3480</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10107.0</th>\n",
       "      <td>0.1810</td>\n",
       "      <td>3.797</td>\n",
       "      <td>10.135</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.4250</td>\n",
       "      <td>0.490</td>\n",
       "      <td>...</td>\n",
       "      <td>0.477</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.912</td>\n",
       "      <td>1.334</td>\n",
       "      <td>1.9040</td>\n",
       "      <td>1.9320</td>\n",
       "      <td>0.729</td>\n",
       "      <td>4.8590</td>\n",
       "      <td>1.1190</td>\n",
       "      <td>AAA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10145.0</th>\n",
       "      <td>0.2760</td>\n",
       "      <td>1.061</td>\n",
       "      <td>7.537</td>\n",
       "      <td>0.415</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.298</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.2410</td>\n",
       "      <td>0.239</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747</td>\n",
       "      <td>0.560</td>\n",
       "      <td>2.982</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.9420</td>\n",
       "      <td>1.2620</td>\n",
       "      <td>0.864</td>\n",
       "      <td>3.4290</td>\n",
       "      <td>3.3700</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10516.0</th>\n",
       "      <td>0.8160</td>\n",
       "      <td>0.260</td>\n",
       "      <td>7.171</td>\n",
       "      <td>0.237</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.1070</td>\n",
       "      <td>0.086</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.496</td>\n",
       "      <td>1.258</td>\n",
       "      <td>0.482</td>\n",
       "      <td>1.2650</td>\n",
       "      <td>2.1550</td>\n",
       "      <td>1.934</td>\n",
       "      <td>1.0710</td>\n",
       "      <td>2.7490</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10696.0</th>\n",
       "      <td>0.4780</td>\n",
       "      <td>1.786</td>\n",
       "      <td>7.206</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.497</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.568</td>\n",
       "      <td>1.940</td>\n",
       "      <td>0.247</td>\n",
       "      <td>1.1930</td>\n",
       "      <td>1.1930</td>\n",
       "      <td>0.465</td>\n",
       "      <td>1.9610</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2017-01-31</th>\n",
       "      <th>88853.0</th>\n",
       "      <td>0.1929</td>\n",
       "      <td>0.355</td>\n",
       "      <td>2.724</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.7301</td>\n",
       "      <td>0.031</td>\n",
       "      <td>...</td>\n",
       "      <td>1.033</td>\n",
       "      <td>1.057</td>\n",
       "      <td>-25.750</td>\n",
       "      <td>0.387</td>\n",
       "      <td>1.5881</td>\n",
       "      <td>1.7302</td>\n",
       "      <td>0.573</td>\n",
       "      <td>4.7919</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>A-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88873.0</th>\n",
       "      <td>0.7940</td>\n",
       "      <td>1.213</td>\n",
       "      <td>10.710</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.387</td>\n",
       "      <td>0.089</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.0850</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.493</td>\n",
       "      <td>1.678</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.7570</td>\n",
       "      <td>1.1000</td>\n",
       "      <td>0.560</td>\n",
       "      <td>1.1950</td>\n",
       "      <td>12.9510</td>\n",
       "      <td>BB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89006.0</th>\n",
       "      <td>0.1230</td>\n",
       "      <td>1.755</td>\n",
       "      <td>7.633</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.4020</td>\n",
       "      <td>0.141</td>\n",
       "      <td>...</td>\n",
       "      <td>0.968</td>\n",
       "      <td>0.956</td>\n",
       "      <td>30.546</td>\n",
       "      <td>0.390</td>\n",
       "      <td>1.5210</td>\n",
       "      <td>1.5210</td>\n",
       "      <td>0.383</td>\n",
       "      <td>9.1170</td>\n",
       "      <td>2.2050</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89070.0</th>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.358</td>\n",
       "      <td>14.380</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.082</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.2000</td>\n",
       "      <td>0.155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.533</td>\n",
       "      <td>0.422</td>\n",
       "      <td>1.139</td>\n",
       "      <td>1.334</td>\n",
       "      <td>1.8210</td>\n",
       "      <td>1.8210</td>\n",
       "      <td>1.210</td>\n",
       "      <td>0.8110</td>\n",
       "      <td>0.1160</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89179.0</th>\n",
       "      <td>0.1060</td>\n",
       "      <td>16.135</td>\n",
       "      <td>33.609</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.238</td>\n",
       "      <td>0.916</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.1650</td>\n",
       "      <td>0.285</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.142</td>\n",
       "      <td>10.925</td>\n",
       "      <td>12.2810</td>\n",
       "      <td>12.2810</td>\n",
       "      <td>0.551</td>\n",
       "      <td>8.8060</td>\n",
       "      <td>7.8145</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25133 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         bm      ps     pcf    dpr    npm    gpm    cfm  \\\n",
       "public_date permno                                                        \n",
       "2010-02-28  10104.0  0.2330   4.692  13.871  0.177  0.234  0.816  0.322   \n",
       "            10107.0  0.1810   3.797  10.135  0.266  0.290  0.846  0.335   \n",
       "            10145.0  0.2760   1.061   7.537  0.415  0.072  0.298  0.102   \n",
       "            10516.0  0.8160   0.260   7.171  0.237  0.025  0.066  0.038   \n",
       "            10696.0  0.4780   1.786   7.206  0.000  0.121  0.497  0.204   \n",
       "...                     ...     ...     ...    ...    ...    ...    ...   \n",
       "2017-01-31  88853.0  0.1929   0.355   2.724  0.000  0.023  0.190  0.093   \n",
       "            88873.0  0.7940   1.213  10.710  0.794  0.059  0.387  0.089   \n",
       "            89006.0  0.1230   1.755   7.633  0.000  0.084  0.351  0.145   \n",
       "            89070.0  0.9730   0.358  14.380  0.000  0.082  0.232  0.092   \n",
       "            89179.0  0.1060  16.135  33.609  0.000  0.238  0.916  0.345   \n",
       "\n",
       "                       roa     roe   roce  ...  debt_assets  debt_capital  \\\n",
       "public_date permno                         ...                              \n",
       "2010-02-28  10104.0  0.227  0.2180  0.242  ...        0.493         0.346   \n",
       "            10107.0  0.321  0.4250  0.490  ...        0.477         0.177   \n",
       "            10145.0  0.141  0.2410  0.239  ...        0.747         0.560   \n",
       "            10516.0  0.086  0.1070  0.086  ...        0.557         0.496   \n",
       "            10696.0  0.144  0.1430  0.143  ...        0.660         0.568   \n",
       "...                    ...     ...    ...  ...          ...           ...   \n",
       "2017-01-31  88853.0  0.059 -0.7301  0.031  ...        1.033         1.057   \n",
       "            88873.0  0.106  0.0850  0.132  ...        0.625         0.493   \n",
       "            89006.0  0.121  0.4020  0.141  ...        0.968         0.956   \n",
       "            89070.0  0.106  0.2000  0.155  ...        0.533         0.422   \n",
       "            89179.0  0.321  0.1650  0.285  ...        0.124         0.032   \n",
       "\n",
       "                     de_ratio  cash_ratio  quick_ratio  curr_ratio  at_turn  \\\n",
       "public_date permno                                                            \n",
       "2010-02-28  10104.0     0.982       1.739       2.2370      2.2450    0.453   \n",
       "            10107.0     0.912       1.334       1.9040      1.9320    0.729   \n",
       "            10145.0     2.982       0.245       0.9420      1.2620    0.864   \n",
       "            10516.0     1.258       0.482       1.2650      2.1550    1.934   \n",
       "            10696.0     1.940       0.247       1.1930      1.1930    0.465   \n",
       "...                       ...         ...          ...         ...      ...   \n",
       "2017-01-31  88853.0   -25.750       0.387       1.5881      1.7302    0.573   \n",
       "            88873.0     1.678       0.217       0.7570      1.1000    0.560   \n",
       "            89006.0    30.546       0.390       1.5210      1.5210    0.383   \n",
       "            89070.0     1.139       1.334       1.8210      1.8210    1.210   \n",
       "            89179.0     0.142      10.925      12.2810     12.2810    0.551   \n",
       "\n",
       "                        ptb  PEG_trailing  splticrm  \n",
       "public_date permno                                   \n",
       "2010-02-28  10104.0  3.9370        1.3480         A  \n",
       "            10107.0  4.8590        1.1190       AAA  \n",
       "            10145.0  3.4290        3.3700         A  \n",
       "            10516.0  1.0710        2.7490         A  \n",
       "            10696.0  1.9610        0.2650       BBB  \n",
       "...                     ...           ...       ...  \n",
       "2017-01-31  88853.0  4.7919        0.2600        A-  \n",
       "            88873.0  1.1950       12.9510       BB+  \n",
       "            89006.0  9.1170        2.2050       BBB  \n",
       "            89070.0  0.8110        0.1160       BBB  \n",
       "            89179.0  8.8060        7.8145         A  \n",
       "\n",
       "[25133 rows x 39 columns]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_month_lag   # checking that everything worked as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_month_lag.loc[:, :'PEG_trailing']\n",
    "y = one_month_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_year_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(6)\n",
    "half_year_lag = half_year_lag.stack(dropna=False)\n",
    "half_year_lag = pd.concat([half_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_year_lag = half_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = half_year_lag.loc[:, :'PEG_trailing']\n",
    "y = half_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(12)\n",
    "one_year_lag = one_year_lag.stack(dropna=False)\n",
    "one_year_lag = pd.concat([one_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_lag = one_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_year_lag.loc[:, :'PEG_trailing']\n",
    "y = one_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "threehalf_year_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(42)\n",
    "threehalf_year_lag = threehalf_year_lag.stack(dropna=False)\n",
    "threehalf_year_lag = pd.concat([threehalf_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "threehalf_year_lag = threehalf_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = threehalf_year_lag.loc[:, :'PEG_trailing']\n",
    "y = threehalf_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, all models with lagged data perform equally well, and while they are not particularly bad, they are a slight downgrade from using recent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we try out what happens when we use a negative lag of one year. This means that we try to predict credit ratings by data that will only come out a year later, so we do not expect this model to perform well. After all, the existing ratios are supposed to influence the credit rating, and the future ratios do not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(-12)\n",
    "reverse_lag = reverse_lag.stack(dropna=False)\n",
    "reverse_lag = pd.concat([reverse_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_lag = reverse_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reverse_lag.loc[:, :'PEG_trailing']\n",
    "y = reverse_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our big surprise, this model actually performs better than using (correctly) lagged data, and it performs equally well as using no lagging of any kind. Since no-one can know the future ratings in advance, we believe this means that not only do the ratios affect the credit ratings, but the credit ratings must in turn affect the future ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
