{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by loading and then merging the datasets we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv('key.csv', sep=',')\n",
    "dat2 = pd.read_csv('SP500_finratios.csv', sep=',', parse_dates=['adate', 'qdate', 'public_date'])\n",
    "dat3 = pd.read_csv('ratings2.csv', sep=',', parse_dates=['datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the columns of the first data set are renamed to match the names of the other data sets\n",
    "dat1.columns = ['gvkey','linktype','permno','permco','linkdt','linkenddt','conm','tic','cusip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the duplicates in the key data set are removed, so that pd.merge() will work\n",
    "#these duplicates come from differences in the variables linkdt and linkenddt, which we don't need\n",
    "dat1 = dat1.set_index('permno')\n",
    "dat1 = dat1[~dat1.index.duplicated(keep='first')]\n",
    "dat1.reset_index(inplace=True)   #The permnos are converted back to a normal variable, otherwise an error can occur when merging on permno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the variable datadate is renamed public_date and both are transformed to the same format, so\n",
    "# that pd.merge()recognises them as one and the same\n",
    "dat3['public_date'] = dat3['datadate']\n",
    "del dat3['datadate']\n",
    "dat2['public_date'] = pd.to_datetime(dat2.public_date)\n",
    "dat3['public_date'] = pd.to_datetime(dat3.public_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1and2 = pd.merge(dat1, dat2, on='permno', how='inner', validate='one_to_many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.merge(dat1and2, dat3, on=['gvkey', 'public_date', 'conm', 'tic', 'cusip'], how='inner', validate='one_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>linktype</th>\n",
       "      <th>permco</th>\n",
       "      <th>linkdt</th>\n",
       "      <th>linkenddt</th>\n",
       "      <th>conm</th>\n",
       "      <th>tic</th>\n",
       "      <th>cusip</th>\n",
       "      <th>adate</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_capital</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>DIVYIELD</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.338</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366</td>\n",
       "      <td>-8.617</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29853</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.741</td>\n",
       "      <td>1.478</td>\n",
       "      <td>1.961</td>\n",
       "      <td>1.961</td>\n",
       "      <td>0.726</td>\n",
       "      <td>9.038</td>\n",
       "      <td>9.364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29854</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>8.184</td>\n",
       "      <td>10.913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29855</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>9.392</td>\n",
       "      <td>12.469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29856</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>8.557</td>\n",
       "      <td>11.359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29857</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.789</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.944</td>\n",
       "      <td>1.944</td>\n",
       "      <td>0.681</td>\n",
       "      <td>7.114</td>\n",
       "      <td>11.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29858 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       permno   gvkey linktype  permco    linkdt linkenddt  \\\n",
       "0       21020    1045       LC   20010  19500101  19620130   \n",
       "1       21020    1045       LC   20010  19500101  19620130   \n",
       "2       21020    1045       LC   20010  19500101  19620130   \n",
       "3       21020    1045       LC   20010  19500101  19620130   \n",
       "4       21020    1045       LC   20010  19500101  19620130   \n",
       "...       ...     ...      ...     ...       ...       ...   \n",
       "29853   13168  199356       LC   53964  20111221         E   \n",
       "29854   13168  199356       LC   53964  20111221         E   \n",
       "29855   13168  199356       LC   53964  20111221         E   \n",
       "29856   13168  199356       LC   53964  20111221         E   \n",
       "29857   13168  199356       LC   53964  20111221         E   \n",
       "\n",
       "                              conm   tic      cusip      adate  ...  \\\n",
       "0      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2008-12-31  ...   \n",
       "1      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "2      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "3      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "4      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "...                            ...   ...        ...        ...  ...   \n",
       "29853              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29854              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29855              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29856              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29857              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "\n",
       "      debt_capital de_ratio  cash_ratio  quick_ratio  curr_ratio  at_turn  \\\n",
       "0            1.338   -9.366       0.428        0.603       0.664    0.816   \n",
       "1            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "2            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "3            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "4            1.366   -8.617       0.551        0.712       0.780    0.799   \n",
       "...            ...      ...         ...          ...         ...      ...   \n",
       "29853        0.286    0.741       1.478        1.961       1.961    0.726   \n",
       "29854        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29855        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29856        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29857        0.289    0.789       1.396        1.944       1.944    0.681   \n",
       "\n",
       "         ptb  PEG_trailing  DIVYIELD  splticrm  \n",
       "0        NaN           NaN       NaN        B-  \n",
       "1        NaN           NaN       NaN        B-  \n",
       "2        NaN           NaN       NaN        B-  \n",
       "3        NaN           NaN       NaN        B-  \n",
       "4        NaN           NaN       NaN        B-  \n",
       "...      ...           ...       ...       ...  \n",
       "29853  9.038         9.364       NaN       NaN  \n",
       "29854  8.184        10.913       NaN       NaN  \n",
       "29855  9.392        12.469       NaN       NaN  \n",
       "29856  8.557        11.359       NaN       NaN  \n",
       "29857  7.114        11.603       NaN       NaN  \n",
       "\n",
       "[29858 rows x 52 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's have a look at it. First of, we're interested in the distribution of the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BBB     4946\n",
       "BBB+    4133\n",
       "A-      3615\n",
       "A       3196\n",
       "BBB-    2654\n",
       "A+      1670\n",
       "BB+     1262\n",
       "AA-      902\n",
       "BB-      824\n",
       "BB       757\n",
       "AA       417\n",
       "B+       305\n",
       "AAA      296\n",
       "AA+      199\n",
       "B-       133\n",
       "B        109\n",
       "CCC+      50\n",
       "D          4\n",
       "CCC        2\n",
       "Name: splticrm, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['splticrm'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as there are only four observations of rating D, and only two observations of rating CCC, our data set does not allow us to draw any conclusions for these ratings and we have to drop them from our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat[dat['splticrm'] != 'CCC']\n",
    "dat = dat[dat['splticrm'] != 'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we consider our numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              bm          ps         pcf        dpr        npm        gpm  \\\n",
      "min     0.001000    0.047000 -224.460000  -0.001000 -51.493000 -37.707000   \n",
      "mean    0.511099    2.530784   12.210846   0.489392   0.080365   0.431986   \n",
      "50%     0.391000    1.814000   11.113000   0.305000   0.092000   0.406000   \n",
      "max   137.237000  145.774000  280.893000  80.554000   1.799000   0.982000   \n",
      "\n",
      "            cfm       roa        roe      roce     efftax     GProf  \\\n",
      "min  -47.694000 -0.595000 -34.647000 -1.111000 -12.365000 -1.143000   \n",
      "mean   0.152864  0.142624   0.166138  0.176364   0.290338  0.294441   \n",
      "50%    0.152500  0.139000   0.137000  0.154000   0.302000  0.263500   \n",
      "max    2.054000  0.626000  15.502000  2.279000  29.944000  1.255000   \n",
      "\n",
      "      equity_invcap  debt_invcap  totdebt_invcap  capital_ratio     int_debt  \\\n",
      "min      -17.816000     0.000000        0.000000      -5.619000     0.000000   \n",
      "mean       0.593529     0.389615        0.471293       0.395291     0.336795   \n",
      "50%        0.633000     0.348000        0.401000       0.353000     0.055000   \n",
      "max        1.000000    18.816000       21.164000      18.816000  1278.000000   \n",
      "\n",
      "      int_totdebt   cash_lt  \n",
      "min      0.000000  0.000000  \n",
      "mean     0.079749  0.311298  \n",
      "50%      0.048000  0.147000  \n",
      "max    115.262000  6.502000   \n",
      "\n",
      "      invt_act   debt_at  debt_ebitda  short_debt  curr_debt   lt_debt  \\\n",
      "min   0.000000  0.000000 -2487.500000    0.000000   0.030000  0.000000   \n",
      "mean  0.222505  0.254492     2.430452    0.158646   0.395143  0.345054   \n",
      "50%   0.194000  0.242000     1.813000    0.097000   0.363000  0.364000   \n",
      "max   5.389000  1.729000  2328.100000    5.712000   1.000000  1.408000   \n",
      "\n",
      "       ocf_lct  cash_debt    fcf_ocf     dltt_be  debt_assets  debt_capital  \\\n",
      "min  -3.037000  -0.916000 -53.180000    0.000000     0.043000      0.002000   \n",
      "mean  0.717987   0.230432   0.571597    0.880104     0.618524      0.499772   \n",
      "50%   0.613000   0.169000   0.725000    0.488000     0.617000      0.472000   \n",
      "max   4.559000   5.736000   2.033000  157.897000     1.919000      4.234000   \n",
      "\n",
      "         de_ratio  cash_ratio  quick_ratio  curr_ratio   at_turn        ptb  \\\n",
      "min  -1228.100000    0.001000      0.09100    0.113000  0.013000   0.181000   \n",
      "mean     2.908976    0.764793      1.49689    1.899965  0.814618   3.899753   \n",
      "50%      1.550000    0.441000      1.18000    1.573000  0.657000   2.629000   \n",
      "max   1818.100000   11.403000     12.82500   12.825000  5.912000  65.486000   \n",
      "\n",
      "      PEG_trailing  \n",
      "min     -33.082000  \n",
      "mean      2.819965  \n",
      "50%       1.167000  \n",
      "max     100.212000  \n"
     ]
    }
   ],
   "source": [
    "des = dat.loc[:, 'bm':'cash_lt'].describe()\n",
    "ind = [3, 1, 5, 7]   #printing the entire .describe() information consumes unnecessarily much computation power, so I index the lines I'm interested in\n",
    "print(des.iloc[ind], '\\n')\n",
    "des = dat.loc[:, 'invt_act':].describe()  #I do this in two steps, because I don't want any variables hidden behind \"...\"\n",
    "ind = [3, 1, 5, 7]\n",
    "print(des.iloc[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output suggests that several variables have extreme outliers - for instance bm has a minimum of 0.001000, a mean of  0.506463, but a maximum of 137.237000. Visualising the data with boxplots shows this quite notably. This means the imputation method we initially considered, which was based on linear regression, is probably not the best way to handle the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2eebc23ab00>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the outliers we show in these boxplots are so extreme that when we tried using linear regression for\n",
    "# the imputation, they influenced the slope so much that some negative values where imputed ...\n",
    "sns.boxplot(x = 'bm', data = dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2eebc2cd2e8>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x = 'int_debt', data = dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2eebd0fffd0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x = 'int_totdebt', data = dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the data we have, we take a look at the data we do not have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a little indulgence\n",
    "class color:\n",
    "   bold = '\\033[1m'\n",
    "   end = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mColumn Names         Total NAs      NAs per observations\u001b[0m\n",
      "permno               0              0.000000\n",
      "gvkey                0              0.000000\n",
      "linktype             0              0.000000\n",
      "permco               0              0.000000\n",
      "linkdt               0              0.000000\n",
      "linkenddt            0              0.000000\n",
      "conm                 0              0.000000\n",
      "tic                  0              0.000000\n",
      "cusip                0              0.000000\n",
      "adate                13             0.000435\n",
      "qdate                0              0.000000\n",
      "public_date          0              0.000000\n",
      "bm                   638            0.021372\n",
      "ps                   12             0.000402\n",
      "pcf                  27             0.000904\n",
      "dpr                  1862           0.062374\n",
      "npm                  12             0.000402\n",
      "gpm                  12             0.000402\n",
      "cfm                  98             0.003283\n",
      "roa                  27             0.000904\n",
      "roe                  666            0.022310\n",
      "roce                 109            0.003651\n",
      "efftax               2108           0.070615\n",
      "GProf                12             0.000402\n",
      "equity_invcap        18             0.000603\n",
      "debt_invcap          65             0.002177\n",
      "totdebt_invcap       82             0.002747\n",
      "capital_ratio        59             0.001976\n",
      "int_debt             3213           0.107631\n",
      "int_totdebt          3044           0.101970\n",
      "cash_lt              26             0.000871\n",
      "invt_act             4275           0.143206\n",
      "debt_at              76             0.002546\n",
      "debt_ebitda          162            0.005427\n",
      "short_debt           1146           0.038389\n",
      "curr_debt            4016           0.134530\n",
      "lt_debt              59             0.001976\n",
      "ocf_lct              4011           0.134363\n",
      "cash_debt            140            0.004690\n",
      "fcf_ocf              571            0.019128\n",
      "dltt_be              661            0.022143\n",
      "debt_assets          26             0.000871\n",
      "debt_capital         197            0.006599\n",
      "de_ratio             26             0.000871\n",
      "cash_ratio           4002           0.134061\n",
      "quick_ratio          4002           0.134061\n",
      "curr_ratio           4002           0.134061\n",
      "at_turn              27             0.000904\n",
      "ptb                  638            0.021372\n",
      "PEG_trailing         10354          0.346844\n",
      "DIVYIELD             6736           0.225647\n",
      "splticrm             4384           0.146858\n"
     ]
    }
   ],
   "source": [
    "col_Names = dat.columns.values\n",
    "total_NAs = pd.isna(dat).sum()\n",
    "percentage_NAs = dat.isna().sum()/len(dat)\n",
    "print(color.bold + \"%-20s %-14s %s\" %(\"Column Names\", \"Total NAs\", \"NAs per observations\") + color.end )\n",
    "#I used the % operator because tab didn't work and this allows me to define the spaces between the items\n",
    "\n",
    "#the loop prints one line after another\n",
    "for item_a, item_b, item_c in zip(col_Names, total_NAs, percentage_NAs):\n",
    "    print(\"%-20s %-14d %.6f\" %(item_a, item_b, item_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown here, 4384 observations have no long term credit rating (splticrm), which means we cannot use those observations for our prediction models. Still, the observations might be helpful for imputing missing values, so we will drop them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variable that stands out is PEG_trailing, and we are unsure how valid it is to impute more than one third of the data. However, dropping the missing values in PEG_trailing is not an option, since they are not missing completely at random: We can show that the missingness is systematic by plotting it against another variable. So, if we were to just drop the missing values, we would bias the remaining data. Instead, we could drop the entire variable, but we might as well keep it in for now, impute the missing values, and if we later find that it reduces the predictive power, we can still drop it. (Spoiler: It does not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2eebd176668>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deXhcd5ng++/vnNq127JUipd4jTc5BEdJCAQT4iyS6Qculy3QQ9N08yT0dDcw9z49wDTP3B7moRtm+t4hNHMvSacZ4BkeGJoGmumJnH0hkMXZJTm249ixLUelxdpVJdX23j9OVakkl6q0lFRa3s/z6JHqrL86qnPe+p33nPcYEUEppZTKxyp1A5RSSi1/GiyUUkoVpMFCKaVUQRoslFJKFaTBQimlVEGuUjdgMdTW1srWrVtL3QyllFpRXnzxxT4R2ZBr3KoMFlu3buWFF14odTOUUmpFMcacm2mcnoZSSilVkAYLpZRSBWmwUEopVZAGC6WUUgVpsFBKKVVQya+GMsZ8H/g9oEdEGnOMN8A9wBEgDPyhiLyUb5ntF4fY/bVWtteWsSdYzqMnehmLJijz2Hzupm184dareOJED/c+dYYLA2E21wQIVnp49EQvI+Nx8pVWtA0YAyKQyJqw0ufiiiofo9EEm2sC3Lh9Hd978k3CseSMy3Jb4Pe4iCWSRHJMt7nGjwCbawLcfWg732w9zonuscltA9ipcO9x2QQ8NrvqKrhx+zpa20Oc7hkllhQM4LIMVQE3g+Eo8axVuW1Dupikx7Zx2YZINM705rgtpgyr9Ln4zp3vBMhsx3KPzcXBMCMTud+zBQS8FtG4EE1oAcuVwjJwVV0Z5/vHCccSM05ngE01fpqurOZoR3fmM20ZSOb5d7sspnwmDUzZBwMeGwthNDrzvgSwp76MmjIfHW8PZfbjdJv+44cauXlPHcCUfX84EmN4PJ5zeel93WAwQDK1PLfLwu+2iEQTU/bvgNvi//39a3OuJ70Pp8fN5DuPnOL+p89edryarfnOn57PU7/j4EzTmFJXnTXGHAJGgR/NECyOAH+OEyxuAO4RkRvyLdPXsEu2/NE9xBNCEufD6rYNSXE+tB+8OsiL54dw2wa/26ZzIMxgJH7Zh3TO7wXYss7PRDxJaHhiAUuaVFfuocLv5nx/mFiBA6xtoNLvYigSR2Rh72U23Lahyu+myu8mnkhyrj+yyGtUKj/bTP0Sl1bld3HPJ5wvN//+1x24bUPXYCTvl7n5cNuGv/9005T1+N02kViCWEL4+gf3zxgwvvPIKe557DSWmQywSYEv3rJz1gf8+cyfPd/5f/hCciJ02s41XclPQ4nIU0B/nkk+hBNIRESeBaqNMQ35lmmMwbYssj8GlrFwWRaWgV+/FsJtGwIeF8aYzDeLhR5cBegbjTIywzeV+egbixLwuAoGCoPzwRiKxEkuQaAAiCWE0Yk4AY+LvtHoEqxRqfxy7SYGGI7EufepM9z71JnMvj/fQGHyjIsl5LL1GOP8dtuGe586M+O89z99FsuQOk5NHq/uf/rsrNo13/mz58un5KehZmEjcCHrdWdqWFf2RMaYu4C7AFyVUyN3dufJMs4/1O+eDJ75ushzFU0U95vKrNuWOjW21B3FRKqBxX7fShWTAJ0DYQSo9rsXtrACpyBmWk/6LMZMxqIJXNOO15Zxhs/GfOfPNV8uJe9ZzEKuQH7Zv0pE7hORJhFpcpVVTV1A1hKSArZliGSde7XyfVWYI49t4bGLt1ln3TZxNlQx38ts2KkVFvM9K1VsTu4iwOaawJR9f14KfCGbaT2RWIJNNYEZ5yvz2Jd9OUyKM3w25jt/rvlyWQl7eCewOev1JuDtfDOICIlkcsqbS0qSeDKZyVnEEkI4GkdEqPQ5HayFHmcNUFvuocJXvA5bbZmHcDSO287fOsEJFFV+F5ZZ+HuZDbdtKPe6CEfj1JZ7lmCNSuWXazcRnFze3Ye2c/eh7Zl9P+Ce3+Ev33HVbZvL1iPi/I4lhLsPbZ9x3s/dtI2kkDpOTR6vPnfTtlm1a77zZ8+Xz0oIFr8G/sA43gUMiUhXoZmMMewOVvDhaxoo97qIJ51u4Bdv2cl/ufMgX//gfuoqfAxFYuxtqOLD1zRQ4XMVPMjaxrl6Y/qHstLnYnd9OUmBbbXl/B+37ir4YXRbznz+GabbXOPH67apq/Dx959uYk992dT3iNMWl+VcMVJT5mFPsIovHd7FnmAF7tS3fgO4LUNtueey7qbbNpPLcNtU+lzkas70YZU+F3//6Sb+9qPvoK7CR1JgT305Fd6Z37MFVHgtPAUCn1peLONcaRRw5/+GanA+sx++pmHKZ7pQb3f6Z3L65AGPTbmn8KFqT30Z129bT2XWfpxu0z2feCc376nj5j11mX1/fbk380Uxl/S+7rYMHsvgTv0EPDa1Ze7L9u+A2+LvP9102XqGIjHqKnx5k9sAX7j1Kr54y078bnvK8Wq2V0PNd/7s+cjzPXM5XA31E+BmoBboBv4vwA0gIt9LXTr7XaAZ59LZz4pI3iqBTU1NooUElVJqbowxL4pIU65xJU9wi8gnC4wX4E+XqDlKKaVyWAmnoZRSSpWYBgullFIFabBQSilVkAYLpZRSBWmwUEopVZAGC6WUUgVpsFBKKVWQBgullFIFabBQSilVkAYLpZRSBWmwUEopVZAGC6WUUgVpsFBKKVWQBgullFIFabBQSilVkAYLpZRSBWmwUEopVZAGC6WUUgVpsFBKKVWQBgullFIFabBQSilVkAYLpZRSBWmwUEopVZAGC6WUUgVpsFBKKVVQyYOFMabZGHPSGHPaGPOVHOOrjDH/0xjzqjGmwxjz2VK0Uyml1rKSBgtjjA38V6AF2Ad80hizb9pkfwocF5F3ADcD/7cxxrOkDVVKqTWu1D2L64HTInJGRKLAT4EPTZtGgApjjAHKgX4gvrTNVEqpta3UwWIjcCHrdWdqWLbvAnuBt4E24Isikpy+IGPMXcaYF4wxL/T29i5We5VSak0qdbAwOYbJtNd3AK8AVwDXAN81xlReNpPIfSLSJCJNtbUbiCcuiydKKaXmqdTBohPYnPV6E04PIttngV+I4zRwFtiTb6EJES4MROgbnSCmQUMppRas1MHiGLDLGLMtlbS+E/j1tGnOA4cBjDH1wG7gTKEFiwjDkRidAxF6RsaJxjVoKKXUfLlKuXIRiRtj/gx4ELCB74tIhzHm86nx3wP+I/ADY0wbzmmrL4tI3xzWweh4nNHxOOVeF1UBN16XvQjvRimlVq+SBgsAEXkAeGDasO9l/f02cHsx1jU6EWd0Ik7A46I64Mbn1qChlFKzUfJgUQrhaJxwNI7fY1MT8GjQUEqpAtZksEiLRBNEohF8bpvqgJuAZ01vDqWUmpEeHYHxWILQUAKv26ba76bMq5tFKaWy6VExy0QsQXcsgcdlUR3wUK5BQymlAA0WOUXjSXqGxxmwLaoCbiq8LpxqI0optTaV+j6LZS2WSNI3MsGF/ghDkRgi028uV0qptUF7FrMQTya5NDrBUDhGld9Nhc+FZWlPQym1dmiwmIN4MsmlsQkGI1EqfW6q/G4NGkqpNUGDxTwkksJAOMpQJEal3wkatgYNpdQqpsFiAZIiDKaDhs9Fld+Ny9Y0kFJq9VmVR7a3+sb4X691MTaxNM9IEhGGIjEuDEToHdFKt0qp1cesxit8vA27pOEz38bnsnjf7g20NAY5sLFqyS5/NcZQ5rWp9nvwuFZlPFZKrULGmBdFpCnXuFV5GqrK78bjshiPJ3mwo5sHO7rZWO2npTHI7fvrqS33Lur6tdKtUmq1WZU9i2sOXis/+udHeexkD61tIU52j2TGWQau37aO5sYgN25fj3uJcgxa6VYptdzl61ms2mDxiwefzLw+0zvK0Y4QDx/vYSgSywyv9ru5bV89zY1BttWWLUnb/B7n9JTfo0FDKbW8rPlgkRZLJHnmzUu0toc49lY/yay3vrehgpbGIDfvrluSmlBa6VYptdxosMihd2SCh46HaG0P8fbgeGa412Vx6KoNHGkMcvWmxU+Ka6VbpdRyocEiDxHhtc4hWttDPHmql4msZ3VfUe1zkuL7gmyoWNykuNu2qCnTSrdKqdLRYDFLoxNxnjjZQ2t7iNe7pibFm7au40hjkBt3LG5S3K2VbpVSJaLBYh7O9o1xtD3EQ8e7pyTFq/xubt1bx5EDDYuaFHdZTtCo9GnQUEotDQ0WCxBLJHnmzCWOtod4/uzUpPjuoJMUv2XP4iXFXZallW6VUktCg0WR9I1O8FBHN0c7QnQORDLDPS6LQ7tqaWkM8o7N1ViL0BOwLaOVbpVSi0qDRZGJCG0XU0nxk72MZyXFG6p8NDcGuWNfPXWVvqKv2zJGK90qpRaFBotFNDYR54mTvbS2d3E8KylugOu21tDc2MC7d6wveo0oY4xWulVKFZUGiyXy1iUnKf7w8W4GwpNJ8Uqfi1v31tNyIMiODeVFXacxhnKvU0pkqUqXKKVWp2UdLIwxzcA9gA3cLyLfzDHNzcC3ATfQJyLvy7fMUgWLtHgiyXNn+3mgLcRzZy9NSYpfVV9OS2OQw3vqKfcVLymulW6VUgu1bIOFMcYGTgG3AZ3AMeCTInI8a5pq4HdAs4icN8bUiUhPvuWWOlhkuzQ6wUPHu2ltvzwp/t6dTlL8mi3FTYprpVul1Hws52BxI/BXInJH6vVXAUTkb7Km+dfAFSLytdku99qmJnnkyd8RjiWYiCWK3ex5ERE63h6mtT3E4yd7GI9NJsWDlT6aG+u5Y3+Q+iImxbXSrVJqLpZzsPgoTo/hc6nXnwZuEJE/y5omffppP1AB3CMiP8qxrLuAuwC2bNly7blz5wDnedmRWIJwNE4kmiCRLH2OJhyN8+TJXlrbQ7S/PZwZboCDV9ZwpDHIe3bWFu10kt9jUxPwaNBQSuW1nB9+lOvcy/SjuQu4FjgM+IFnjDHPisipKTOJ3AfcB9DU1JRZhm05CeD0TXPjsQTjsQThqPO7FAIeFy0HGmg50MD5/jBH20M82BFiIBzjxXMDvHhugIp0UrwxyM66hSXFI9EEkWhEK90qpeat1EeNTmBz1utNwNs5pukTkTFgzBjzFPAOnFzHnPncduqguTx6HVvWBbjr0Hb+6D1bee5sP63tIZ49c4mR8Ti/fPkiv3z5IjvrnKT4rXvrqPC5572u8ViC0FBCK90qpeas1KehXDgH/cPARZwE96dEpCNrmr3Ad4E7AA/wPHCniLTPtNympiZ54YUX5tyeiXiCSNTpdUzEk5Rq2/SPRXk4lRQ/3x/ODHfbhptSSfGDV9YsOCnucVlUB7TSrVLKUZSchTHmOzkGDwEviMg/L6BxR3Aui7WB74vIN4wxnwcQke+lpvkL4LNAEufy2m/nW+Z8g0W2ZFIIxxKpUzgJ4slk4ZmKTEQ43pVKip/oJZJ12qyuwktzY5Dm/UGCVQtLirtti+qAm3KtdKvUmlasYHEfsAf4x9SgjwAdOKeRzojIl4rQ1qIoRrCYLt3riMQSjMeWvtcRiSZ48pRzp3jbxWlJ8S3VNDc28N5dC0uKu22LSr9WulVqrSpWsHgMuF1E4qnXLuAhnHsk2kRkX5Hau2CLESyyJTO5jtL0Oi70hznaEeKhjm4ujUUzw8u9Lg7vqaPlQJBddeXzPuBrpVul1qZiBYuTwPUiMpR6XQU8JyJ7jDEvi8g7i9biBVrsYDFdqXIdiaTwfCop/syZS1MS9Ds2lDl3iu+tp8o/v6S4VrpVam0pVrD4Y+BrwBM4Zz8OAX8N/ATnxrq/KEpri2Cpg0W2UvU6BsKTSfFzl6Ymxd+zo5aWA0EObqmZV6VarXSr1NpQtJvyjDENwPU4weJ5EZl+meuyUMpgMd1S9zpEhBOhER5oc+4UD0enJsXv2F9Pc2OQhir/nJetlW6VWt2KGSw2AleSdX+GiDy14BYW2XIKFtmWutcRiSX4zaleHmgP8Vrn0JRx79xSTUtjkPfurMU7xzu7tdKtUqtTsU5DfQv4BM4VUOmjnIjIB4vSyiJarsFiuqW8wqpzIHWn+PFuLo1OJsXLvDaH9zh3il9VP7ekuFa6VWp1KWaC+2oRmShm4xbDSgkW2Zaq15FICi+c66e1LcTv3rxEPCspvr22jObGILftracqMLekuFa6VWrlK1awaAU+JiKjxWzcYliJwWK6iXiC8WiScCy+aL2OwXCUh1/vobWti7eykuIuy/Dunes50tjAtVfOLSmulW6VWrmKFSz+Cacm06NApnchIl8oRiOLaTUEi2yL3etIJ8WPtod47EQPY1lJ8Q3lXm5PJcU3Vs8+Ka6VbpVaeYoVLD6Ta7iI/HABbVsUqy1YTBeNJ50rrBah1zEeS/CbN/pobe/ilQtTk+LXbK6iubGBQ7tqZx0EtNKtUivHsn2exWJZ7cEi22L2Oi4ORniwI8SD7d30jk6mqso8Nrek7hTfXV8xq6S4VrpVavlbULAwxvxMRD5ujGnj8mdNICJXF6eZxbOWgsV0i9HrSCSFF88N0Noe4ren+6YkxbdlkuJ1VAc8BZellW6VWr4WGiwaRKTLGHNlrvEicq4IbSyqtRwssiWTwnh8stcRSyy81zEUjvHIiW5a20Kc6RvLDHdZhnfvWE9zY5Drtq4rmBR32xY1ZRo0lFpO9DSUAorb6xARTnWP0toe4tET3YxNTCbF15d7aN7vlE/fWJM/Ka7l0ZVaPhbasxghx+knnJIfIiKVC29icWmwKKyYvY6JWIKnT/fxQHuIl88PThl39aYqWhqDHLpqA/48SXG3bVEVcFOhQUOpktGehSqoWL2OrqEID7Z3c7QjRM/IZFI84LF5/+46WhqD7G2YOSnuspygoc/UUGrpLbRnUSkiw8aYdbnGi0h/EdpYVBosFkZk6hVW8+l1JJLCS+cHONoe4unTfcQSk5+zK9cHaGkMctu+empmSIpr0FBq6S00WPyLiPyeMeYszumo7D1XRGR78ZpaHBosiisaTxJJPWJ2PJYgOcdex3AkxiOv99Da3sWbvZNJcdsy3Lh9PS2NQa7fljspnn4QU6Vfg4ZSi01PQ6miERHGY0nC0TjhefQ6TnWP0NoW4tETPYxOxDPD15d5nDvF9wfZvC5w2Xy2Zaj2e/TpfUotomKWKK8BdgG+9DAtUb62xRLJzOmqSCwx61xHNJ7k6dN9tLZ18dL5wSlXUBzYWElzYwM3X7UBv2dqUty2jNPT8OnT+5QqtmKV+/gc8EVgE/AK8C7gGRG5pVgNLRYNFqUx315HaGicBztCHO0I0T08mRT3u23ev3sDLQeC7GuonHIaSh/5qlTxFStYtAHXAc+KyDXGmD3AfxCRTxSvqcWhwWJ5SPc6xlP5jkK5jqQIL6XuFJ+eFN+yLkBzY5Db99WzrmwyKa6PfFWqeIoVLI6JyHXGmFeAG0RkwhjziohcU8zGFoMGi+Un3etwrrKKE43n73UMR2I8dqKHB9pDnO6ZrIpvGbhxu3On+Lu2r88ECA0aSi1csYLFL4HPAl8CbgEGALeIHClWQ4tFg8XyF09MXmEViSVIJGf+HJ7uce4Uf+T1bkbGJ5Pi68o83L7PSYpvWe8kxS1jqPC5qA54NGgoNUdFvxrKGPM+oAo4KiLRQtMvNQ0WK0/6VFU4lmAilsg5TTSe5Len+2htD/HiuYEpSfH9V1TS0hjk5t0bCHicy2wrfS6q/G5c+pxwpWZlwcHCGGMBr4lI4yI0rhm4B7CB+0XkmzNMdx3wLPAJEfl5vmVqsFjZEpmy63HGo8mcZde7h8d5qKOb1vYQoeHxzHCf2+Lmq+o4ciDI/isqsSzL6Wlo0FCqoGKdhvox8FUROV/EhtnAKeA2oBM4BnxSRI7nmO5hYBz4vgaLtWUinup1pJLl2ZIivHJhkNa2EE+90TslKb6pxk9LKileW+Gj3Os88tWtQUOpnIoVLB7DuRrqeSBzG66IfHABDbsR+CsRuSP1+qupZf7NtOm+BMRS6/8XDRZrV3avIxKdmusYGY/x2IleWtu7ONU9NSl+/bZ1HGls4F3b11Ed8FAVcON16SNflcqWL1jM5WEC/6FI7cm2EbiQ9boTuCF7AmPMRuDDOEn16xahDWoFsS1DudeVeQ5Gdq4D4EPXXMGHrrmCN3tGae0I8cjxbobH4zx7pp9nz/RTE3Bz6956jhwIsu+KKqr8bn1OuFKzMJdgcUREvpw9wBjzLeDJBaw/1+Uq07s63wa+LCKJfLWBjDF3AXcBbNmyZQFNUiuJz23jc9vU4FxhFU4Fj131FeyoK+eu927nd29e4mh7F8feGmAgHOMfX+zkH1/sZF9DBS2NDbQcCLKpJqBBQ6k85nIa6iUROTht2GsLeazqbE5DpQoYpqNELRAG7hKRX820XD0NpbLvJo/EEkTjSXqGx3nweDdH20N0DWUlxV0W79u9gQ++4wpu3r2BMq+7hC1XqnQWWnX2T4B/DWwH3swaVQH8VkT+1QIa5sJJcB8GLuIkuD8lIh0zTP8DNGeh5iG71zE2EeeVC4M80NbFU2/0TblBcGO1n9+7uoGPNW1ix4ZyrXSr1pSFBosqoAb4G+ArWaNGsp9lYYypEZGBeTTuCM6pJhvnSqdvGGM+DyAi35s27Q/QYKEWSESYiDulSHpGxnmwPcQD7SFOhkYy01gGbti2no9eu5EPHLgCn0dPUanVb0lKlOc6TVUqGizUXKR7HW2dQ/zq5Ys8dLyboUgsM77K7+YDVzfw+9dvYf/GqhK2VKnFtVTB4mUReWdRFrZAGizUfIkIw5E4rR1d/Orlizx/tp/sSiT7Gir5+HWb+MjBTVT4NLehVhftWSg1T+f7x/jZsU5++fJFLg5GMsO9Lovb9tXzqeu3cOOO9ZrbUKuCBgulFkhE+O3pPn567AIPH+9mIispvqnGz/9+cCOfvH4LDVX+ErZSqYXR01BKFdHweIxfvXyRn73QSfvFoczwdPn0j1+3mZbGBjwuLSuiVpaiBovUHdXpS0PeFpF4avi67KujSkmDhVoqp7pH+Mlz5/nVKxcZCE8mxatTSfFPXr+FRk2KqxVioZfOfhXnuRVfT70+DwwCHuCH0+s4LQcaLNRSi8aTPHI8xE+OXeC3p/umJMX3BCv4yMGNfPidm6it8JaukUoVsNBg8RLwXhEZS71+WUTemaoE+6SI3FT0Fi+QBgtVSt3D4/z0+fP8/MVOLgxMJsU9Lov3XbWBjxzcyKFdGwh451JtR6nFt+BgkZ24Nsb8oYj8IGvB1xazscWgwUItByLC7968xE+eP88jx7sZz0qKN1T5aGkM8uGDG9lWW07AbWPpk/1UiS00WJwC9otIbNpwL9AuIruK1tIi0WChlpvBcJR/eqmTX7x0kY63hzPDDdC0tYaWxgYO762j2u8h4LX1mRuqJBYaLP4aCAJ/JiLh1LAy4LtASES+WuT2LpgGC7VcSephTf/j2AUe7AhNSYpX+lzcureelsYgexoqKfO6CHhsrYarlsxCg4UNfAP4HHAuNXgL8A/A19JXQy0nGizUSjAQjnK0vYtfv9LFc2cvTUmKX1VfTktjkFv21FEd8OD32AQ8Lj1dpRZVsZ6U5wd2pl6eFpFIvulLSYOFWkki0QSne0f4n6+8TWt76LKk+Ht31tLSGOSaLdXYloXPbRFwu/B7bL2XQxXVQnsW/1ZE/lPq74+JyD9mjftrEfl3RW1tEWiwUCvRRDzBYDjKc2cHaG3r4vGTPYzHJpPi9ZVemvcHuaMxSLDSB4Dbtgikeh0+t6VlR9SCFO1qqBxXRi2bEh/ZNFiolSyeSDIYidEzPM7jJ3pobQ/RPi0pfvDKGo40BnnPztpM78IYg99tOz/a61DzsNBncJsZ/s71Wim1QC7borbcS7XfTX2ljw9cfQVv9Y1xtCPEQ8e76R+L8uK5AV48N0CFz8XhPXUcOdDAzrpywtE44Wgcxpxeh89tE/A4AURzHWohZhMsZIa/c71WShWJy7ZYX+6lOuChyu9m24Yy/vimbTx39hKt7SGePdPPyHicX73yNr965W12biin5UCQw3vqqPS7iSWSxBJJRsZjGGMyuQ69NFfNx2xOQyWAMZxehB/nGdikXvtEZNkV9dfTUGo1SiaFkfE4Q5EY8WSS/rEoDx/vprU9xPn+cGY6t224KZUUP3hlDVaOPIbmOlQuS1J1djnRYKFWMxFhZCLOUDhGLJFERDjeNUxre4jHT/QSiSUy09ZVOEnx5sYgwSpfzuVZxuBL5TkCHu11rGULTXD7gM/jXDb7Gs5zspfdvRXZNFiotUBEGJ2IM5gKGgCRWIInT/bS2h6iLat8OsDBLdW0NDbw3l21eZPfmutYuxYaLP4HEAN+A7QA50Tki0VvZRFpsFBrzdhEnIFwlGhW/akL/WEnKd7RzaWxaGZ4uddJirccCLKrrjzvKShjDF6Xc8rK77HxuvRu8tVsocGiTUQOpP52Ac8vx8tls2mwUGtVrqCRSArH3uqntT3E7968RCLrVvEdG8poaQxyeG89Vf7C6UeXZeFPBQ69m3z1KXbV2WV5b0U2DRZqrRubiDMYiTGRlb8Ap8TII8e7eaA9xLlLU5Pi79lRS8uBIAe31GDPIghkX2Hl81ja61gFFhos0ldDwdQrogwgIlJZxLYWhQYLpRyRaILBSJRIdGrQEBFOhEZobQ/x2IkewtGpSfE79tdzx/4gV1TP/pniLsvC57EIeFz43fasAo5aXvRqKKXWuPFYgqFIjLGJy69NicQS/OaNPlrbuni1c2pS/JrN1Rw5EOS9O2vxzrH6rdftnKrya+XcFUODhVIKcB7/OhiJMjaRINe+f3EgwtGOEEc7QlwanUyKl3ltDu9xyqdfVZ8/KZ6LbZlMGRK/28all+cuS8s6WBhjmoF7ABu4X0S+OW387wNfTr0cBf5ERF7Nt0wNFkrlF0skGYrEGBmP5wwaiaTwwrl+WtucpHg8Kym+vbaM5sYgt+2tpyowv3tyPS7ndFXAY+N16U2By8WyDRapZ2WcAm4DOoFjwCdF5HjWNO8GXheRAWNMC/BXInJDvuVqsFBqduKpoDE8Q9AA5yl/j7zuFDQ82zeWGe6yDGW9ueEAABevSURBVO/euZ6WxiBNV66bd47CMmbKFVba6yid5RwsbsQ5+N+Rev1VABH5mxmmr8F5lOvGfMvVYKHU3CSSwmA4ysh4nOQMxwQR4WT3CK1tTlJ8LCspXlvu4Y7UneIb55AUz8XjsvC7tRRJKSy06uxi2ghcyHrdCeTrNfwx0LqoLVJqDbItkylaOBSJMRyJXRY0jDHsCVayJ1jJn9y8w0mKt3fxyoUh+kaj/Pi58/z4ufO8Y1MVLQcaOLSrdl6J7Wg8STTu9Hi0FMnyUepgkesrQ86vNcaY9+MEi5tmGH8XcBfAli1bitU+pdYU2zKsK3Oq3A5HYgzlCBoAPrfNbfvquW1fPW8POknxB9u76R2d4NXOIV7tHOLvHrW5ZU8dzY1B9gQr5tVDSIpkyq5fQgsgltKKOA1ljLka+CXQIiKnCi1XT0MpVRzJpDA87gSN7Du/c0kkhRfPDdDaHuK3p/umJMW3rg/Q0hjktn31VAc8RWlb5mFP2usomuWcs3DhJLgPAxdxEtyfEpGOrGm2AI8BfyAiv5vNcjVYKFVc6fLog5FowaABMBSO8cgJp3z6md7JpLhtGd69w0mKX7d1/knxXNy2lQkcfretvY55WLbBAsAYcwT4Ns6ls98XkW8YYz4PICLfM8bcD3wEOJeaJT7Tm0nTYKHU4hARhiOTz9SYzfRv9IzS2hbikRPdjE1MJsXXl3u4Y189zY1BNtUEitrO7F6H362PmJ2tZR0sFoMGC6UWl4gwPO48U2M2QQNgIpbg6dN9tLaHeOn84JRxBzZWceRAkENXbcC/CHd7Z/c6fC4tgDgTDRZKqUUx/UFMs9U1FOHB9m6OdoToGZnIDA94bN6/u46WxiB7G+aXFC8kXQAx3fPQAoiTNFgopRad8yCmqeXRC0kkhZfOD3C0PcTTp/uIJSaPR1emkuK37q1nXVlxkuK5ZBdAXOtl1zVYKKWWTDjqPL1vfFp59EKGIzEePdFDa1uI072jmeG2ZXjX9nUcaWzg+m3FTYrnkn5KoM+99gogarBQSi258ViCgfDl5dFn441up3z6oyd6GBmfrJS7vszDbfucgoab1xU3KZ6LbZlUrmNtlF3XYKGUKpnxWILBcIxw9PLy6IVE40knKd7WxUvnB6fcsXtgYyXNjQ3cfNUG/J6l6QGs9rLrGiyUUiU3EU8wFI4xmuOZGrMRGhrnwVT59O7hyaS4323z/t0baG4Msv+KyiW7vyK77HrA41oVvQ4NFkqpZSP9TI3R8fkFjaQIL58f5IG2rsuS4lvWBWhuDHL7vsVNiueyGsqua7BQSi07sUSSwVRPY77HoZHxGI+d6OGBthBv9EwmxS0D79ru3Cl+w7Z1S172fKWWXddgoZRatooRNABO94zS2h7ikde7pyTFawJubt9XT0tjA1vWL35SPJeVUnZdg4VSatmbzYOYZiMaT/Lb1J3iL54bmJIU39dQyZEDQW7evYGApzRFt7N7HX738iqAqMFCKbVizOZBTLPVPZxKird3Exoezwz3uS1uvsq5U7xx49IlxXNJl133L4MCiBoslFIrTiIpMz6Iaa6SIrxyYZDWthC/Od035S7zTTV+WlJJ8fXl3oU2e0FKXQBRg4VSasWayzM1ZmN0PM6jJ3o42h7iZPdIZrhl4Pptzp3i79q+9EnxXJa67LoGC6XUipd+psZsy6PPxpu9qaT48W6GpyXFb91bT8uBIFvXlxVlXQuVLoAYcLvwexan16HBQim1asy30m0+0XiS3715iaPtXRx7a3pSvILmxgbev3sDZd5SP4l6ksua2usoRgFEDRZKqVVpZDzGYBGDBkDP8DgPHXee8tc1lJUUd1m8L3Wn+NUbq5bV5a/FKruuwUIptaqNTcQZjMSYmGOl23ySIrzWOURre4inTvUykZUU31jtp7mxntv3BdlQUdqkeC7pXkf6psDZ9jo0WCil1oT5lkcvZHQizuMnemhtD3EiNDUpft3WdbQcCHLj9vXL6p6JNGMMXpc1q7LrGiyUUmvKQsqjF3K2b4zW9i4ePt7DUCSWGV7ld3PbvjpaGhvYVrs8kuK55Cu7rsFCKbUmjccSDEVijM2z0m0+sUSSZ968xAPtIV54q5/sq3r3BCtoaQzy/j11lC+jpHgu2WXX/R6XBgul1Nq10PLohfSOTPBwKil+cTCSGe51WRy6agMtjUGu3lSFtYyS4rnsqKvQYKGUUtG4U39qoUULZyIivHZxiNa2EE9OS4o3VPloaQxyx/7lmRQHDRZKKTVFPJFkMBJjZIFFC/MZnYjzxMleWtu7eL1ralK86coaWg40cOP29Ute0iMfDRZKKZVDMYsW5vPWpTFa20I8fLybwaykeKXPxa2pZ4rv2FC+aOufLQ0WSimVRyIpDEec+lOLGTRiiSTPnumntb2L589OTYrvrq+guTHI4T11lPtKkxTXYKGUUrNQ7KKF+fSNTvBQRzdHO0J0DkwmxT0ui0O7amluDHLN5uolTYov62BhjGkG7gFs4H4R+ea08SY1/ggQBv5QRF7Kt8y5BIsnTvTwraMnONM3BsC29QG+0rKXm/fUTZnm3qfOcGEgzOaaADduX8czZ/o51T1MLCF4XBa76iq4+9D2zHzZy43Fk2BABCzL4LZAMETjySk1aGzLYBDSOTHLQI3fjd/rYnNNILP86e25+9B2AO596gxv9IwQjSdx24ar6iu5cfs6fnbsPJ1DzgPuDU5J5o9du4lnzvRPWUb2ey60zaavP7tdL5/vZzw+8+fKALYFwQovFX4PIxNxSCbpHYtNSQh6bEM0sfq+zKwUe+rLAMPp3tHMZ9KkfnypmkQbyr2ICKPRBAbnABhNCGUem8N7NvB61whnL4UB2F5bxpeb9wDwzdbXOd07RiIpuGzDjtoy9jZU8EB7d+Yz4Lbgz2/Zxdm+Uf751S5yHbvdFlQFPJkn421bH2BvQwWPnuhldCKOMQaXJXhdrpz7adoTJ3r42q/auDg4ntkn7dQbtizDuoCbMq+bgXA0s39tXV/Onddt5vrt63Juv+fP9PPTYxfoGo7QUOmfcVoRoe2ic6f4kyd7GZ+WFG/eH+T2/fXUV/oK/s9mu86Z5nvg6/8qGu05mzP7XtJgYYyxgVPAbUAncAz4pIgcz5rmCPDnOMHiBuAeEbkh33JnGyyeONHDX/z8VQbCMdL3pSQFqgNu/vaj78gcAP/9rztw206d+UtjE/SMRKnw2oylb/gRqK3w4LZtvv7B/QCZ5SaSwkK3cKXXYkOln1hC+OjBjfz8pYuZ9kRS15EbwGUb+kaizt4MlHttBsPxnOs3QH2ll9pyL5FYglhC+PoH9xcMGNO3R3redLsujY4zFp19nR4LKPdaDE8Ur7aPWhoG5wuNMYYyt8XQhLM/uCzni1FCnPGu1M6VFCjz2iSTQjiaIPt7gIEF7Sfp9cQSzv5mGaYEF4MzPns/zf5i94WfvMTwROEb+CzASuWjq/1u3C6bL96y67ID8vNn+rnnsTdwWU7NpvFYknhSck6bLRyN8/iJXlrbQxzvGp7S/qatNbQ0Bnn3jtqcSfH5rjN7vuf+9rPhaO+5nHcUljoNfz1wWkTOiEgU+CnwoWnTfAj4kTieBaqNMQ3FWPm9T51hZDyObRlsy3J+jGF0Is69T53JTOO2DQGPC2MMw5E4loHh8TgWBpdlYVnOcLdtuPepM1OWW4xQPDyRJOBx4bYN9z99dkp7Ah4XoxNxRsbjTtusVJswDEVyBwpwdsyR8XhmGem2z2abTV9/drvmEigAxKCBYoUSnAOybQxDE4n0dxSSMnngTwqT+1ZqPxmLJkhKqpdiFh4osteTvd7skzcCl+2nafc+dYbRWd7pncRZj8EwFk3gsgw/PXbhsul+euwCLsv5QmVwfs80bbaAx8UHrm7gu596J//tD5v4eNMmagJuBDj21gBf/5fX+fi9z/Ddx07zZs9oUdaZPV8+pb61cCOQ/U46cXoPhabZCHRlT2SMuQu4C2DLli2zWvmFgTDxZHLKQ06McZJdnQPhzDTVfndmfDSRxDIQE2fa9DzRRBK/26ZzIIzAZcstBr/b6c1smfZPTSQFESEhJnPrvpn2zSqXaFalznTbC5m+PfK1azZWYcpsTREm94PMsBn+p8Y406fHZ2YrRrTIucKpy52+n6ZdGAgX3FemL9YYJ1ntc1uEhiOXTdM1HKFyWpJ6pmlncuX6Mj7/vh187qZtPHe2n6PtIZ45c4nh8Ti/ePkiv3j5IrvqymlpDHJ4b92815lrvlxKHSxyZW6m/9tmMw0ich9wHzinoWaz8s01AfpGJpCsA7+IkzvYVBPITNMzMp55uLvHtjIBIz2fiDM8Ektk5ksvt5gisQRlHufUT/bD5m3LQCpQxJOSadP0rvh0nqxglt32fKZvj+ntmqt0W9XKZLj8/5e9L2WTrN7ElHGL9f/Psf7p+yk4n+m3ByOzChhO+w1GnJzaRDxJsNJ/2XQNlX4ujU1M+bY+Hss9bSEu2+I9O2t5z85a+seiPNQRorU9xIWBCG/0jPLGY6f5/558kzKPi8FkjGq/O1M+fTbrzNXWXEp9GqoT2Jz1ehPw9jymmZe7D22nwucikRQSyaTzI0K515VJGt99aDuxhBCOOjfvVPpdJMW5PjqJEE8mSSad4bGEcPeh7VOWW4zrGCq9FuFonFhC+NxN26a0JxyNU+51UeFzOW1LptqEUOV3zbh+A1T4XJllpNs+m202ff3Z7SrzzO0jZcR5f2rlSecsEiJUee3Msdkyk9/wLMPkvpXaT8o8tvNlC+cALuT+RjgX6fVkrzf72G/gsv007e5D2yn35KnEOu39pvevSr8bMHz+fdsvyyHced1m4kkhEksgOL/jSeHO6zZPX/ycrCvzcOf1W/jBZ6/jO3deQ0tjEJ/bIpYQBiMxekejnLkUpm9sgpHx2KzWmd3WfEqd4HbhJLgPAxdxEtyfEpGOrGk+APwZkwnu74jI9fmWu1hXQ3UOhNmUdTXUG93DRJfgaqiA18WmHFcdpduT62ooj23YNYurobKXMderoabPO9+roUYn4oheDbXszOVqqPTFHnO5GurN3jHiS3A1lNsSPPO4GsqVerO2ZbGh3EOFz03v6MSU/St7WeFonIHw5DM10lcYhYYjBOdwZdJcRaIJnjjZw9GOEG0Xh6eM21VXzieu28xNO3MnxdOW/dVQkLna6ds4V6p9X0S+YYz5PICIfC916ex3gWacS2c/KyJ5I4HeZ6GUKpVINMFgZHHKoxdyvj/M0fYQDx3vpn8smhle4XNxeE8dLY1BdtVXzDj/sr7PYjFosFBKldp4LMFgOEY4ujiVbvNJJIXnzl6itT3Es2f6p9xguHNDOc2NQW7dW5c6lTZJg4VSSpXIYj5TYzb6x6I88no3rW0hzvVPXgXmtg037aylpTHIwStrsIzJGyxKfTWUUkqtaulHmS72MzVmsq7Mw8ebNvOxazfxetcID7R38fiJXiKxBI+f7OXxk73UVXhp3h/MuxwNFkoptQS8Lpu6SpvqeJLBSJSxicSilUfPxRjDvisq2XdFJX/6/p08daqXB9pCtF0comdkgh89ey7v/BoslFJqCXlcFnUVPmIB50FMi/lMjZn43TZ37HcexNQ54CTFH+zoJl+40JyFUkqVUDzhBI3hEgSNbImkcFWwUnMWSim1HLlsi/XlXqoDHidoLPIzNWaSLhU0Ew0WSim1DNiWYV2Zhyq/m+FIjOHxxX+mxlxosFBKqWXEtgw16aCxRA9img0NFkoptQxZlqE6kA4acYbCMeLJ0pXz12ChlFLLmDGGKr+bSp+LkQknaMQSSx80NFgopdQKYIyh0uemwus88GxwiYOGBgullFpBjDFU+NxU+NypoOE8F3yxabBQSqkVqtzrotzrYmwizmBksjz6YtBgoZRSK1yZ10WZ10U46pyeGl+EoKHBQimlVomAx0XA42I8lmAgXNxnamiwUEqpVcbntmmo8hf1mRoaLJRSapXyuW2CVcUpj67BQimlVrlilEfXYKGUUmtEdnn0wVRPY7ZBw1rktimllFpm3LbFhgovm2v8VPrdGJO/4ixoz0IppdYsl21RW+6lJuBhMBzNO632LJRSao2zLcP6cm/eaTRYKKWUKkiDhVJKqYI0WCillCqoZMHCGLPOGPOwMeaN1O+aHNNsNsY8box53RjTYYz5YinaqpRSa10pexZfAR4VkV3Ao6nX08WB/1NE9gLvAv7UGLNvCduolFKK0gaLDwE/TP39Q+B/mz6BiHSJyEupv0eA14GNS9ZCpZRSQGmDRb2IdIETFIC6fBMbY7YC7wSem2H8XcaYF4wxL/T29ha5qUoptbYt6k15xphHgGCOUX85x+WUA/8EfElEhnNNIyL3AfcBNDU1za3oiVJKqbwWNViIyK0zjTPGdBtjGkSkyxjTAPTMMJ0bJ1D8WER+sUhNVUoplYeZa+XBoq3YmP8MXBKRbxpjvgKsE5F/O20ag5PP6BeRL81h2b3AuQU2sRboW+AyVgPdDg7dDg7dDpNW47a4UkQ25BpRymCxHvgZsAU4D3xMRPqNMVcA94vIEWPMTcBvgDYg/UTyfyciDyxB+14QkabFXs9yp9vBodvBodth0lrbFiUrJCgil4DDOYa/DRxJ/f00ULgcolJKqUWld3ArpZQqSIPFzO4rdQOWCd0ODt0ODt0Ok9bUtihZzkIppdTKoT0LpZRSBWmwUEopVdCaDxbGmP9sjDlhjHnNGPNLY0x11rivGmNOG2NOGmPuyBp+rTGmLTXuO2Y2D7Bd5owxH0tV9k0aY5qmjVsz2yEXY0xz6r2fTt0TtGoZY75vjOkxxrRnDZuxQvRMn42VbqaK12txW2SIyJr+AW4HXKm/vwV8K/X3PuBVwAtsA94E7NS454EbcS7rbQVaSv0+irAd9gK7gSeApqzha2o75Nguduo9bwc8qW2xr9TtWsT3ewg4CLRnDftPwFdSf39lNvvISv8BGoCDqb8rgFOp97vmtkX6Z833LETkIRGJp14+C2xK/f0h4KciMiEiZ4HTwPWp0iSVIvKMOJ+SH5GjYu5KIyKvi8jJHKPW1HbI4XrgtIicEZEo8FOcbbIqichTQP+0wTNViM752ViShi4ymbni9ZrbFmlrPlhM80c435DB+WBcyBrXmRq2MfX39OGr1VrfDjO9/7VkpgrRa2LbTKt4vWa3Rcnu4F5K+arfisg/p6b5S5yHLf04PVuO6SXP8GVvNtsh12w5hq3o7TBHa+V9zseq3zbTK17nScut+m2xJoKF5Kl+C2CM+Qzwe8Dh1CkVcL4ZbM6abBPwdmr4phzDl71C22EGq247zNFM738tmalC9KreNjNUvF6T2wL0NBTGmGbgy8AHRSScNerXwJ3GGK8xZhuwC3g+1fUcMca8K3X1zx8AM30rXw3W+nY4BuwyxmwzxniAO3G2yVrya+Azqb8/w+T/OednowTtK7rUZ/ofgNdF5P/JGrXmtkVGqTPspf7BSURdAF5J/Xwva9xf4lzVcJKsK32AJqA9Ne67pO6EX8k/wIdxvh1NAN3Ag2txO8ywbY7gXA3zJs4pu5K3aRHf60+ALiCW+jz8MbAeeBR4I/V7XaHPxkr/AW7COY30Wtax4cha3BbpHy33oZRSqqA1fxpKKaVUYRoslFJKFaTBQimlVEEaLJRSShWkwUIppVRBGiyUWgaMMT9JVT7+N6Vui1K56KWzSpWYMSYIPCciV5a6LUrNRHsWSi0CY8zW1HNSfpjqMfzcGBMwxlxnjPmdMeZVY8zzxpgK4CGgzhjzijHmvaVuu1K5aM9CqUWQqlR6FrhJRH5rjPk+cAL4PPAJETlmjKkEwjh1hP5FRBpL1V6lCtGehVKL54KI/Db1938H7gC6ROQYgIgMy+SzVJRa1jRYKLV4pnfbh3MMU2pF0GCh1OLZYoy5MfX3J3GexHiFMeY6AGNMhTFmTTwmQK18GiyUWjyvA58xxrwGrAP+DvgE8HfGmFeBhwFfCdun1KxpglupRZBKcGvSWq0a2rNQSilVkPYslFJKFaQ9C6WUUgVpsFBKKVWQBgullFIFabBQSilVkAYLpZRSBf3/1D1fSkLGggEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(dat.pcf, dat.PEG_trailing.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we fill in the missing data by using IterativeImputer. Its method is to model each feature with missing values as a function of other features in a round-robin regression. As stated above, linear regression is not an ideal function here, so we decided to use scikit-learn's ExtraTreesRegressor, because it works better with extreme outliers and non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_dat = dat.loc[:, 'bm':'PEG_trailing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the imputation method\n",
    "imp = IterativeImputer(estimator = ExtraTreesRegressor(n_estimators=10, random_state=1, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\impute\\_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# actually imputing the values\n",
    "imputed = imp.fit_transform(numerical_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that random_state=1 doesn't seem to do anything. We get different imputed values every time we run the code, and so what we write might not always line up 100% with the output (because we are referring to the output of previous runthroughs). We try to be transparent by writing below each output the test score we were given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the imputation returns an array, so we transform the data back to a dataframe\n",
    "imputed = pd.DataFrame(data = imputed, columns = numerical_dat.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the missing data is imputed, we add public_date, gvkey and splticrm to the data frame. The first two will be needed later for further analysis, the latter is added so we can drop all observations with missing credit rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = pd.concat([imputed, dat.loc[:, 'public_date'], dat.loc[:, 'gvkey'], dat.loc[:, 'splticrm']], axis = 'columns')\n",
    "imputed = imputed.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>public_date</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1101</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>9.8788</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.5750</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>12.4490</td>\n",
       "      <td>-5.1641</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.153</td>\n",
       "      <td>3.287</td>\n",
       "      <td>11.6451</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.8044</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>13.2599</td>\n",
       "      <td>-6.2057</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.152</td>\n",
       "      <td>3.258</td>\n",
       "      <td>11.6451</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.8044</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>13.2599</td>\n",
       "      <td>-6.2057</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.640</td>\n",
       "      <td>11.6451</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.8044</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>13.2599</td>\n",
       "      <td>-6.2057</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0999</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.753</td>\n",
       "      <td>10.1018</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-1.9213</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.617</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>11.0560</td>\n",
       "      <td>-3.4429</td>\n",
       "      <td>2010-05-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29832</th>\n",
       "      <td>0.0780</td>\n",
       "      <td>11.083</td>\n",
       "      <td>28.586</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>0.728</td>\n",
       "      <td>1.982</td>\n",
       "      <td>2.851</td>\n",
       "      <td>2.851</td>\n",
       "      <td>0.650</td>\n",
       "      <td>11.9060</td>\n",
       "      <td>4.6905</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29833</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>13.734</td>\n",
       "      <td>37.120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>14.7840</td>\n",
       "      <td>6.3142</td>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29834</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.452</td>\n",
       "      <td>33.655</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>13.4040</td>\n",
       "      <td>6.3142</td>\n",
       "      <td>2016-11-30</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29835</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>11.099</td>\n",
       "      <td>29.996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>11.9470</td>\n",
       "      <td>6.2857</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29836</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.672</td>\n",
       "      <td>30.415</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.717</td>\n",
       "      <td>1.714</td>\n",
       "      <td>2.451</td>\n",
       "      <td>2.451</td>\n",
       "      <td>0.665</td>\n",
       "      <td>13.1570</td>\n",
       "      <td>6.3140</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25468 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bm      ps     pcf      dpr    npm    gpm    cfm    roa     roe  \\\n",
       "0      0.1101   0.113  -5.253   9.8788 -0.072  0.163 -0.018  0.024 -1.5750   \n",
       "1      0.0951   0.153   3.287  11.6451 -0.074  0.150 -0.018  0.010 -4.8044   \n",
       "2      0.0951   0.152   3.258  11.6451 -0.074  0.150 -0.018  0.010 -4.8044   \n",
       "3      0.0951   0.123   2.640  11.6451 -0.074  0.150 -0.018  0.010 -4.8044   \n",
       "4      0.0999   0.127   2.753  10.1018 -0.079  0.150 -0.025  0.009 -1.9213   \n",
       "...       ...     ...     ...      ...    ...    ...    ...    ...     ...   \n",
       "29832  0.0780  11.083  28.586   0.0000  0.243  0.982  0.279  0.258  0.2820   \n",
       "29833  0.0740  13.734  37.120   0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29834  0.0740  12.452  33.655   0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29835  0.0740  11.099  29.996   0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29836  0.0740  12.672  30.415   0.0000  0.212  0.978  0.253  0.240  0.2500   \n",
       "\n",
       "        roce  ...  de_ratio  cash_ratio  quick_ratio  curr_ratio  at_turn  \\\n",
       "0     -0.058  ...    -9.366       0.428        0.603       0.664    0.816   \n",
       "1     -0.092  ...    -8.291       0.629        0.787       0.859    0.787   \n",
       "2     -0.092  ...    -8.291       0.629        0.787       0.859    0.787   \n",
       "3     -0.092  ...    -8.291       0.629        0.787       0.859    0.787   \n",
       "4     -0.110  ...    -8.617       0.551        0.712       0.780    0.799   \n",
       "...      ...  ...       ...         ...          ...         ...      ...   \n",
       "29832  0.268  ...     0.728       1.982        2.851       2.851    0.650   \n",
       "29833  0.250  ...     0.704       1.986        2.595       2.595    0.682   \n",
       "29834  0.250  ...     0.704       1.986        2.595       2.595    0.682   \n",
       "29835  0.250  ...     0.704       1.986        2.595       2.595    0.682   \n",
       "29836  0.250  ...     0.717       1.714        2.451       2.451    0.665   \n",
       "\n",
       "           ptb  PEG_trailing  public_date     gvkey  splticrm  \n",
       "0      12.4490       -5.1641   2010-01-31    1045.0        B-  \n",
       "1      13.2599       -6.2057   2010-02-28    1045.0        B-  \n",
       "2      13.2599       -6.2057   2010-03-31    1045.0        B-  \n",
       "3      13.2599       -6.2057   2010-04-30    1045.0        B-  \n",
       "4      11.0560       -3.4429   2010-05-31    1045.0        B-  \n",
       "...        ...           ...          ...       ...       ...  \n",
       "29832  11.9060        4.6905   2016-09-30  189491.0       BBB  \n",
       "29833  14.7840        6.3142   2016-10-31  189491.0       BBB  \n",
       "29834  13.4040        6.3142   2016-11-30  189491.0       BBB  \n",
       "29835  11.9470        6.2857   2016-12-31  189491.0       BBB  \n",
       "29836  13.1570        6.3140   2017-01-31  189491.0       BBB  \n",
       "\n",
       "[25468 rows x 41 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the time has come to train our prediction model. We assume that including all variables will lead to overfitting, and using just a subset of all variables should improve the result. We test that assumption by trying out both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputed.loc[:, :'PEG_trailing']   # using all variables at our disposal\n",
    "y = pd.factorize(imputed.loc[:, 'splticrm'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.96\n",
    "Test score:       0.97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['roe', 'curr_ratio', 'bm', 'de_ratio', 'dpr', 'at_turn', 'debt_ebitda']   # using just a subset\n",
    "X = imputed.loc[:, subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.95\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.95\n",
    "Test score:       0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing both models, it turns out that using all variables is actually more helpful than using just a subset of variables. Apparently, random forest is not very susceptible to overfitting. Next, we try a support vector classifier (SVC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputed.loc[:, :'PEG_trailing']   # using all variables at our disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.94\n",
      "Test score:       0.95\n",
      "Best parameters: {'classifier': SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
      "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False), 'classifier__C': 100, 'classifier__gamma': 1, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and SVC estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [SVC(kernel='rbf')],\n",
    "               'classifier__gamma': [1, 10],\n",
    "               'classifier__C': [10, 100]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe,\n",
    "                  param_grid=param_grid,\n",
    "                  scoring='accuracy',\n",
    "                  cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.94\n",
    "Test score:       0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['roe', 'curr_ratio', 'bm', 'de_ratio', 'dpr', 'at_turn', 'debt_ebitda']   # using just a subset\n",
    "X = imputed.loc[:, subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.91\n",
      "Test score:       0.93\n",
      "Best parameters: {'classifier': SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=10, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 10, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and SVC estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [SVC(kernel='rbf')],\n",
    "               'classifier__gamma': [1, 10],\n",
    "               'classifier__C': [10, 100]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.91\n",
    "Test score:       0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for both the entire dataset and the subset, random forest works better than SVC. So from now on, we will only use random forest and the entire set of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are interested in how earlier data compares to more recent data in their predictive power. To do this, we split the dataset in two halves, one half containing all observations until the end of June 2013, the other half containing all later observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsthalf = imputed.set_index('public_date')\n",
    "firsthalf.sort_index()\n",
    "firsthalf = firsthalf.loc['20100131':'20130701']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_capital</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>0.2733</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>7.4565</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.5750</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.338</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>4.6775</td>\n",
       "      <td>-0.8018</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.153</td>\n",
       "      <td>3.287</td>\n",
       "      <td>7.8767</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-8.1385</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>4.3605</td>\n",
       "      <td>-2.2431</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31</th>\n",
       "      <td>0.2910</td>\n",
       "      <td>0.152</td>\n",
       "      <td>3.258</td>\n",
       "      <td>7.8767</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-8.1385</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>4.3605</td>\n",
       "      <td>-2.2431</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <td>0.2919</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.640</td>\n",
       "      <td>7.8767</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-8.1385</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>4.3605</td>\n",
       "      <td>-2.2350</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-31</th>\n",
       "      <td>0.2919</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.753</td>\n",
       "      <td>6.8026</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-8.4056</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366</td>\n",
       "      <td>-8.617</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>4.3605</td>\n",
       "      <td>0.2084</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-28</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.228</td>\n",
       "      <td>13.788</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.0580</td>\n",
       "      <td>5.9700</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-31</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.384</td>\n",
       "      <td>15.541</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.3200</td>\n",
       "      <td>5.9842</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-30</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.702</td>\n",
       "      <td>19.114</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.8530</td>\n",
       "      <td>4.4181</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-31</th>\n",
       "      <td>0.4520</td>\n",
       "      <td>1.690</td>\n",
       "      <td>19.380</td>\n",
       "      <td>0.3610</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.537</td>\n",
       "      <td>1.789</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.809</td>\n",
       "      <td>2.7350</td>\n",
       "      <td>3.5246</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-30</th>\n",
       "      <td>0.4520</td>\n",
       "      <td>1.692</td>\n",
       "      <td>19.402</td>\n",
       "      <td>0.3610</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.537</td>\n",
       "      <td>1.789</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.809</td>\n",
       "      <td>2.7380</td>\n",
       "      <td>3.5326</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12107 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bm     ps     pcf     dpr    npm    gpm    cfm    roa  \\\n",
       "public_date                                                              \n",
       "2010-01-31   0.2733  0.113  -5.253  7.4565 -0.072  0.163 -0.018  0.024   \n",
       "2010-02-28   0.2910  0.153   3.287  7.8767 -0.074  0.150 -0.018  0.010   \n",
       "2010-03-31   0.2910  0.152   3.258  7.8767 -0.074  0.150 -0.018  0.010   \n",
       "2010-04-30   0.2919  0.123   2.640  7.8767 -0.074  0.150 -0.018  0.010   \n",
       "2010-05-31   0.2919  0.127   2.753  6.8026 -0.079  0.150 -0.025  0.009   \n",
       "...             ...    ...     ...     ...    ...    ...    ...    ...   \n",
       "2013-02-28   0.4470  1.228  13.788  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-03-31   0.4470  1.384  15.541  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-04-30   0.4470  1.702  19.114  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-05-31   0.4520  1.690  19.380  0.3610  0.062  0.426  0.101  0.124   \n",
       "2013-06-30   0.4520  1.692  19.402  0.3610  0.062  0.426  0.101  0.124   \n",
       "\n",
       "                roe   roce  ...  debt_capital  de_ratio  cash_ratio  \\\n",
       "public_date                 ...                                       \n",
       "2010-01-31  -1.5750 -0.058  ...         1.338    -9.366       0.428   \n",
       "2010-02-28  -8.1385 -0.092  ...         1.376    -8.291       0.629   \n",
       "2010-03-31  -8.1385 -0.092  ...         1.376    -8.291       0.629   \n",
       "2010-04-30  -8.1385 -0.092  ...         1.376    -8.291       0.629   \n",
       "2010-05-31  -8.4056 -0.110  ...         1.366    -8.617       0.551   \n",
       "...             ...    ...  ...           ...       ...         ...   \n",
       "2013-02-28   0.1050  0.132  ...         0.426     1.248       0.550   \n",
       "2013-03-31   0.1050  0.132  ...         0.426     1.248       0.550   \n",
       "2013-04-30   0.1050  0.132  ...         0.426     1.248       0.550   \n",
       "2013-05-31   0.1040  0.135  ...         0.423     1.242       0.537   \n",
       "2013-06-30   0.1040  0.135  ...         0.423     1.242       0.537   \n",
       "\n",
       "             quick_ratio  curr_ratio  at_turn     ptb  PEG_trailing     gvkey  \\\n",
       "public_date                                                                     \n",
       "2010-01-31         0.603       0.664    0.816  4.6775       -0.8018    1045.0   \n",
       "2010-02-28         0.787       0.859    0.787  4.3605       -2.2431    1045.0   \n",
       "2010-03-31         0.787       0.859    0.787  4.3605       -2.2431    1045.0   \n",
       "2010-04-30         0.787       0.859    0.787  4.3605       -2.2350    1045.0   \n",
       "2010-05-31         0.712       0.780    0.799  4.3605        0.2084    1045.0   \n",
       "...                  ...         ...      ...     ...           ...       ...   \n",
       "2013-02-28         1.787       2.396    0.810  2.0580        5.9700  189491.0   \n",
       "2013-03-31         1.787       2.396    0.810  2.3200        5.9842  189491.0   \n",
       "2013-04-30         1.787       2.396    0.810  2.8530        4.4181  189491.0   \n",
       "2013-05-31         1.789       2.404    0.809  2.7350        3.5246  189491.0   \n",
       "2013-06-30         1.789       2.404    0.809  2.7380        3.5326  189491.0   \n",
       "\n",
       "             splticrm  \n",
       "public_date            \n",
       "2010-01-31         B-  \n",
       "2010-02-28         B-  \n",
       "2010-03-31         B-  \n",
       "2010-04-30         B-  \n",
       "2010-05-31         B-  \n",
       "...               ...  \n",
       "2013-02-28        BBB  \n",
       "2013-03-31        BBB  \n",
       "2013-04-30        BBB  \n",
       "2013-05-31        BBB  \n",
       "2013-06-30        BBB  \n",
       "\n",
       "[12107 rows x 40 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsthalf   # checking if splitting the data worked as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = firsthalf.loc[:, :'PEG_trailing']\n",
    "y = firsthalf.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.98\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.97\n",
    "Test score:       0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondhalf = imputed.set_index('public_date')\n",
    "secondhalf.sort_index()\n",
    "secondhalf = secondhalf.loc['20130701':'20170201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = secondhalf.loc[:, :'PEG_trailing']\n",
    "y = secondhalf.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.96\n",
    "Test score:       0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our surprise, it seems like the more recent data is worse at predicting the long term credit ratings. We wonder if this means that using lagged data would improve our prediction of them. We try this out, and we start by lagging all data by one month, then half a year, then one year, and finally three and a half years (which is basically the same as using the independent variables in firsthalf to predict the dependent variable in second half)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged = imputed.set_index(['public_date', 'gvkey']) # we need these indeces to instruct unstack().shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_month_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(1)\n",
    "# lagged.loc[:, :'PEG_trailing'] because we want to shift everything BUT splticrm\n",
    "# unstack() makes sure that the data is shifted within its group only, defined by gvkey\n",
    "# the 1 in shift() is for 1 month\n",
    "one_month_lag = one_month_lag.stack(dropna=False) # stack back together\n",
    "one_month_lag = pd.concat([one_month_lag, lagged.loc[:, 'splticrm']], axis = 'columns') # adding splticrm again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we added splticrm we need to get rid of the first month, for which shift created NAs\n",
    "one_month_lag = one_month_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_assets</th>\n",
       "      <th>debt_capital</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_date</th>\n",
       "      <th>gvkey</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2010-02-28</th>\n",
       "      <th>1045.0</th>\n",
       "      <td>0.2733</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>7.4565</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.575</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.120</td>\n",
       "      <td>1.338</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>4.6775</td>\n",
       "      <td>-0.8018</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075.0</th>\n",
       "      <td>1.2520</td>\n",
       "      <td>1.197</td>\n",
       "      <td>3.547</td>\n",
       "      <td>0.7720</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.719</td>\n",
       "      <td>0.562</td>\n",
       "      <td>2.596</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.7940</td>\n",
       "      <td>5.6816</td>\n",
       "      <td>BBB-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078.0</th>\n",
       "      <td>0.2570</td>\n",
       "      <td>2.313</td>\n",
       "      <td>9.064</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.455</td>\n",
       "      <td>1.390</td>\n",
       "      <td>0.553</td>\n",
       "      <td>1.296</td>\n",
       "      <td>1.554</td>\n",
       "      <td>0.625</td>\n",
       "      <td>3.5190</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161.0</th>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.995</td>\n",
       "      <td>6.506</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.981</td>\n",
       "      <td>68.517</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.645</td>\n",
       "      <td>1.916</td>\n",
       "      <td>0.731</td>\n",
       "      <td>7.2410</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209.0</th>\n",
       "      <td>0.3630</td>\n",
       "      <td>1.720</td>\n",
       "      <td>10.387</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.599</td>\n",
       "      <td>0.525</td>\n",
       "      <td>1.536</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.272</td>\n",
       "      <td>0.664</td>\n",
       "      <td>2.5700</td>\n",
       "      <td>2.5090</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2017-01-31</th>\n",
       "      <th>184700.0</th>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.179</td>\n",
       "      <td>3.227</td>\n",
       "      <td>7.5210</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.561</td>\n",
       "      <td>3.043</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.901</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.177</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>1.1199</td>\n",
       "      <td>BB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186310.0</th>\n",
       "      <td>1.1930</td>\n",
       "      <td>0.191</td>\n",
       "      <td>3.814</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.623</td>\n",
       "      <td>0.532</td>\n",
       "      <td>1.650</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.872</td>\n",
       "      <td>1.257</td>\n",
       "      <td>2.973</td>\n",
       "      <td>1.1830</td>\n",
       "      <td>3.7031</td>\n",
       "      <td>BB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186989.0</th>\n",
       "      <td>0.4020</td>\n",
       "      <td>2.031</td>\n",
       "      <td>22.723</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.340</td>\n",
       "      <td>1.270</td>\n",
       "      <td>1.907</td>\n",
       "      <td>0.929</td>\n",
       "      <td>2.6960</td>\n",
       "      <td>3.2642</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188255.0</th>\n",
       "      <td>0.4090</td>\n",
       "      <td>1.224</td>\n",
       "      <td>12.075</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.328</td>\n",
       "      <td>1.518</td>\n",
       "      <td>2.104</td>\n",
       "      <td>0.903</td>\n",
       "      <td>2.2320</td>\n",
       "      <td>11.5930</td>\n",
       "      <td>BBB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189491.0</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>11.099</td>\n",
       "      <td>29.996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>11.9470</td>\n",
       "      <td>5.3835</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25133 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bm      ps     pcf     dpr    npm    gpm    cfm  \\\n",
       "public_date gvkey                                                           \n",
       "2010-02-28  1045.0    0.2733   0.113  -5.253  7.4565 -0.072  0.163 -0.018   \n",
       "            1075.0    1.2520   1.197   3.547  0.7720  0.084  0.330  0.251   \n",
       "            1078.0    0.2570   2.313   9.064  0.4680  0.167  0.648  0.236   \n",
       "            1161.0    0.1280   0.995   6.506  0.0220  0.180  0.571  0.330   \n",
       "            1209.0    0.3630   1.720  10.387  0.4390  0.101  0.381  0.206   \n",
       "...                      ...     ...     ...     ...    ...    ...    ...   \n",
       "2017-01-31  184700.0  0.9050   0.179   3.227  7.5210  0.029  0.178  0.055   \n",
       "            186310.0  1.1930   0.191   3.814  0.0260  0.038  0.085  0.050   \n",
       "            186989.0  0.4020   2.031  22.723  0.1930  0.047  0.362  0.074   \n",
       "            188255.0  0.4090   1.224  12.075  0.1440  0.069  0.420  0.101   \n",
       "            189491.0  0.0740  11.099  29.996  0.0000  0.217  0.981  0.256   \n",
       "\n",
       "                        roa    roe   roce  ...  debt_assets  debt_capital  \\\n",
       "public_date gvkey                          ...                              \n",
       "2010-02-28  1045.0    0.024 -1.575 -0.058  ...        1.120         1.338   \n",
       "            1075.0    0.090  0.056  0.089  ...        0.719         0.562   \n",
       "            1078.0    0.193  0.259  0.199  ...        0.581         0.455   \n",
       "            1161.0    0.088 -0.671 -0.031  ...        0.885         0.981   \n",
       "            1209.0    0.175  0.161  0.143  ...        0.599         0.525   \n",
       "...                     ...    ...    ...  ...          ...           ...   \n",
       "2017-01-31  184700.0  0.094  0.127  0.123  ...        0.753         0.561   \n",
       "            186310.0  0.212  0.266  0.337  ...        0.623         0.532   \n",
       "            186989.0  0.094  0.065  0.104  ...        0.375         0.213   \n",
       "            188255.0  0.147  0.113  0.162  ...        0.495         0.361   \n",
       "            189491.0  0.239  0.254  0.250  ...        0.413         0.321   \n",
       "\n",
       "                      de_ratio  cash_ratio  quick_ratio  curr_ratio  at_turn  \\\n",
       "public_date gvkey                                                              \n",
       "2010-02-28  1045.0      -9.366       0.428        0.603       0.664    0.816   \n",
       "            1075.0       2.596       0.091        0.654       0.843    0.268   \n",
       "            1078.0       1.390       0.553        1.296       1.554    0.625   \n",
       "            1161.0      68.517       1.212        1.645       1.916    0.731   \n",
       "            1209.0       1.536       0.127        0.984       1.272    0.664   \n",
       "...                        ...         ...          ...         ...      ...   \n",
       "2017-01-31  184700.0     3.043       0.134        0.901       1.133    1.177   \n",
       "            186310.0     1.650       0.296        0.872       1.257    2.973   \n",
       "            186989.0     0.600       0.340        1.270       1.907    0.929   \n",
       "            188255.0     0.980       0.328        1.518       2.104    0.903   \n",
       "            189491.0     0.704       1.986        2.595       2.595    0.682   \n",
       "\n",
       "                          ptb  PEG_trailing  splticrm  \n",
       "public_date gvkey                                      \n",
       "2010-02-28  1045.0     4.6775       -0.8018        B-  \n",
       "            1075.0     0.7940        5.6816      BBB-  \n",
       "            1078.0     3.5190        0.2280        AA  \n",
       "            1161.0     7.2410        0.5450        B-  \n",
       "            1209.0     2.5700        2.5090         A  \n",
       "...                       ...           ...       ...  \n",
       "2017-01-31  184700.0   0.7790        1.1199       BB+  \n",
       "            186310.0   1.1830        3.7031       BB+  \n",
       "            186989.0   2.6960        3.2642       BBB  \n",
       "            188255.0   2.2320       11.5930      BBB+  \n",
       "            189491.0  11.9470        5.3835       BBB  \n",
       "\n",
       "[25133 rows x 39 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_month_lag   # checking that everything worked as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_month_lag.loc[:, :'PEG_trailing']\n",
    "y = one_month_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.96\n",
    "Test score:       0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_year_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(6)\n",
    "half_year_lag = half_year_lag.stack(dropna=False)\n",
    "half_year_lag = pd.concat([half_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_year_lag = half_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = half_year_lag.loc[:, :'PEG_trailing']\n",
    "y = half_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.96\n",
    "Test score:       0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(12)\n",
    "one_year_lag = one_year_lag.stack(dropna=False)\n",
    "one_year_lag = pd.concat([one_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_lag = one_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_year_lag.loc[:, :'PEG_trailing']\n",
    "y = one_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.96\n",
    "Test score:       0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "threehalf_year_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(42)\n",
    "threehalf_year_lag = threehalf_year_lag.stack(dropna=False)\n",
    "threehalf_year_lag = pd.concat([threehalf_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "threehalf_year_lag = threehalf_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = threehalf_year_lag.loc[:, :'PEG_trailing']\n",
    "y = threehalf_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.96\n",
    "Test score:       0.96"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turns out, all models with lagged data perform equally well, and while they are not particularly bad, they are a slight downgrade from using recent data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we try out what happens when we use a negative lag of one year. This means that we try to predict credit ratings by data that will only come out a year later, so we do not expect this model to perform well. After all, the existing ratios are supposed to influence the credit rating, and the future ratios do not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_lag = lagged.loc[:, :'PEG_trailing'].unstack().shift(-12)\n",
    "reverse_lag = reverse_lag.stack(dropna=False)\n",
    "reverse_lag = pd.concat([reverse_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_lag = reverse_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reverse_lag.loc[:, :'PEG_trailing']\n",
    "y = reverse_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.96\n",
    "Test score:       0.97"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our big surprise, this model actually performs better than using (correctly) lagged data, and it performs equally well as using no lagging of any kind. Since no-one can know the future ratings in advance, we believe this means that not only do the ratios affect the credit ratings, but the credit ratings must in turn affect the future ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to see if we can further improve our model (full set of variables, random forest) by adding even more information, that is less directly related to the credit ratings. That information consists of: GIC sector, common shares traded, price close, price high and price low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "moredat = pd.read_csv('additional_variables.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only keep the variables I'm interested in\n",
    "selCols = ['gvkey', 'fyear', 'cshtr_c', 'prcc_c', 'prch_c', 'prcl_c', 'gsector']\n",
    "moredat = moredat.loc[:, selCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable indfmt, which we don't need, is responsible for duplicates that we need to remove\n",
    "moredat = moredat.set_index(['gvkey', 'fyear'])\n",
    "moredat = moredat[~moredat.index.duplicated(keep='first')]\n",
    "moredat.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>fyear</th>\n",
       "      <th>cshtr_c</th>\n",
       "      <th>prcc_c</th>\n",
       "      <th>prch_c</th>\n",
       "      <th>prcl_c</th>\n",
       "      <th>gsector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>103342631.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>24.96</td>\n",
       "      <td>10.49</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>88443622.0</td>\n",
       "      <td>27.47</td>\n",
       "      <td>28.61</td>\n",
       "      <td>14.91</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>80676553.0</td>\n",
       "      <td>19.17</td>\n",
       "      <td>31.66</td>\n",
       "      <td>14.96</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>85991627.0</td>\n",
       "      <td>18.68</td>\n",
       "      <td>23.67</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>68957400.0</td>\n",
       "      <td>28.01</td>\n",
       "      <td>31.55</td>\n",
       "      <td>16.02</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81394</th>\n",
       "      <td>327451</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81395</th>\n",
       "      <td>328795</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81396</th>\n",
       "      <td>328795</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81397</th>\n",
       "      <td>328795</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81398</th>\n",
       "      <td>328795</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81399 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        gvkey   fyear      cshtr_c  prcc_c  prch_c  prcl_c  gsector\n",
       "0        1004  2009.0  103342631.0   22.98   24.96   10.49     20.0\n",
       "1        1004  2010.0   88443622.0   27.47   28.61   14.91     20.0\n",
       "2        1004  2011.0   80676553.0   19.17   31.66   14.96     20.0\n",
       "3        1004  2012.0   85991627.0   18.68   23.67   10.00     20.0\n",
       "4        1004  2013.0   68957400.0   28.01   31.55   16.02     20.0\n",
       "...       ...     ...          ...     ...     ...     ...      ...\n",
       "81394  327451  2016.0          NaN     NaN     NaN     NaN     20.0\n",
       "81395  328795  2013.0          NaN     NaN     NaN     NaN     20.0\n",
       "81396  328795  2014.0          NaN     NaN     NaN     NaN     20.0\n",
       "81397  328795  2015.0          NaN     NaN     NaN     NaN     20.0\n",
       "81398  328795  2016.0          NaN     NaN     NaN     NaN     20.0\n",
       "\n",
       "[81399 rows x 7 columns]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moredat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I also need the year in imputed, so I can merge the data sets on it\n",
    "imputed['fyear'] = imputed['public_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = pd.merge(imputed, moredat, on=['fyear', 'gvkey'], how='inner', validate = 'many_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>public_date</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>splticrm</th>\n",
       "      <th>fyear</th>\n",
       "      <th>cshtr_c</th>\n",
       "      <th>prcc_c</th>\n",
       "      <th>prch_c</th>\n",
       "      <th>prcl_c</th>\n",
       "      <th>gsector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1101</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>9.8788</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.5750</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.1641</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.153</td>\n",
       "      <td>3.287</td>\n",
       "      <td>11.6451</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.8044</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.2057</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.152</td>\n",
       "      <td>3.258</td>\n",
       "      <td>11.6451</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.8044</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.2057</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0951</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.640</td>\n",
       "      <td>11.6451</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.8044</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.2057</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0999</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.753</td>\n",
       "      <td>10.1018</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-1.9213</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.4429</td>\n",
       "      <td>2010-05-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24915</th>\n",
       "      <td>0.0780</td>\n",
       "      <td>11.892</td>\n",
       "      <td>30.675</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6905</td>\n",
       "      <td>2016-08-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24916</th>\n",
       "      <td>0.0780</td>\n",
       "      <td>11.083</td>\n",
       "      <td>28.586</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>4.6905</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24917</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>13.734</td>\n",
       "      <td>37.120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3142</td>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24918</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.452</td>\n",
       "      <td>33.655</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>6.3142</td>\n",
       "      <td>2016-11-30</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24919</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>11.099</td>\n",
       "      <td>29.996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>6.2857</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24920 rows Ã— 47 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bm      ps     pcf      dpr    npm    gpm    cfm    roa     roe  \\\n",
       "0      0.1101   0.113  -5.253   9.8788 -0.072  0.163 -0.018  0.024 -1.5750   \n",
       "1      0.0951   0.153   3.287  11.6451 -0.074  0.150 -0.018  0.010 -4.8044   \n",
       "2      0.0951   0.152   3.258  11.6451 -0.074  0.150 -0.018  0.010 -4.8044   \n",
       "3      0.0951   0.123   2.640  11.6451 -0.074  0.150 -0.018  0.010 -4.8044   \n",
       "4      0.0999   0.127   2.753  10.1018 -0.079  0.150 -0.025  0.009 -1.9213   \n",
       "...       ...     ...     ...      ...    ...    ...    ...    ...     ...   \n",
       "24915  0.0780  11.892  30.675   0.0000  0.243  0.982  0.279  0.258  0.2820   \n",
       "24916  0.0780  11.083  28.586   0.0000  0.243  0.982  0.279  0.258  0.2820   \n",
       "24917  0.0740  13.734  37.120   0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "24918  0.0740  12.452  33.655   0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "24919  0.0740  11.099  29.996   0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "\n",
       "        roce  ...  PEG_trailing  public_date     gvkey  splticrm  fyear  \\\n",
       "0     -0.058  ...       -5.1641   2010-01-31    1045.0        B-   2010   \n",
       "1     -0.092  ...       -6.2057   2010-02-28    1045.0        B-   2010   \n",
       "2     -0.092  ...       -6.2057   2010-03-31    1045.0        B-   2010   \n",
       "3     -0.092  ...       -6.2057   2010-04-30    1045.0        B-   2010   \n",
       "4     -0.110  ...       -3.4429   2010-05-31    1045.0        B-   2010   \n",
       "...      ...  ...           ...          ...       ...       ...    ...   \n",
       "24915  0.268  ...        4.6905   2016-08-31  189491.0       BBB   2016   \n",
       "24916  0.268  ...        4.6905   2016-09-30  189491.0       BBB   2016   \n",
       "24917  0.250  ...        6.3142   2016-10-31  189491.0       BBB   2016   \n",
       "24918  0.250  ...        6.3142   2016-11-30  189491.0       BBB   2016   \n",
       "24919  0.250  ...        6.2857   2016-12-31  189491.0       BBB   2016   \n",
       "\n",
       "            cshtr_c  prcc_c  prch_c   prcl_c  gsector  \n",
       "0      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "1      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "2      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "3      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "4      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "...             ...     ...     ...      ...      ...  \n",
       "24915  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "24916  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "24917  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "24918  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "24919  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "\n",
       "[24920 rows x 47 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gvkey      0\n",
       "fyear      0\n",
       "cshtr_c    0\n",
       "prcc_c     0\n",
       "prch_c     0\n",
       "prcl_c     0\n",
       "gsector    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isna(alldat.loc[:, selCols]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = np.append(numerical_dat.columns.values, ['cshtr_c', 'prcc_c', 'prch_c', 'prcl_c', 'gsector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = alldat.loc[:, numerical_cols]   # using all variables at our disposal\n",
    "y = pd.factorize(alldat.loc[:, 'splticrm'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.98\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best CV accuracy: 0.97\n",
    "Test score:       0.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we find that adding the variables GIC sector, common shares traded, price close, price high and price low has improved our prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
