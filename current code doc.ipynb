{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by loading and then merging the datasets we want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1 = pd.read_csv('key.csv', sep=',')\n",
    "dat2 = pd.read_csv('SP500_finratios.csv', sep=',', parse_dates=['adate', 'qdate', 'public_date'])\n",
    "dat3 = pd.read_csv('ratings2.csv', sep=',', parse_dates=['datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the columns of the first data set are renamed to match the names of the other data sets\n",
    "dat1.columns = ['gvkey','linktype','permno','permco','linkdt','linkenddt','conm','tic','cusip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the duplicates in the key data set are removed, so that pd.merge() will work\n",
    "#these duplicates come from differences in the variables linkdt and linkenddt, which we don't need\n",
    "dat1 = dat1.set_index('permno')\n",
    "dat1 = dat1[~dat1.index.duplicated(keep='first')]\n",
    "dat1.reset_index(inplace=True)   #The permnos are converted back to a normal variable, otherwise an error can occur when merging on permno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the variable datadate is renamed public_date and both are transformed to the same format, so\n",
    "# that pd.merge()recognises them as one and the same\n",
    "dat3['public_date'] = dat3['datadate']\n",
    "del dat3['datadate']\n",
    "dat2['public_date'] = pd.to_datetime(dat2.public_date)\n",
    "dat3['public_date'] = pd.to_datetime(dat3.public_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat1and2 = pd.merge(dat1, dat2, on='permno', how='inner', validate='one_to_many')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.merge(dat1and2, dat3, on=['gvkey', 'public_date', 'conm', 'tic', 'cusip'], how='inner', validate='one_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>linktype</th>\n",
       "      <th>permco</th>\n",
       "      <th>linkdt</th>\n",
       "      <th>linkenddt</th>\n",
       "      <th>conm</th>\n",
       "      <th>tic</th>\n",
       "      <th>cusip</th>\n",
       "      <th>adate</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_capital</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>DIVYIELD</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2008-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.338</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.376</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21020</td>\n",
       "      <td>1045</td>\n",
       "      <td>LC</td>\n",
       "      <td>20010</td>\n",
       "      <td>19500101</td>\n",
       "      <td>19620130</td>\n",
       "      <td>AMERICAN AIRLINES GROUP INC</td>\n",
       "      <td>AAL</td>\n",
       "      <td>02376R102</td>\n",
       "      <td>2009-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366</td>\n",
       "      <td>-8.617</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29853</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286</td>\n",
       "      <td>0.741</td>\n",
       "      <td>1.478</td>\n",
       "      <td>1.961</td>\n",
       "      <td>1.961</td>\n",
       "      <td>0.726</td>\n",
       "      <td>9.038</td>\n",
       "      <td>9.364</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29854</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>8.184</td>\n",
       "      <td>10.913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29855</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>9.392</td>\n",
       "      <td>12.469</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29856</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.775</td>\n",
       "      <td>1.467</td>\n",
       "      <td>2.010</td>\n",
       "      <td>2.010</td>\n",
       "      <td>0.678</td>\n",
       "      <td>8.557</td>\n",
       "      <td>11.359</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29857</th>\n",
       "      <td>13168</td>\n",
       "      <td>199356</td>\n",
       "      <td>LC</td>\n",
       "      <td>53964</td>\n",
       "      <td>20111221</td>\n",
       "      <td>E</td>\n",
       "      <td>TRIPADVISOR INC</td>\n",
       "      <td>TRIP</td>\n",
       "      <td>896945201</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289</td>\n",
       "      <td>0.789</td>\n",
       "      <td>1.396</td>\n",
       "      <td>1.944</td>\n",
       "      <td>1.944</td>\n",
       "      <td>0.681</td>\n",
       "      <td>7.114</td>\n",
       "      <td>11.603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29858 rows Ã— 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       permno   gvkey linktype  permco    linkdt linkenddt  \\\n",
       "0       21020    1045       LC   20010  19500101  19620130   \n",
       "1       21020    1045       LC   20010  19500101  19620130   \n",
       "2       21020    1045       LC   20010  19500101  19620130   \n",
       "3       21020    1045       LC   20010  19500101  19620130   \n",
       "4       21020    1045       LC   20010  19500101  19620130   \n",
       "...       ...     ...      ...     ...       ...       ...   \n",
       "29853   13168  199356       LC   53964  20111221         E   \n",
       "29854   13168  199356       LC   53964  20111221         E   \n",
       "29855   13168  199356       LC   53964  20111221         E   \n",
       "29856   13168  199356       LC   53964  20111221         E   \n",
       "29857   13168  199356       LC   53964  20111221         E   \n",
       "\n",
       "                              conm   tic      cusip      adate  ...  \\\n",
       "0      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2008-12-31  ...   \n",
       "1      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "2      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "3      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "4      AMERICAN AIRLINES GROUP INC   AAL  02376R102 2009-12-31  ...   \n",
       "...                            ...   ...        ...        ...  ...   \n",
       "29853              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29854              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29855              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29856              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "29857              TRIPADVISOR INC  TRIP  896945201 2014-12-31  ...   \n",
       "\n",
       "      debt_capital de_ratio  cash_ratio  quick_ratio  curr_ratio  at_turn  \\\n",
       "0            1.338   -9.366       0.428        0.603       0.664    0.816   \n",
       "1            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "2            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "3            1.376   -8.291       0.629        0.787       0.859    0.787   \n",
       "4            1.366   -8.617       0.551        0.712       0.780    0.799   \n",
       "...            ...      ...         ...          ...         ...      ...   \n",
       "29853        0.286    0.741       1.478        1.961       1.961    0.726   \n",
       "29854        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29855        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29856        0.278    0.775       1.467        2.010       2.010    0.678   \n",
       "29857        0.289    0.789       1.396        1.944       1.944    0.681   \n",
       "\n",
       "         ptb  PEG_trailing  DIVYIELD  splticrm  \n",
       "0        NaN           NaN       NaN        B-  \n",
       "1        NaN           NaN       NaN        B-  \n",
       "2        NaN           NaN       NaN        B-  \n",
       "3        NaN           NaN       NaN        B-  \n",
       "4        NaN           NaN       NaN        B-  \n",
       "...      ...           ...       ...       ...  \n",
       "29853  9.038         9.364       NaN       NaN  \n",
       "29854  8.184        10.913       NaN       NaN  \n",
       "29855  9.392        12.469       NaN       NaN  \n",
       "29856  8.557        11.359       NaN       NaN  \n",
       "29857  7.114        11.603       NaN       NaN  \n",
       "\n",
       "[29858 rows x 52 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's have a look at it. First of, we're interested in the distribution of the ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BBB     4946\n",
       "BBB+    4133\n",
       "A-      3615\n",
       "A       3196\n",
       "BBB-    2654\n",
       "A+      1670\n",
       "BB+     1262\n",
       "AA-      902\n",
       "BB-      824\n",
       "BB       757\n",
       "AA       417\n",
       "B+       305\n",
       "AAA      296\n",
       "AA+      199\n",
       "B-       133\n",
       "B        109\n",
       "CCC+      50\n",
       "D          4\n",
       "CCC        2\n",
       "Name: splticrm, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat['splticrm'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as there are only four observations of rating D, and only two observations of rating CCC, our data set does not allow us to draw any conclusions for these ratings and we have to drop them from our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat[dat['splticrm'] != 'CCC']\n",
    "dat = dat[dat['splticrm'] != 'D']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we consider our numerical data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              bm          ps         pcf        dpr        npm        gpm  \\\n",
      "min     0.001000    0.047000 -224.460000  -0.001000 -51.493000 -37.707000   \n",
      "mean    0.511099    2.530784   12.210846   0.489392   0.080365   0.431986   \n",
      "50%     0.391000    1.814000   11.113000   0.305000   0.092000   0.406000   \n",
      "max   137.237000  145.774000  280.893000  80.554000   1.799000   0.982000   \n",
      "\n",
      "            cfm       roa        roe      roce     efftax     GProf  \\\n",
      "min  -47.694000 -0.595000 -34.647000 -1.111000 -12.365000 -1.143000   \n",
      "mean   0.152864  0.142624   0.166138  0.176364   0.290338  0.294441   \n",
      "50%    0.152500  0.139000   0.137000  0.154000   0.302000  0.263500   \n",
      "max    2.054000  0.626000  15.502000  2.279000  29.944000  1.255000   \n",
      "\n",
      "      equity_invcap  debt_invcap  totdebt_invcap  capital_ratio     int_debt  \\\n",
      "min      -17.816000     0.000000        0.000000      -5.619000     0.000000   \n",
      "mean       0.593529     0.389615        0.471293       0.395291     0.336795   \n",
      "50%        0.633000     0.348000        0.401000       0.353000     0.055000   \n",
      "max        1.000000    18.816000       21.164000      18.816000  1278.000000   \n",
      "\n",
      "      int_totdebt   cash_lt  \n",
      "min      0.000000  0.000000  \n",
      "mean     0.079749  0.311298  \n",
      "50%      0.048000  0.147000  \n",
      "max    115.262000  6.502000   \n",
      "\n",
      "      invt_act   debt_at  debt_ebitda  short_debt  curr_debt   lt_debt  \\\n",
      "min   0.000000  0.000000 -2487.500000    0.000000   0.030000  0.000000   \n",
      "mean  0.222505  0.254492     2.430452    0.158646   0.395143  0.345054   \n",
      "50%   0.194000  0.242000     1.813000    0.097000   0.363000  0.364000   \n",
      "max   5.389000  1.729000  2328.100000    5.712000   1.000000  1.408000   \n",
      "\n",
      "       ocf_lct  cash_debt    fcf_ocf     dltt_be  debt_assets  debt_capital  \\\n",
      "min  -3.037000  -0.916000 -53.180000    0.000000     0.043000      0.002000   \n",
      "mean  0.717987   0.230432   0.571597    0.880104     0.618524      0.499772   \n",
      "50%   0.613000   0.169000   0.725000    0.488000     0.617000      0.472000   \n",
      "max   4.559000   5.736000   2.033000  157.897000     1.919000      4.234000   \n",
      "\n",
      "         de_ratio  cash_ratio  quick_ratio  curr_ratio   at_turn        ptb  \\\n",
      "min  -1228.100000    0.001000      0.09100    0.113000  0.013000   0.181000   \n",
      "mean     2.908976    0.764793      1.49689    1.899965  0.814618   3.899753   \n",
      "50%      1.550000    0.441000      1.18000    1.573000  0.657000   2.629000   \n",
      "max   1818.100000   11.403000     12.82500   12.825000  5.912000  65.486000   \n",
      "\n",
      "      PEG_trailing  \n",
      "min     -33.082000  \n",
      "mean      2.819965  \n",
      "50%       1.167000  \n",
      "max     100.212000  \n"
     ]
    }
   ],
   "source": [
    "des = dat.loc[:, 'bm':'cash_lt'].describe()\n",
    "ind = [3, 1, 5, 7]   #printing the entire .describe() information consumes unnecessarily much computation power, so I index the lines I'm interested in\n",
    "print(des.iloc[ind], '\\n')\n",
    "des = dat.loc[:, 'invt_act':].describe()  #I do this in two steps, because I don't want any variables hidden behind \"...\"\n",
    "ind = [3, 1, 5, 7]\n",
    "print(des.iloc[ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output suggests that several variables have extreme outliers - for instance bm has a minimum of 0.001000, a mean of  0.506463, but a maximum of 137.237000. Visualising the data with boxplots shows this quite notably. This means the imputation method we initially considered, which was based on linear regression, is probably not the best way to handle the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b1883fcc18>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# the outliers we show in these boxplots are so extreme that when we tried using linear regression for\n",
    "# the imputation, they influenced the slope so much that some negative values where imputed ...\n",
    "sns.boxplot(x = 'bm', data = dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b1884da908>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x = 'int_debt', data = dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b188f7c748>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWcAAAEHCAYAAAByTIfXAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAMwUlEQVR4nO3dbYyld1nH8d+1XUTbFQt2NUiJi4agQFTspgEsZFlMLNhQNCRgwJBoIkZTKGoqyJvddyQYg9anIGKJNsuLCoo1rBCggKLgLBAoD4UqT5UqJQqyNOHJvy/OPTC0M90zs3POXGf4fJLJztzzn/O/r3347tl7Z+6pMUYA6OXAXp8AAPcmzgANiTNAQ+IM0JA4AzR0cDuLL7nkknHkyJEFnQrA/nTmzJnPjTEOb+djthXnI0eOZG1tbXtnBfBtrqo+ud2PcVkDoCFxBmhInAEaEmeAhsQZoCFxBmhInAEaEmeAhsQZoCFxBmhInAEaEmeAhsQZoCFxBmhInAEaEmeAhsQZoCFxBmhInAEaWkqcr7/++lx//fXL2ApgX1hKnE+fPp3Tp08vYyuAfcFlDYCGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgIXEGaEicARoSZ4CGxBmgoYPL2OTuu+9exjYA+8ZS4jzGWMY2APuGyxoADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQENLjfOxY8c2fbnuuuty/PjxnDlzJmtrazl+/HhOnTr1jWOLtL7fvPvMu36rdcueDzg/223EbqkxxtyLjx49OtbW1ra9ybFjx+Zad+jQoSTJ2bNnv+XYzTffvO0953XVVVfl7Nmzc+8z7/qt1q0fX7fo+YDzs91GbKaqzowxjm7nYxb+zHneMCezKG8M1/qxRf2Ntba29o395tln3vVbrdt4fN0i5wPOz3YbsZsW/sx5O3HeyqKeXW73Wey867dad8/j8+4L7I3d+pfuQp45V9WvVNVaVa3ddddd2z6p3bBZ0BbxuOfaZ971W62bdz3Qw3YbsZvOGecxxivGGEfHGEcPHz68jHO6l/Vr0Yt+3HPtM+/6rdbNux7oYbuN2E0r8al0J0+eXMjjnjhxYlv7zLt+q3X3PD7vvsDe2G4jdtPC43zLLbfMvfbQoUOb/k112WWX7fJZzRw9evRbntWea59512+1buPxdYucDzg/223EbmrxzPnyyy/PgQMHcvLkyZw4cSIHDhzI8573vG8cW6T1/ebdZ971W61b9nzA+dluI3bLUj/PeTvPogH2i5af5wzA9okzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEMHl7FJVS1jG4B9YylxvvDCC5exDcC+4bIGQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEMHl7HJlVdeuYxtAPaNpcT5mmuuWcY2APuGyxoADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0JM4ADYkzQEPiDNCQOAM0VGOM+RdX3ZXkkzvc65Ikn9vhx3Zlpv722zyJmVbFxpl+cIxxeDsfvK04n4+qWhtjHF3KZktipv722zyJmVbF+c7ksgZAQ+IM0NAy4/yKJe61LGbqb7/Nk5hpVZzXTEu75gzA/FzWAGhInAEaWnicq+rKqrqtqm6vqhcter9FqKqHVtVbq+rDVfXBqnrBdPxBVfWmqvrY9OMD9/pct6uqLqiq91bVzdPbKz1TVV1cVTdV1UemX6/HrfJMVfXC6ffcrVV1qqq+cxXnqapXVdVnq+rWDce2nKOqXjw147aq+pm9OeutbTHPy6bfd++vqtdV1cUb3rfteRYa56q6IMkfJXlKkkcm+YWqeuQi91yQryX5zTHGjyZ5bJJfn+Z4UZI3jzEenuTN09ur5gVJPrzh7VWf6feTnB5j/EiSH89stpWcqaoekuT5SY6OMR6d5IIkz8pqznNDkivvcWzTOaY/W89K8qjpY/54akknN+Te87wpyaPHGD+W5KNJXpzsfJ5FP3O+PMntY4x/H2N8Jclrkly94D133RjjzjHGe6bXv5jZH/iHZDbLq6dlr07y9L05w52pqkuT/GySV244vLIzVdUDkjwxyZ8nyRjjK2OMz2eFZ0pyMMl3VdXBJBcm+UxWcJ4xxtuT/Pc9Dm81x9VJXjPG+PIY4+NJbs+sJW1sNs8Y441jjK9Nb/5Lkkun13c0z6Lj/JAkn97w9h3TsZVVVUeSPCbJu5J8/xjjzmQW8CTft3dntiMvT3Jdkv/bcGyVZ/qhJHcl+YvpUs0rq+qirOhMY4z/SPK7ST6V5M4kXxhjvDErOs8mtppjP3Tjl5K8YXp9R/MsOs61ybGV/dy9qjqU5K+TXDvG+N+9Pp/zUVVXJfnsGOPMXp/LLjqY5CeT/MkY4zFJvpTV+Cf/pqZrsFcneViSH0hyUVU9Z2/PailWuhtV9ZLMLoXeuH5ok2XnnGfRcb4jyUM3vH1pZv8sWzlVdb/MwnzjGOO10+H/qqoHT+9/cJLP7tX57cBPJXlaVX0is8tNx6vqr7LaM92R5I4xxrumt2/KLNarOtNPJ/n4GOOuMcZXk7w2yeOzuvPc01ZzrGw3quq5Sa5K8uzxzS8i2dE8i47zvyZ5eFU9rKq+I7OL4q9f8J67rqoqs+uYHx5j/N6Gd70+yXOn15+b5G+XfW47NcZ48Rjj0jHGkcx+Xd4yxnhOVnum/0zy6ap6xHToyUk+lNWd6VNJHltVF06/B5+c2f93rOo897TVHK9P8qyqun9VPSzJw5O8ew/Ob1uq6sokv53kaWOMuze8a2fzjDEW+pLkqZn9z+W/JXnJovdb0AxXZPbPkPcned/08tQk35vZ/zJ/bPrxQXt9rjuc71iSm6fXV3qmJD+RZG36tfqbJA9c5ZmSnEzykSS3JvnLJPdfxXmSnMrsuvlXM3sm+cv3NUeSl0zNuC3JU/b6/Oec5/bMri2vN+JPz2ceX74N0JCvEARoSJwBGhJngIbEGaAhcQZoSJwBGhJnFqqq3jnHmmur6sJzrPmdOR7n4qr6tft4/w1V9YxzPMYtVXWv75hcVceq6vHnOgfYLeLMQo0x5gnatZndce2+nDPOSS5OsmWcz9OxzL50GpZCnFmoqjo7/Xhsela6fiP8G2vm+Znd1OetVfXWLR7jpZndNvN9VXXjdOw3phvQ31pV105LX5rkh6d1L5se/w+r6kNV9ffZcPe2qrqsqt5WVWeq6h/W7/EweU5VvXN67MunOxH+apIXTo/9hF3+aYJ72+svg/Syv1+SnJ1+PJbkC5nd9OVAkn9OcsX0vk8kuWSex5levyzJB5JclORQkg9mdhvXI0lu3bDu5zO7AfoFmf0F8Pkkz0hyvyTvTHJ4WvfMJK+aXr8lyZ9Nrz9x/fGSnEjyW3v98+nl2+fl4G7HHu7Du8cYdyRJVb0vs5j+4w4e54okrxtjfGl6rNcmeULufVOtJyY5Ncb4epLPVNVbpuOPSPLoJG+a3U8oF2R2n4R1p5LZDdWr6gEbv90QLIs4s0xf3vD617Pz33+b3R93K5vdPKaSfHCM8bg5P8YNaFg615zp4ItJvvsca7463VM7Sd6e5OnTrTQvSvJzSd6xyeO8PbNbNV4wXVN+0nT8tiSHq+pxyexe3VX1qA0f98zp+BWZffeRL8x5jrBrPHOmg1ckeUNV3TnGeNJ9rHl/Vb1njPHsqroh37wn7ivHGO9Nkqr6p5p9R+Q3ZPYtuI5ndn36o0nelsy+t+D0KXV/UFXfk9mfg5dndu06Sf5n+hTAB2T27YaS5O+S3FRVVye5Zozxjt0aHjbjlqEADbmsAdCQyxq0UlXvyuy7fWz0i2OMD+zF+cBecVkDoCGXNQAaEmeAhsQZoCFxBmjo/wEgAtqFwZsvpQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(x = 'int_totdebt', data = dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also noticed that DIVYIELD was not included in the summary above. Apparently, it is encoded as a string, which we fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.loc[:, 'DIVYIELD'] = dat.loc[:, 'DIVYIELD'].str.replace('%', '')\n",
    "dat.loc[:, 'DIVYIELD'] = dat.loc[:, 'DIVYIELD'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    23116.000000\n",
       "mean         2.295734\n",
       "std          1.333045\n",
       "min          0.030000\n",
       "25%          1.320000\n",
       "50%          2.150000\n",
       "75%          3.060000\n",
       "max         16.000000\n",
       "Name: DIVYIELD, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.loc[:, 'DIVYIELD'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at the data we have, we take a look at the data we do not have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a little indulgence\n",
    "class color:\n",
    "   bold = '\\033[1m'\n",
    "   end = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mColumn Names         Total NAs      NAs per observations\u001b[0m\n",
      "permno               0              0.000000\n",
      "gvkey                0              0.000000\n",
      "linktype             0              0.000000\n",
      "permco               0              0.000000\n",
      "linkdt               0              0.000000\n",
      "linkenddt            0              0.000000\n",
      "conm                 0              0.000000\n",
      "tic                  0              0.000000\n",
      "cusip                0              0.000000\n",
      "adate                13             0.000435\n",
      "qdate                0              0.000000\n",
      "public_date          0              0.000000\n",
      "bm                   638            0.021372\n",
      "ps                   12             0.000402\n",
      "pcf                  27             0.000904\n",
      "dpr                  1862           0.062374\n",
      "npm                  12             0.000402\n",
      "gpm                  12             0.000402\n",
      "cfm                  98             0.003283\n",
      "roa                  27             0.000904\n",
      "roe                  666            0.022310\n",
      "roce                 109            0.003651\n",
      "efftax               2108           0.070615\n",
      "GProf                12             0.000402\n",
      "equity_invcap        18             0.000603\n",
      "debt_invcap          65             0.002177\n",
      "totdebt_invcap       82             0.002747\n",
      "capital_ratio        59             0.001976\n",
      "int_debt             3213           0.107631\n",
      "int_totdebt          3044           0.101970\n",
      "cash_lt              26             0.000871\n",
      "invt_act             4275           0.143206\n",
      "debt_at              76             0.002546\n",
      "debt_ebitda          162            0.005427\n",
      "short_debt           1146           0.038389\n",
      "curr_debt            4016           0.134530\n",
      "lt_debt              59             0.001976\n",
      "ocf_lct              4011           0.134363\n",
      "cash_debt            140            0.004690\n",
      "fcf_ocf              571            0.019128\n",
      "dltt_be              661            0.022143\n",
      "debt_assets          26             0.000871\n",
      "debt_capital         197            0.006599\n",
      "de_ratio             26             0.000871\n",
      "cash_ratio           4002           0.134061\n",
      "quick_ratio          4002           0.134061\n",
      "curr_ratio           4002           0.134061\n",
      "at_turn              27             0.000904\n",
      "ptb                  638            0.021372\n",
      "PEG_trailing         10354          0.346844\n",
      "DIVYIELD             6736           0.225647\n",
      "splticrm             4384           0.146858\n"
     ]
    }
   ],
   "source": [
    "col_Names = dat.columns.values\n",
    "total_NAs = pd.isna(dat).sum()\n",
    "percentage_NAs = dat.isna().sum()/len(dat)\n",
    "print(color.bold + \"%-20s %-14s %s\" %(\"Column Names\", \"Total NAs\", \"NAs per observations\") + color.end )\n",
    "#I used the % operator because tab didn't work and this allows me to define the spaces between the items\n",
    "\n",
    "#the loop prints one line after another\n",
    "for item_a, item_b, item_c in zip(col_Names, total_NAs, percentage_NAs):\n",
    "    print(\"%-20s %-14d %.6f\" %(item_a, item_b, item_c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown here, 4384 observations have no long term credit rating (splticrm), which means we cannot use those observations for our prediction models. Still, the observations might be helpful for imputing missing values, so we will drop them later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another variable that stands out is PEG_trailing, and we are unsure how valid it is to impute more than one third of the data. However, dropping the missing values in PEG_trailing is not an option, since they are not missing completely at random: We can show that the missingness is systematic by plotting it against another variable. So, if we were to just drop the missing values, we would bias the remaining data. Instead, we could drop the entire variable, but we might as well keep it in for now, impute the missing values, and if we later find that it reduces the predictive power, we can still drop it. (Spoiler: It does not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2b188fd1c50>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.regplot(dat.pcf, dat.PEG_trailing.isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we fill in the missing data by using IterativeImputer. Its method is to model each feature with missing values as a function of other features in a round-robin regression. As stated above, linear regression is not an ideal function here, so we decided to use scikit-learn's ExtraTreesRegressor, because it works better with extreme outliers and non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_dat = dat.loc[:, 'bm':'DIVYIELD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the imputation method\n",
    "random.seed(42)\n",
    "imp = IterativeImputer(estimator = ExtraTreesRegressor(n_estimators=10, n_jobs=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\impute\\_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# actually imputing the values\n",
    "imputed = imp.fit_transform(numerical_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the imputation returns an array, so we transform the data back to a dataframe\n",
    "imputed = pd.DataFrame(data = imputed, columns = numerical_dat.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the missing data is imputed, we add public_date, gvkey and splticrm to the data frame. The first two will be needed later for further analysis, the latter is added so we can drop all observations with missing credit rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed = pd.concat([imputed, dat.loc[:, 'public_date'], dat.loc[:, 'gvkey'], dat.loc[:, 'splticrm']], axis = 'columns')\n",
    "imputed = imputed.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>DIVYIELD</th>\n",
       "      <th>public_date</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1496</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>2.6608</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.5750</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>10.2184</td>\n",
       "      <td>-3.7486</td>\n",
       "      <td>3.011</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.153</td>\n",
       "      <td>3.287</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>14.9507</td>\n",
       "      <td>-3.1414</td>\n",
       "      <td>2.890</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.152</td>\n",
       "      <td>3.258</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>14.9507</td>\n",
       "      <td>-3.1414</td>\n",
       "      <td>2.890</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.640</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>14.9507</td>\n",
       "      <td>-3.1414</td>\n",
       "      <td>2.890</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.753</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-4.4832</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>13.8473</td>\n",
       "      <td>-3.1414</td>\n",
       "      <td>3.424</td>\n",
       "      <td>2010-05-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29832</th>\n",
       "      <td>0.0780</td>\n",
       "      <td>11.083</td>\n",
       "      <td>28.586</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>1.982</td>\n",
       "      <td>2.851</td>\n",
       "      <td>2.851</td>\n",
       "      <td>0.650</td>\n",
       "      <td>11.9060</td>\n",
       "      <td>1.7537</td>\n",
       "      <td>1.199</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29833</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>13.734</td>\n",
       "      <td>37.120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>14.7840</td>\n",
       "      <td>3.6747</td>\n",
       "      <td>1.518</td>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29834</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.452</td>\n",
       "      <td>33.655</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>13.4040</td>\n",
       "      <td>2.8981</td>\n",
       "      <td>1.458</td>\n",
       "      <td>2016-11-30</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29835</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>11.099</td>\n",
       "      <td>29.996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>11.9470</td>\n",
       "      <td>2.8981</td>\n",
       "      <td>1.458</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29836</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.672</td>\n",
       "      <td>30.415</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714</td>\n",
       "      <td>2.451</td>\n",
       "      <td>2.451</td>\n",
       "      <td>0.665</td>\n",
       "      <td>13.1570</td>\n",
       "      <td>2.7580</td>\n",
       "      <td>1.442</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25468 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bm      ps     pcf     dpr    npm    gpm    cfm    roa     roe  \\\n",
       "0      0.1496   0.113  -5.253  2.6608 -0.072  0.163 -0.018  0.024 -1.5750   \n",
       "1      0.0710   0.153   3.287  7.3744 -0.074  0.150 -0.018  0.010 -4.4859   \n",
       "2      0.0710   0.152   3.258  7.3744 -0.074  0.150 -0.018  0.010 -4.4859   \n",
       "3      0.0710   0.123   2.640  7.3744 -0.074  0.150 -0.018  0.010 -4.4859   \n",
       "4      0.0964   0.127   2.753  7.3744 -0.079  0.150 -0.025  0.009 -4.4832   \n",
       "...       ...     ...     ...     ...    ...    ...    ...    ...     ...   \n",
       "29832  0.0780  11.083  28.586  0.0000  0.243  0.982  0.279  0.258  0.2820   \n",
       "29833  0.0740  13.734  37.120  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29834  0.0740  12.452  33.655  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29835  0.0740  11.099  29.996  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "29836  0.0740  12.672  30.415  0.0000  0.212  0.978  0.253  0.240  0.2500   \n",
       "\n",
       "        roce  ...  cash_ratio  quick_ratio  curr_ratio  at_turn      ptb  \\\n",
       "0     -0.058  ...       0.428        0.603       0.664    0.816  10.2184   \n",
       "1     -0.092  ...       0.629        0.787       0.859    0.787  14.9507   \n",
       "2     -0.092  ...       0.629        0.787       0.859    0.787  14.9507   \n",
       "3     -0.092  ...       0.629        0.787       0.859    0.787  14.9507   \n",
       "4     -0.110  ...       0.551        0.712       0.780    0.799  13.8473   \n",
       "...      ...  ...         ...          ...         ...      ...      ...   \n",
       "29832  0.268  ...       1.982        2.851       2.851    0.650  11.9060   \n",
       "29833  0.250  ...       1.986        2.595       2.595    0.682  14.7840   \n",
       "29834  0.250  ...       1.986        2.595       2.595    0.682  13.4040   \n",
       "29835  0.250  ...       1.986        2.595       2.595    0.682  11.9470   \n",
       "29836  0.250  ...       1.714        2.451       2.451    0.665  13.1570   \n",
       "\n",
       "       PEG_trailing  DIVYIELD  public_date     gvkey  splticrm  \n",
       "0           -3.7486     3.011   2010-01-31    1045.0        B-  \n",
       "1           -3.1414     2.890   2010-02-28    1045.0        B-  \n",
       "2           -3.1414     2.890   2010-03-31    1045.0        B-  \n",
       "3           -3.1414     2.890   2010-04-30    1045.0        B-  \n",
       "4           -3.1414     3.424   2010-05-31    1045.0        B-  \n",
       "...             ...       ...          ...       ...       ...  \n",
       "29832        1.7537     1.199   2016-09-30  189491.0       BBB  \n",
       "29833        3.6747     1.518   2016-10-31  189491.0       BBB  \n",
       "29834        2.8981     1.458   2016-11-30  189491.0       BBB  \n",
       "29835        2.8981     1.458   2016-12-31  189491.0       BBB  \n",
       "29836        2.7580     1.442   2017-01-31  189491.0       BBB  \n",
       "\n",
       "[25468 rows x 42 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the time has come to train our prediction model. We assume that including all variables will lead to overfitting, and using just a subset of all variables should improve the result. We test that assumption by trying out both."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputed.loc[:, :'DIVYIELD']   # using all variables at our disposal\n",
    "y = pd.factorize(imputed.loc[:, 'splticrm'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['roe', 'curr_ratio', 'bm', 'de_ratio', 'dpr', 'at_turn', 'debt_ebitda']   # using just a subset\n",
    "X = imputed.loc[:, subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.95\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When comparing both models, it turns out that using all variables is actually more helpful than using just a subset of variables. Apparently, random forest is not very susceptible to overfitting. Now we come back to the variable PEG_trailing, of which we weren't sure whether to include it, because it had so many missing values. We try out if we can improve the 'full set' model by just excluding this one variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputed.loc[:, :'ptb']\n",
    "X = pd.concat([X, imputed.loc[:, 'DIVYIELD']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the prediction score does not change when we exclude PEG_trailing, so we might as well leave it in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we try using a support vector classifier (SVC) instead of random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = imputed.loc[:, :'DIVYIELD']   # using all variables at our disposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.93\n",
      "Test score:       0.95\n",
      "Best parameters: {'classifier': SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=1, kernel='rbf', max_iter=-1,\n",
      "    probability=False, random_state=None, shrinking=True, tol=0.001,\n",
      "    verbose=False), 'classifier__C': 10, 'classifier__gamma': 1, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and SVC estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [SVC(kernel='rbf')],\n",
    "               'classifier__gamma': [1, 10],\n",
    "               'classifier__C': [10, 100]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.91\n",
      "Test score:       0.93\n",
      "Best parameters: {'classifier': SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=1, kernel='poly',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 1, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Doing the same with another kernel function\n",
    "# Create Pipeline object with standard scaler and SVC estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [SVC(kernel='poly')],\n",
    "               'classifier__gamma': [1, 10],\n",
    "               'classifier__C': [10, 100]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.11\n",
      "Test score:       0.10\n",
      "Best parameters: {'classifier': SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=10, kernel='sigmoid',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False), 'classifier__C': 10, 'classifier__gamma': 10, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Doing the same with another kernel function\n",
    "# Create Pipeline object with standard scaler and SVC estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [SVC(kernel='sigmoid')],\n",
    "               'classifier__gamma': [1, 10],\n",
    "               'classifier__C': [10, 100]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, using the radial bias function as the kernel is the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset = ['roe', 'curr_ratio', 'bm', 'de_ratio', 'dpr', 'at_turn', 'debt_ebitda']   # using just a subset\n",
    "X = imputed.loc[:, subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.91\n",
      "Test score:       0.93\n",
      "Best parameters: {'classifier': SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
      "    decision_function_shape='ovr', degree=3, gamma=10, kernel='rbf',\n",
      "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "    tol=0.001, verbose=False), 'classifier__C': 100, 'classifier__gamma': 10, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and SVC estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', SVC())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [SVC(kernel='rbf')],\n",
    "               'classifier__gamma': [1, 10],\n",
    "               'classifier__C': [10, 100]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that for both the entire dataset and the subset, random forest works better than SVC. So from now on, we will only use random forest and the entire set of variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are interested in how earlier data compares to more recent data in their predictive power. To do this, we split the dataset in two halves, one half containing all observations until the end of June 2013, the other half containing all later observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "firsthalf = imputed.set_index('public_date')\n",
    "firsthalf.sort_index()\n",
    "firsthalf = firsthalf.loc['20100131':'20130701']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>DIVYIELD</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>0.1496</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>2.6608</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.5750</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>10.2184</td>\n",
       "      <td>-3.7486</td>\n",
       "      <td>3.011</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.153</td>\n",
       "      <td>3.287</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>14.9507</td>\n",
       "      <td>-3.1414</td>\n",
       "      <td>2.890</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-31</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.152</td>\n",
       "      <td>3.258</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>14.9507</td>\n",
       "      <td>-3.1414</td>\n",
       "      <td>2.890</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-04-30</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.640</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.291</td>\n",
       "      <td>0.629</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.859</td>\n",
       "      <td>0.787</td>\n",
       "      <td>14.9507</td>\n",
       "      <td>-3.1414</td>\n",
       "      <td>2.890</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-05-31</th>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.753</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-4.4832</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>-8.617</td>\n",
       "      <td>0.551</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.780</td>\n",
       "      <td>0.799</td>\n",
       "      <td>13.8473</td>\n",
       "      <td>-3.1414</td>\n",
       "      <td>3.424</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-28</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.228</td>\n",
       "      <td>13.788</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.0580</td>\n",
       "      <td>8.1603</td>\n",
       "      <td>1.880</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-31</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.384</td>\n",
       "      <td>15.541</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.3200</td>\n",
       "      <td>4.9096</td>\n",
       "      <td>1.670</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-30</th>\n",
       "      <td>0.4470</td>\n",
       "      <td>1.702</td>\n",
       "      <td>19.114</td>\n",
       "      <td>0.3410</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.1050</td>\n",
       "      <td>0.132</td>\n",
       "      <td>...</td>\n",
       "      <td>1.248</td>\n",
       "      <td>0.550</td>\n",
       "      <td>1.787</td>\n",
       "      <td>2.396</td>\n",
       "      <td>0.810</td>\n",
       "      <td>2.8530</td>\n",
       "      <td>1.6294</td>\n",
       "      <td>1.350</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-31</th>\n",
       "      <td>0.4520</td>\n",
       "      <td>1.690</td>\n",
       "      <td>19.380</td>\n",
       "      <td>0.3610</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.537</td>\n",
       "      <td>1.789</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.809</td>\n",
       "      <td>2.7350</td>\n",
       "      <td>1.4140</td>\n",
       "      <td>1.350</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-06-30</th>\n",
       "      <td>0.4520</td>\n",
       "      <td>1.692</td>\n",
       "      <td>19.402</td>\n",
       "      <td>0.3610</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.1040</td>\n",
       "      <td>0.135</td>\n",
       "      <td>...</td>\n",
       "      <td>1.242</td>\n",
       "      <td>0.537</td>\n",
       "      <td>1.789</td>\n",
       "      <td>2.404</td>\n",
       "      <td>0.809</td>\n",
       "      <td>2.7380</td>\n",
       "      <td>1.4140</td>\n",
       "      <td>1.350</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12107 rows Ã— 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 bm     ps     pcf     dpr    npm    gpm    cfm    roa  \\\n",
       "public_date                                                              \n",
       "2010-01-31   0.1496  0.113  -5.253  2.6608 -0.072  0.163 -0.018  0.024   \n",
       "2010-02-28   0.0710  0.153   3.287  7.3744 -0.074  0.150 -0.018  0.010   \n",
       "2010-03-31   0.0710  0.152   3.258  7.3744 -0.074  0.150 -0.018  0.010   \n",
       "2010-04-30   0.0710  0.123   2.640  7.3744 -0.074  0.150 -0.018  0.010   \n",
       "2010-05-31   0.0964  0.127   2.753  7.3744 -0.079  0.150 -0.025  0.009   \n",
       "...             ...    ...     ...     ...    ...    ...    ...    ...   \n",
       "2013-02-28   0.4470  1.228  13.788  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-03-31   0.4470  1.384  15.541  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-04-30   0.4470  1.702  19.114  0.3410  0.062  0.427  0.102  0.123   \n",
       "2013-05-31   0.4520  1.690  19.380  0.3610  0.062  0.426  0.101  0.124   \n",
       "2013-06-30   0.4520  1.692  19.402  0.3610  0.062  0.426  0.101  0.124   \n",
       "\n",
       "                roe   roce  ...  de_ratio  cash_ratio  quick_ratio  \\\n",
       "public_date                 ...                                      \n",
       "2010-01-31  -1.5750 -0.058  ...    -9.366       0.428        0.603   \n",
       "2010-02-28  -4.4859 -0.092  ...    -8.291       0.629        0.787   \n",
       "2010-03-31  -4.4859 -0.092  ...    -8.291       0.629        0.787   \n",
       "2010-04-30  -4.4859 -0.092  ...    -8.291       0.629        0.787   \n",
       "2010-05-31  -4.4832 -0.110  ...    -8.617       0.551        0.712   \n",
       "...             ...    ...  ...       ...         ...          ...   \n",
       "2013-02-28   0.1050  0.132  ...     1.248       0.550        1.787   \n",
       "2013-03-31   0.1050  0.132  ...     1.248       0.550        1.787   \n",
       "2013-04-30   0.1050  0.132  ...     1.248       0.550        1.787   \n",
       "2013-05-31   0.1040  0.135  ...     1.242       0.537        1.789   \n",
       "2013-06-30   0.1040  0.135  ...     1.242       0.537        1.789   \n",
       "\n",
       "             curr_ratio  at_turn      ptb  PEG_trailing  DIVYIELD     gvkey  \\\n",
       "public_date                                                                   \n",
       "2010-01-31        0.664    0.816  10.2184       -3.7486     3.011    1045.0   \n",
       "2010-02-28        0.859    0.787  14.9507       -3.1414     2.890    1045.0   \n",
       "2010-03-31        0.859    0.787  14.9507       -3.1414     2.890    1045.0   \n",
       "2010-04-30        0.859    0.787  14.9507       -3.1414     2.890    1045.0   \n",
       "2010-05-31        0.780    0.799  13.8473       -3.1414     3.424    1045.0   \n",
       "...                 ...      ...      ...           ...       ...       ...   \n",
       "2013-02-28        2.396    0.810   2.0580        8.1603     1.880  189491.0   \n",
       "2013-03-31        2.396    0.810   2.3200        4.9096     1.670  189491.0   \n",
       "2013-04-30        2.396    0.810   2.8530        1.6294     1.350  189491.0   \n",
       "2013-05-31        2.404    0.809   2.7350        1.4140     1.350  189491.0   \n",
       "2013-06-30        2.404    0.809   2.7380        1.4140     1.350  189491.0   \n",
       "\n",
       "             splticrm  \n",
       "public_date            \n",
       "2010-01-31         B-  \n",
       "2010-02-28         B-  \n",
       "2010-03-31         B-  \n",
       "2010-04-30         B-  \n",
       "2010-05-31         B-  \n",
       "...               ...  \n",
       "2013-02-28        BBB  \n",
       "2013-03-31        BBB  \n",
       "2013-04-30        BBB  \n",
       "2013-05-31        BBB  \n",
       "2013-06-30        BBB  \n",
       "\n",
       "[12107 rows x 41 columns]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firsthalf   # checking if splitting the data worked as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = firsthalf.loc[:, :'DIVYIELD']\n",
    "y = firsthalf.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.98\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "secondhalf = imputed.set_index('public_date')\n",
    "secondhalf.sort_index()\n",
    "secondhalf = secondhalf.loc['20130701':'20170201']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = secondhalf.loc[:, :'DIVYIELD']\n",
    "y = secondhalf.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our surprise, it seems like the more recent data is slightly worse at predicting the long term credit ratings. (Yes, the difference is small, but we tried many different random.seeds and the difference is consistent.) We wonder if this means that using lagged data would improve our prediction of the ratings. We try this out, and we start by lagging all data by one month, then half a year, then one year, and finally three and a half years (which is basically the same as using the independent variables in firsthalf to predict the dependent variable in second half)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "lagged = imputed.set_index(['public_date', 'gvkey']) # we need these indeces to instruct unstack().shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_month_lag = lagged.loc[:, :'DIVYIELD'].unstack().shift(1)\n",
    "# lagged.loc[:, :'DIVYIELD'] because we want to shift everything BUT splticrm\n",
    "# unstack() makes sure that the data is shifted within its group only, defined by gvkey\n",
    "# the 1 in shift() is for 1 month\n",
    "one_month_lag = one_month_lag.stack(dropna=False) # stack back together\n",
    "one_month_lag = pd.concat([one_month_lag, lagged.loc[:, 'splticrm']], axis = 'columns') # adding splticrm again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after we added splticrm we need to get rid of the first month, for which shift created NAs\n",
    "one_month_lag = one_month_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>debt_capital</th>\n",
       "      <th>de_ratio</th>\n",
       "      <th>cash_ratio</th>\n",
       "      <th>quick_ratio</th>\n",
       "      <th>curr_ratio</th>\n",
       "      <th>at_turn</th>\n",
       "      <th>ptb</th>\n",
       "      <th>PEG_trailing</th>\n",
       "      <th>DIVYIELD</th>\n",
       "      <th>splticrm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public_date</th>\n",
       "      <th>gvkey</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2010-02-28</th>\n",
       "      <th>1045.0</th>\n",
       "      <td>0.1496</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>2.6608</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.575</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>1.338</td>\n",
       "      <td>-9.366</td>\n",
       "      <td>0.428</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.664</td>\n",
       "      <td>0.816</td>\n",
       "      <td>10.2184</td>\n",
       "      <td>-3.7486</td>\n",
       "      <td>3.011</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1075.0</th>\n",
       "      <td>1.2520</td>\n",
       "      <td>1.197</td>\n",
       "      <td>3.547</td>\n",
       "      <td>0.7720</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562</td>\n",
       "      <td>2.596</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.654</td>\n",
       "      <td>0.843</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.7940</td>\n",
       "      <td>9.1553</td>\n",
       "      <td>5.980</td>\n",
       "      <td>BBB-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1078.0</th>\n",
       "      <td>0.2570</td>\n",
       "      <td>2.313</td>\n",
       "      <td>9.064</td>\n",
       "      <td>0.4680</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.648</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.193</td>\n",
       "      <td>0.259</td>\n",
       "      <td>0.199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455</td>\n",
       "      <td>1.390</td>\n",
       "      <td>0.553</td>\n",
       "      <td>1.296</td>\n",
       "      <td>1.554</td>\n",
       "      <td>0.625</td>\n",
       "      <td>3.5190</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>3.700</td>\n",
       "      <td>AA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161.0</th>\n",
       "      <td>0.1280</td>\n",
       "      <td>0.995</td>\n",
       "      <td>6.506</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.571</td>\n",
       "      <td>0.330</td>\n",
       "      <td>0.088</td>\n",
       "      <td>-0.671</td>\n",
       "      <td>-0.031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.981</td>\n",
       "      <td>68.517</td>\n",
       "      <td>1.212</td>\n",
       "      <td>1.645</td>\n",
       "      <td>1.916</td>\n",
       "      <td>0.731</td>\n",
       "      <td>7.2410</td>\n",
       "      <td>0.5450</td>\n",
       "      <td>1.955</td>\n",
       "      <td>B-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1209.0</th>\n",
       "      <td>0.3630</td>\n",
       "      <td>1.720</td>\n",
       "      <td>10.387</td>\n",
       "      <td>0.4390</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.381</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.143</td>\n",
       "      <td>...</td>\n",
       "      <td>0.525</td>\n",
       "      <td>1.536</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.984</td>\n",
       "      <td>1.272</td>\n",
       "      <td>0.664</td>\n",
       "      <td>2.5700</td>\n",
       "      <td>2.5090</td>\n",
       "      <td>2.840</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2017-01-31</th>\n",
       "      <th>184700.0</th>\n",
       "      <td>0.9050</td>\n",
       "      <td>0.179</td>\n",
       "      <td>3.227</td>\n",
       "      <td>7.5210</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561</td>\n",
       "      <td>3.043</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.901</td>\n",
       "      <td>1.133</td>\n",
       "      <td>1.177</td>\n",
       "      <td>0.7790</td>\n",
       "      <td>6.6212</td>\n",
       "      <td>3.297</td>\n",
       "      <td>BB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186310.0</th>\n",
       "      <td>1.1930</td>\n",
       "      <td>0.191</td>\n",
       "      <td>3.814</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.085</td>\n",
       "      <td>0.050</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532</td>\n",
       "      <td>1.650</td>\n",
       "      <td>0.296</td>\n",
       "      <td>0.872</td>\n",
       "      <td>1.257</td>\n",
       "      <td>2.973</td>\n",
       "      <td>1.1830</td>\n",
       "      <td>1.4548</td>\n",
       "      <td>2.620</td>\n",
       "      <td>BB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186989.0</th>\n",
       "      <td>0.4020</td>\n",
       "      <td>2.031</td>\n",
       "      <td>22.723</td>\n",
       "      <td>0.1930</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.362</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.094</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.104</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.340</td>\n",
       "      <td>1.270</td>\n",
       "      <td>1.907</td>\n",
       "      <td>0.929</td>\n",
       "      <td>2.6960</td>\n",
       "      <td>12.2018</td>\n",
       "      <td>0.890</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188255.0</th>\n",
       "      <td>0.4090</td>\n",
       "      <td>1.224</td>\n",
       "      <td>12.075</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.420</td>\n",
       "      <td>0.101</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.113</td>\n",
       "      <td>0.162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.980</td>\n",
       "      <td>0.328</td>\n",
       "      <td>1.518</td>\n",
       "      <td>2.104</td>\n",
       "      <td>0.903</td>\n",
       "      <td>2.2320</td>\n",
       "      <td>3.1272</td>\n",
       "      <td>1.600</td>\n",
       "      <td>BBB+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189491.0</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>11.099</td>\n",
       "      <td>29.996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321</td>\n",
       "      <td>0.704</td>\n",
       "      <td>1.986</td>\n",
       "      <td>2.595</td>\n",
       "      <td>2.595</td>\n",
       "      <td>0.682</td>\n",
       "      <td>11.9470</td>\n",
       "      <td>2.8981</td>\n",
       "      <td>1.458</td>\n",
       "      <td>BBB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25133 rows Ã— 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          bm      ps     pcf     dpr    npm    gpm    cfm  \\\n",
       "public_date gvkey                                                           \n",
       "2010-02-28  1045.0    0.1496   0.113  -5.253  2.6608 -0.072  0.163 -0.018   \n",
       "            1075.0    1.2520   1.197   3.547  0.7720  0.084  0.330  0.251   \n",
       "            1078.0    0.2570   2.313   9.064  0.4680  0.167  0.648  0.236   \n",
       "            1161.0    0.1280   0.995   6.506  0.0220  0.180  0.571  0.330   \n",
       "            1209.0    0.3630   1.720  10.387  0.4390  0.101  0.381  0.206   \n",
       "...                      ...     ...     ...     ...    ...    ...    ...   \n",
       "2017-01-31  184700.0  0.9050   0.179   3.227  7.5210  0.029  0.178  0.055   \n",
       "            186310.0  1.1930   0.191   3.814  0.0260  0.038  0.085  0.050   \n",
       "            186989.0  0.4020   2.031  22.723  0.1930  0.047  0.362  0.074   \n",
       "            188255.0  0.4090   1.224  12.075  0.1440  0.069  0.420  0.101   \n",
       "            189491.0  0.0740  11.099  29.996  0.0000  0.217  0.981  0.256   \n",
       "\n",
       "                        roa    roe   roce  ...  debt_capital  de_ratio  \\\n",
       "public_date gvkey                          ...                           \n",
       "2010-02-28  1045.0    0.024 -1.575 -0.058  ...         1.338    -9.366   \n",
       "            1075.0    0.090  0.056  0.089  ...         0.562     2.596   \n",
       "            1078.0    0.193  0.259  0.199  ...         0.455     1.390   \n",
       "            1161.0    0.088 -0.671 -0.031  ...         0.981    68.517   \n",
       "            1209.0    0.175  0.161  0.143  ...         0.525     1.536   \n",
       "...                     ...    ...    ...  ...           ...       ...   \n",
       "2017-01-31  184700.0  0.094  0.127  0.123  ...         0.561     3.043   \n",
       "            186310.0  0.212  0.266  0.337  ...         0.532     1.650   \n",
       "            186989.0  0.094  0.065  0.104  ...         0.213     0.600   \n",
       "            188255.0  0.147  0.113  0.162  ...         0.361     0.980   \n",
       "            189491.0  0.239  0.254  0.250  ...         0.321     0.704   \n",
       "\n",
       "                      cash_ratio  quick_ratio  curr_ratio  at_turn      ptb  \\\n",
       "public_date gvkey                                                             \n",
       "2010-02-28  1045.0         0.428        0.603       0.664    0.816  10.2184   \n",
       "            1075.0         0.091        0.654       0.843    0.268   0.7940   \n",
       "            1078.0         0.553        1.296       1.554    0.625   3.5190   \n",
       "            1161.0         1.212        1.645       1.916    0.731   7.2410   \n",
       "            1209.0         0.127        0.984       1.272    0.664   2.5700   \n",
       "...                          ...          ...         ...      ...      ...   \n",
       "2017-01-31  184700.0       0.134        0.901       1.133    1.177   0.7790   \n",
       "            186310.0       0.296        0.872       1.257    2.973   1.1830   \n",
       "            186989.0       0.340        1.270       1.907    0.929   2.6960   \n",
       "            188255.0       0.328        1.518       2.104    0.903   2.2320   \n",
       "            189491.0       1.986        2.595       2.595    0.682  11.9470   \n",
       "\n",
       "                      PEG_trailing  DIVYIELD  splticrm  \n",
       "public_date gvkey                                       \n",
       "2010-02-28  1045.0         -3.7486     3.011        B-  \n",
       "            1075.0          9.1553     5.980      BBB-  \n",
       "            1078.0          0.2280     3.700        AA  \n",
       "            1161.0          0.5450     1.955        B-  \n",
       "            1209.0          2.5090     2.840         A  \n",
       "...                            ...       ...       ...  \n",
       "2017-01-31  184700.0        6.6212     3.297       BB+  \n",
       "            186310.0        1.4548     2.620       BB+  \n",
       "            186989.0       12.2018     0.890       BBB  \n",
       "            188255.0        3.1272     1.600      BBB+  \n",
       "            189491.0        2.8981     1.458       BBB  \n",
       "\n",
       "[25133 rows x 40 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_month_lag   # checking that everything worked as intended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_month_lag.loc[:, :'DIVYIELD']\n",
    "y = one_month_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.96\n",
      "Test score:       0.96\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_year_lag = lagged.loc[:, :'DIVYIELD'].unstack().shift(6)\n",
    "half_year_lag = half_year_lag.stack(dropna=False)\n",
    "half_year_lag = pd.concat([half_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "half_year_lag = half_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = half_year_lag.loc[:, :'DIVYIELD']\n",
    "y = half_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.98\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_lag = lagged.loc[:, :'DIVYIELD'].unstack().shift(12)\n",
    "one_year_lag = one_year_lag.stack(dropna=False)\n",
    "one_year_lag = pd.concat([one_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year_lag = one_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = one_year_lag.loc[:, :'DIVYIELD']\n",
    "y = one_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.98\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=100, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': 100, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "threehalf_year_lag = lagged.loc[:, :'DIVYIELD'].unstack().shift(42)\n",
    "threehalf_year_lag = threehalf_year_lag.stack(dropna=False)\n",
    "threehalf_year_lag = pd.concat([threehalf_year_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "threehalf_year_lag = threehalf_year_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = threehalf_year_lag.loc[:, :'DIVYIELD']\n",
    "y = threehalf_year_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly enough (and again: consistently over many different seeds) the predictive accuracy is slightly worse for a one-month-lag of the independent variables compared to using the most current data. However, both a half-year lag and a one-year lag perform better than using the most current data, and lagging the dependent variables by three and a half years is about as good as using current data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we try out what happens when we use a negative lag of one year. This means that we try to predict credit ratings by data that will only come out a year later, so we do not expect this model to perform well. After all, the existing ratios are supposed to influence the credit rating, and the future ratios do not exist yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_lag = lagged.loc[:, :'DIVYIELD'].unstack().shift(-12)\n",
    "reverse_lag = reverse_lag.stack(dropna=False)\n",
    "reverse_lag = pd.concat([reverse_lag, lagged.loc[:, 'splticrm']], axis = 'columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_lag = reverse_lag.dropna(axis = 'rows', how = 'any')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = reverse_lag.loc[:, :'DIVYIELD']\n",
    "y = reverse_lag.loc[:, 'splticrm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To our surprise, this model actually performs quite well. Since no-one can know the future ratings in advance, this probably means that the future ratings are very similar to the current ratings. Another interpretation is that not only do the ratios affect the credit ratings, but the credit ratings must in turn affect the future ratios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to see if we can further improve our model (full set of variables, random forest) by adding even more information, that is less directly related to the credit ratings. That information consists of: GIC sector, common shares traded, price close, price high and price low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "moredat = pd.read_csv('additional_variables.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I only keep the variables I'm interested in\n",
    "selCols = ['gvkey', 'datadate', 'cshtr_c', 'prcc_c', 'prch_c', 'prcl_c', 'gsector']\n",
    "moredat = moredat.loc[:, selCols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because there is only one date for each year and company, I can reduce the information to year\n",
    "moredat['year'] = pd.to_datetime(moredat.datadate, format = '%Y%m%d')\n",
    "moredat['year'] = pd.to_datetime(moredat.year).dt.year\n",
    "del moredat['datadate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the variable indfmt, which we don't need, is responsible for duplicates that we need to remove\n",
    "moredat = moredat.set_index(['gvkey', 'year'])\n",
    "moredat = moredat[~moredat.index.duplicated(keep='first')]\n",
    "moredat.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gvkey</th>\n",
       "      <th>year</th>\n",
       "      <th>cshtr_c</th>\n",
       "      <th>prcc_c</th>\n",
       "      <th>prch_c</th>\n",
       "      <th>prcl_c</th>\n",
       "      <th>gsector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1004</td>\n",
       "      <td>2010</td>\n",
       "      <td>103342631.0</td>\n",
       "      <td>22.98</td>\n",
       "      <td>24.96</td>\n",
       "      <td>10.49</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1004</td>\n",
       "      <td>2011</td>\n",
       "      <td>88443622.0</td>\n",
       "      <td>27.47</td>\n",
       "      <td>28.61</td>\n",
       "      <td>14.91</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1004</td>\n",
       "      <td>2012</td>\n",
       "      <td>80676553.0</td>\n",
       "      <td>19.17</td>\n",
       "      <td>31.66</td>\n",
       "      <td>14.96</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1004</td>\n",
       "      <td>2013</td>\n",
       "      <td>85991627.0</td>\n",
       "      <td>18.68</td>\n",
       "      <td>23.67</td>\n",
       "      <td>10.00</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1004</td>\n",
       "      <td>2014</td>\n",
       "      <td>68957400.0</td>\n",
       "      <td>28.01</td>\n",
       "      <td>31.55</td>\n",
       "      <td>16.02</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81314</th>\n",
       "      <td>327451</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81315</th>\n",
       "      <td>328795</td>\n",
       "      <td>2013</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81316</th>\n",
       "      <td>328795</td>\n",
       "      <td>2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81317</th>\n",
       "      <td>328795</td>\n",
       "      <td>2015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81318</th>\n",
       "      <td>328795</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81319 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        gvkey  year      cshtr_c  prcc_c  prch_c  prcl_c  gsector\n",
       "0        1004  2010  103342631.0   22.98   24.96   10.49     20.0\n",
       "1        1004  2011   88443622.0   27.47   28.61   14.91     20.0\n",
       "2        1004  2012   80676553.0   19.17   31.66   14.96     20.0\n",
       "3        1004  2013   85991627.0   18.68   23.67   10.00     20.0\n",
       "4        1004  2014   68957400.0   28.01   31.55   16.02     20.0\n",
       "...       ...   ...          ...     ...     ...     ...      ...\n",
       "81314  327451  2016          NaN     NaN     NaN     NaN     20.0\n",
       "81315  328795  2013          NaN     NaN     NaN     NaN     20.0\n",
       "81316  328795  2014          NaN     NaN     NaN     NaN     20.0\n",
       "81317  328795  2015          NaN     NaN     NaN     NaN     20.0\n",
       "81318  328795  2016          NaN     NaN     NaN     NaN     20.0\n",
       "\n",
       "[81319 rows x 7 columns]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moredat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I also need the year in imputed, so I can merge the data sets on it\n",
    "imputed['year'] = imputed['public_date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldat = pd.merge(imputed, moredat, on=['year', 'gvkey'], how='left', validate = 'many_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bm</th>\n",
       "      <th>ps</th>\n",
       "      <th>pcf</th>\n",
       "      <th>dpr</th>\n",
       "      <th>npm</th>\n",
       "      <th>gpm</th>\n",
       "      <th>cfm</th>\n",
       "      <th>roa</th>\n",
       "      <th>roe</th>\n",
       "      <th>roce</th>\n",
       "      <th>...</th>\n",
       "      <th>DIVYIELD</th>\n",
       "      <th>public_date</th>\n",
       "      <th>gvkey</th>\n",
       "      <th>splticrm</th>\n",
       "      <th>year</th>\n",
       "      <th>cshtr_c</th>\n",
       "      <th>prcc_c</th>\n",
       "      <th>prch_c</th>\n",
       "      <th>prcl_c</th>\n",
       "      <th>gsector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1496</td>\n",
       "      <td>0.113</td>\n",
       "      <td>-5.253</td>\n",
       "      <td>2.6608</td>\n",
       "      <td>-0.072</td>\n",
       "      <td>0.163</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-1.5750</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>...</td>\n",
       "      <td>3.011</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.153</td>\n",
       "      <td>3.287</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>2.890</td>\n",
       "      <td>2010-02-28</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.152</td>\n",
       "      <td>3.258</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>2.890</td>\n",
       "      <td>2010-03-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0710</td>\n",
       "      <td>0.123</td>\n",
       "      <td>2.640</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-4.4859</td>\n",
       "      <td>-0.092</td>\n",
       "      <td>...</td>\n",
       "      <td>2.890</td>\n",
       "      <td>2010-04-30</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0964</td>\n",
       "      <td>0.127</td>\n",
       "      <td>2.753</td>\n",
       "      <td>7.3744</td>\n",
       "      <td>-0.079</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-4.4832</td>\n",
       "      <td>-0.110</td>\n",
       "      <td>...</td>\n",
       "      <td>3.424</td>\n",
       "      <td>2010-05-31</td>\n",
       "      <td>1045.0</td>\n",
       "      <td>B-</td>\n",
       "      <td>2010</td>\n",
       "      <td>3.061813e+09</td>\n",
       "      <td>7.79</td>\n",
       "      <td>10.50</td>\n",
       "      <td>5.8602</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25463</th>\n",
       "      <td>0.0780</td>\n",
       "      <td>11.083</td>\n",
       "      <td>28.586</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.279</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.2820</td>\n",
       "      <td>0.268</td>\n",
       "      <td>...</td>\n",
       "      <td>1.199</td>\n",
       "      <td>2016-09-30</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25464</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>13.734</td>\n",
       "      <td>37.120</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.518</td>\n",
       "      <td>2016-10-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25465</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.452</td>\n",
       "      <td>33.655</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.458</td>\n",
       "      <td>2016-11-30</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25466</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>11.099</td>\n",
       "      <td>29.996</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.217</td>\n",
       "      <td>0.981</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.239</td>\n",
       "      <td>0.2540</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.458</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2016</td>\n",
       "      <td>3.308091e+08</td>\n",
       "      <td>49.52</td>\n",
       "      <td>54.99</td>\n",
       "      <td>31.6700</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25467</th>\n",
       "      <td>0.0740</td>\n",
       "      <td>12.672</td>\n",
       "      <td>30.415</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.212</td>\n",
       "      <td>0.978</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.240</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.442</td>\n",
       "      <td>2017-01-31</td>\n",
       "      <td>189491.0</td>\n",
       "      <td>BBB</td>\n",
       "      <td>2017</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25468 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           bm      ps     pcf     dpr    npm    gpm    cfm    roa     roe  \\\n",
       "0      0.1496   0.113  -5.253  2.6608 -0.072  0.163 -0.018  0.024 -1.5750   \n",
       "1      0.0710   0.153   3.287  7.3744 -0.074  0.150 -0.018  0.010 -4.4859   \n",
       "2      0.0710   0.152   3.258  7.3744 -0.074  0.150 -0.018  0.010 -4.4859   \n",
       "3      0.0710   0.123   2.640  7.3744 -0.074  0.150 -0.018  0.010 -4.4859   \n",
       "4      0.0964   0.127   2.753  7.3744 -0.079  0.150 -0.025  0.009 -4.4832   \n",
       "...       ...     ...     ...     ...    ...    ...    ...    ...     ...   \n",
       "25463  0.0780  11.083  28.586  0.0000  0.243  0.982  0.279  0.258  0.2820   \n",
       "25464  0.0740  13.734  37.120  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "25465  0.0740  12.452  33.655  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "25466  0.0740  11.099  29.996  0.0000  0.217  0.981  0.256  0.239  0.2540   \n",
       "25467  0.0740  12.672  30.415  0.0000  0.212  0.978  0.253  0.240  0.2500   \n",
       "\n",
       "        roce  ...  DIVYIELD  public_date     gvkey  splticrm  year  \\\n",
       "0     -0.058  ...     3.011   2010-01-31    1045.0        B-  2010   \n",
       "1     -0.092  ...     2.890   2010-02-28    1045.0        B-  2010   \n",
       "2     -0.092  ...     2.890   2010-03-31    1045.0        B-  2010   \n",
       "3     -0.092  ...     2.890   2010-04-30    1045.0        B-  2010   \n",
       "4     -0.110  ...     3.424   2010-05-31    1045.0        B-  2010   \n",
       "...      ...  ...       ...          ...       ...       ...   ...   \n",
       "25463  0.268  ...     1.199   2016-09-30  189491.0       BBB  2016   \n",
       "25464  0.250  ...     1.518   2016-10-31  189491.0       BBB  2016   \n",
       "25465  0.250  ...     1.458   2016-11-30  189491.0       BBB  2016   \n",
       "25466  0.250  ...     1.458   2016-12-31  189491.0       BBB  2016   \n",
       "25467  0.250  ...     1.442   2017-01-31  189491.0       BBB  2017   \n",
       "\n",
       "            cshtr_c  prcc_c  prch_c   prcl_c  gsector  \n",
       "0      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "1      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "2      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "3      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "4      3.061813e+09    7.79   10.50   5.8602     20.0  \n",
       "...             ...     ...     ...      ...      ...  \n",
       "25463  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "25464  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "25465  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "25466  3.308091e+08   49.52   54.99  31.6700     20.0  \n",
       "25467           NaN     NaN     NaN      NaN      NaN  \n",
       "\n",
       "[25468 rows x 48 columns]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gvkey         0\n",
       "splticrm      0\n",
       "year          0\n",
       "cshtr_c     303\n",
       "prcc_c      303\n",
       "prch_c      303\n",
       "prcl_c      303\n",
       "gsector     303\n",
       "dtype: int64"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isna(alldat.loc[:, 'gvkey' : 'gsector']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have new missing values, we need to do another round of imputation. Again we chose IterativeImputer, and with the same specifications as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember that I stored all numerical variables in numerical_dat, right before the first imputation process\n",
    "interesting_cols = np.append(numerical_dat.columns.values, ['year', 'cshtr_c', 'prcc_c', 'prch_c', 'prcl_c', 'gsector'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\impute\\_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  \" reached.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "random.seed(69)\n",
    "allimputed = imp.fit_transform(alldat.loc[:, interesting_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = allimputed   # using all variables at our disposal\n",
    "y = pd.factorize(alldat.loc[:, 'splticrm'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV accuracy: 0.97\n",
      "Test score:       0.97\n",
      "Best parameters: {'classifier': RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
      "                       criterion='gini', max_depth=None, max_features='auto',\n",
      "                       max_leaf_nodes=None, max_samples=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
      "                       n_jobs=None, oob_score=False, random_state=None,\n",
      "                       verbose=0, warm_start=False), 'classifier__max_depth': None, 'classifier__min_samples_split': 2, 'scaler': StandardScaler(copy=True, with_mean=True, with_std=True)}\n"
     ]
    }
   ],
   "source": [
    "# Create Pipeline object with standard scaler and RandomForest estimator\n",
    "pipe = Pipeline([('scaler', StandardScaler()), \n",
    "                 ('classifier', RandomForestClassifier())])\n",
    "\n",
    "# Define the hyperparameter values to be tested\n",
    "param_grid = [{'scaler': [StandardScaler()],\n",
    "               'classifier': [RandomForestClassifier(criterion='gini')],\n",
    "               'classifier__max_depth': [1, 10, 100, None],\n",
    "               'classifier__min_samples_split': [2, 10, 20]}]\n",
    "\n",
    "# Run brute-force grid search\n",
    "gs = GridSearchCV(estimator=pipe, param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train, y_train)\n",
    "\n",
    "print('Best CV accuracy: {:.2f}'.format(gs.best_score_))\n",
    "print('Test score:       {:.2f}'.format(gs.score(X_test, y_test)))\n",
    "print('Best parameters: {}'.format(gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that adding the variables GIC sector, common shares traded, price close, price high and price low has almost no effect on our prediction model. The test score does not improve, only the cross-validation accuracy has slightly improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, our best models are the ones using a full set of variables, random forest and lagged data by one or half a year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
